{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Thunder speech A Hackable speech recognition library. What to expect from this project: End-to-end speech recognition models Simple fine-tuning to new languages Inference support as a first-class feature Developer oriented api What it's not: A general-purpose speech toolkit A collection of complex systems that require thousands of gpu-hours and expert knowledge, only focusing on the state-of-the-art results Quick usage guide Install Install the library from PyPI: pip install thunder-speech Load the model and train it from thunder.registry import load_pretrained from thunder.quartznet.compatibility import QuartznetCheckpoint # Tab completion works to discover other QuartznetCheckpoint.* module = load_pretrained ( QuartznetCheckpoint . QuartzNet5x5LS_En ) # It also accepts the string identifier module = load_pretrained ( \"QuartzNet5x5LS_En\" ) # Or models from the huggingface hub module = load_pretrained ( \"facebook/wav2vec2-large-960h\" ) Export to a pure pytorch model using torchscript module . to_torchscript ( \"model_ready_for_inference.pt\" ) # Optional step: also export audio loading pipeline from thunder.data.dataset import AudioFileLoader loader = AudioFileLoader ( sample_rate = 16000 ) scripted_loader = torch . jit . script ( loader ) scripted_loader . save ( \"audio_loader.pt\" ) Run inference in production import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) # transcriptions is a list of strings with the captions. transcriptions = model . predict ( audio ) More quick tips If you want to know how to access the raw probabilities and decode manually or fine-tune the models you can access the documentation here . Contributing The first step to contribute is to do an editable installation of the library: git clone https://github.com/scart97/thunder-speech.git cd thunder-speech poetry install pre-commit install Then, make sure that everything is working. You can run the test suit, that is based on pytest: RUN_SLOW=1 poetry run pytest Here the RUN_SLOW flag is used to run all the tests, including the ones that might download checkpoints or do small training runs and are marked as slow. If you don't have a CUDA capable gpu, some tests will be unconditionally skipped. Influences This library has heavy influence of the best practices in the pytorch ecosystem. The original model code, including checkpoints, is based on the NeMo ASR toolkit. From there also came the inspiration for the fine-tuning and prediction api's. The data loading and processing is loosely based on my experience using fast.ai. It tries to decouple transforms that happen at the item level from the ones that are efficiently implemented for the whole batch at the GPU. Also, the idea that default parameters should be great. The overall organization of code and decoupling follows the pytorch-lightning ideals, with self-contained modules that try to reduce the boilerplate necessary. Finally, the transformers library inspired the simple model implementations, with a clear separation in folders containing the specific code that you need to understand each architecture and preprocessing, and their strong test suit.","title":"Home"},{"location":"#thunder-speech","text":"A Hackable speech recognition library. What to expect from this project: End-to-end speech recognition models Simple fine-tuning to new languages Inference support as a first-class feature Developer oriented api What it's not: A general-purpose speech toolkit A collection of complex systems that require thousands of gpu-hours and expert knowledge, only focusing on the state-of-the-art results","title":"Thunder speech"},{"location":"#quick-usage-guide","text":"","title":"Quick usage guide"},{"location":"#install","text":"Install the library from PyPI: pip install thunder-speech","title":"Install"},{"location":"#load-the-model-and-train-it","text":"from thunder.registry import load_pretrained from thunder.quartznet.compatibility import QuartznetCheckpoint # Tab completion works to discover other QuartznetCheckpoint.* module = load_pretrained ( QuartznetCheckpoint . QuartzNet5x5LS_En ) # It also accepts the string identifier module = load_pretrained ( \"QuartzNet5x5LS_En\" ) # Or models from the huggingface hub module = load_pretrained ( \"facebook/wav2vec2-large-960h\" )","title":"Load the model and train it"},{"location":"#export-to-a-pure-pytorch-model-using-torchscript","text":"module . to_torchscript ( \"model_ready_for_inference.pt\" ) # Optional step: also export audio loading pipeline from thunder.data.dataset import AudioFileLoader loader = AudioFileLoader ( sample_rate = 16000 ) scripted_loader = torch . jit . script ( loader ) scripted_loader . save ( \"audio_loader.pt\" )","title":"Export to a pure pytorch model using torchscript"},{"location":"#run-inference-in-production","text":"import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) # transcriptions is a list of strings with the captions. transcriptions = model . predict ( audio )","title":"Run inference in production"},{"location":"#more-quick-tips","text":"If you want to know how to access the raw probabilities and decode manually or fine-tune the models you can access the documentation here .","title":"More quick tips"},{"location":"#contributing","text":"The first step to contribute is to do an editable installation of the library: git clone https://github.com/scart97/thunder-speech.git cd thunder-speech poetry install pre-commit install Then, make sure that everything is working. You can run the test suit, that is based on pytest: RUN_SLOW=1 poetry run pytest Here the RUN_SLOW flag is used to run all the tests, including the ones that might download checkpoints or do small training runs and are marked as slow. If you don't have a CUDA capable gpu, some tests will be unconditionally skipped.","title":"Contributing"},{"location":"#influences","text":"This library has heavy influence of the best practices in the pytorch ecosystem. The original model code, including checkpoints, is based on the NeMo ASR toolkit. From there also came the inspiration for the fine-tuning and prediction api's. The data loading and processing is loosely based on my experience using fast.ai. It tries to decouple transforms that happen at the item level from the ones that are efficiently implemented for the whole batch at the GPU. Also, the idea that default parameters should be great. The overall organization of code and decoupling follows the pytorch-lightning ideals, with self-contained modules that try to reduce the boilerplate necessary. Finally, the transformers library inspired the simple model implementations, with a clear separation in folders containing the specific code that you need to understand each architecture and preprocessing, and their strong test suit.","title":"Influences"},{"location":"Custom%20Data/","text":"Writing a custom data pipeline There's a BaseSpeechDataset class that can be used as base to load the data. The library expects that each element in the dataset will be a tuple (audio_tensor, text_label), where the audio tensor has shape (channels, time) and text_label is the corresponding label as a string. The BaseSpeechDataset has two important properties: 1. A list (or iterable) .items , that has all the metadata to load every item in the dataset 2. The .loader module. That is a pytorch class that uses torchaudio to load audio tensors and can apply resampling and mono conversion. It was designed to be exported independently of the dataset, so that the same data loading can be used during inference. To get each element in the dataset, the following code is used, and each function call can be overwritten to control functionality: class BaseSpeechDataset ( Dataset ): def __getitem__ ( self , index : int ) -> Tuple [ Tensor , str ]: item = self . get_item ( index ) # Dealing with input audio , sr = self . open_audio ( item ) audio = self . preprocess_audio ( audio , sr ) # Dealing with output text = self . open_text ( item ) text = self . preprocess_text ( text ) return audio , text The flow of loading the data happens as follows: self.get_item is called with a specific index. It uses self.items to return the specific metadata to that example All the metadata is sent to self.open_audio . The relevant subset is used to load the audio tensor and corresponding sample rate, using self.loader.open_audio(...) Inside self.preprocess_audio the audio tensor is resampled and converted to mono if necessary using self.loader.preprocess_audio(...) . At this point, any augmentation that happens at the signal level to individual items can be applied. Only the audio tensor is returned, because it's assumed that every audio in the dataset will be resampled to the same sample rate self.open_text uses the same metadata to open the corresponding text label self.preprocess_text can be used to apply any transform directly to the text. Common options are lower case, expanding contractions ( I'm becomes I am ), expanding numbers ( 42 becomes forty two ) and removing punctuation Example: Loading data from nemo This example will implement thunder.data.datamodule.ManifestDatamodule and thunder.data.dataset.ManifestSpeechDataset . Load source The nemo manifest file follows the Json lines format, where each line is a valid json containing the metadata relevant to one example: {\"audio_filepath\": \"commonvoice/pt/train/22026127.mp3\", \"duration\": 4.32, \"text\": \"Quatro\"} {\"audio_filepath\": \"commonvoice/pt/train/23920071.mp3\", \"duration\": 2.256, \"text\": \"Oito\"} {\"audio_filepath\": \"commonvoice/pt/train/20272843.mp3\", \"duration\": 2.544, \"text\": \"Eu vou desligar\"} We can load this using the stdlib json and pathlib modules: from pathlib import Path import json file = Path ( \"manifest.json\" ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] The result is a list, where each element is a dictionary with the relevant data to a single example in the dataset. Let's start to wrap this code inside a BaseSpeechDataset : from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) Load audio We know that the \"audio_filepath\" key is related to the input: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) Load text The text is already loaded inside the \"text\" key: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ] Fix text The only text processing that will be applied in this example is transforming all the characters to lowercase: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ] def preprocess_text ( self , text : str ) -> str : return text . lower () Datamodule with sources Just wrap the datasets inside a BaseDataModule . Implement get_dataset to return the dataset for each split. from thunder.data.datamodule import BaseDataModule class ManifestDatamodule ( BaseDataModule ): def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate ) Using the datamodule datamodule = ManifestDatamodule ( \"train_manifest.json\" , \"val_manifest.json\" , \"test_manifest.json\" , batch_size = 32 )","title":"Writing a custom data pipeline"},{"location":"Custom%20Data/#writing-a-custom-data-pipeline","text":"There's a BaseSpeechDataset class that can be used as base to load the data. The library expects that each element in the dataset will be a tuple (audio_tensor, text_label), where the audio tensor has shape (channels, time) and text_label is the corresponding label as a string. The BaseSpeechDataset has two important properties: 1. A list (or iterable) .items , that has all the metadata to load every item in the dataset 2. The .loader module. That is a pytorch class that uses torchaudio to load audio tensors and can apply resampling and mono conversion. It was designed to be exported independently of the dataset, so that the same data loading can be used during inference. To get each element in the dataset, the following code is used, and each function call can be overwritten to control functionality: class BaseSpeechDataset ( Dataset ): def __getitem__ ( self , index : int ) -> Tuple [ Tensor , str ]: item = self . get_item ( index ) # Dealing with input audio , sr = self . open_audio ( item ) audio = self . preprocess_audio ( audio , sr ) # Dealing with output text = self . open_text ( item ) text = self . preprocess_text ( text ) return audio , text The flow of loading the data happens as follows: self.get_item is called with a specific index. It uses self.items to return the specific metadata to that example All the metadata is sent to self.open_audio . The relevant subset is used to load the audio tensor and corresponding sample rate, using self.loader.open_audio(...) Inside self.preprocess_audio the audio tensor is resampled and converted to mono if necessary using self.loader.preprocess_audio(...) . At this point, any augmentation that happens at the signal level to individual items can be applied. Only the audio tensor is returned, because it's assumed that every audio in the dataset will be resampled to the same sample rate self.open_text uses the same metadata to open the corresponding text label self.preprocess_text can be used to apply any transform directly to the text. Common options are lower case, expanding contractions ( I'm becomes I am ), expanding numbers ( 42 becomes forty two ) and removing punctuation","title":"Writing a custom data pipeline"},{"location":"Custom%20Data/#example-loading-data-from-nemo","text":"This example will implement thunder.data.datamodule.ManifestDatamodule and thunder.data.dataset.ManifestSpeechDataset .","title":"Example: Loading data from nemo"},{"location":"Custom%20Data/#load-source","text":"The nemo manifest file follows the Json lines format, where each line is a valid json containing the metadata relevant to one example: {\"audio_filepath\": \"commonvoice/pt/train/22026127.mp3\", \"duration\": 4.32, \"text\": \"Quatro\"} {\"audio_filepath\": \"commonvoice/pt/train/23920071.mp3\", \"duration\": 2.256, \"text\": \"Oito\"} {\"audio_filepath\": \"commonvoice/pt/train/20272843.mp3\", \"duration\": 2.544, \"text\": \"Eu vou desligar\"} We can load this using the stdlib json and pathlib modules: from pathlib import Path import json file = Path ( \"manifest.json\" ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] The result is a list, where each element is a dictionary with the relevant data to a single example in the dataset. Let's start to wrap this code inside a BaseSpeechDataset : from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate )","title":"Load source"},{"location":"Custom%20Data/#load-audio","text":"We know that the \"audio_filepath\" key is related to the input: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ])","title":"Load audio"},{"location":"Custom%20Data/#load-text","text":"The text is already loaded inside the \"text\" key: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"Load text"},{"location":"Custom%20Data/#fix-text","text":"The only text processing that will be applied in this example is transforming all the characters to lowercase: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ] def preprocess_text ( self , text : str ) -> str : return text . lower ()","title":"Fix text"},{"location":"Custom%20Data/#datamodule-with-sources","text":"Just wrap the datasets inside a BaseDataModule . Implement get_dataset to return the dataset for each split. from thunder.data.datamodule import BaseDataModule class ManifestDatamodule ( BaseDataModule ): def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"Datamodule with sources"},{"location":"Custom%20Data/#using-the-datamodule","text":"datamodule = ManifestDatamodule ( \"train_manifest.json\" , \"val_manifest.json\" , \"test_manifest.json\" , batch_size = 32 )","title":"Using the datamodule"},{"location":"Ultimate%20guide/","text":"The ultimate guide to speech recognition This guide is meant to give you all the steps necessary to achieve a decent (but not necessarily state-of-the-art) speech recognition system in a new language. Gathering the data Speech recognition systems are really sensitive to the quality of data used to train them. Also, they usually require from hundreds to thousands of hours depending on the quality expected. Some good sources for data are Mozilla commonvoice , the OpenSLR project or Tatoeba . After you download some initial data, there's a number of data quality problems that are expected and need to be fixed if you want to increase the performance of the trained models. First, list all the audio files by increasing size and check if there's any corrupted file (usually they're very small). Remove them from the training data. Then install sox , that's the best tool to inspect and convert audio files. It should come with a basic tool to inspect any file in the terminal, called soxi . As an example: $ soxi example_file.wav Input File : 'example_file.wav' Channels : 1 Sample Rate : 16000 Precision : 16-bit Duration : 00:00:04.27 = 94053 samples ~ 319.908 CDDA sectors File Size : 188k Bit Rate : 353k Sample Encoding: 16-bit Signed Integer PCM That's the usual format of files used in speech recognition research. Wav files, encoded with a 16-bit PCM codec and a sample rate of 16 kHz. The file format and codec can vary and will only affect the quality of the audio, but the sample rate is the essential one. Trained models will only predict well on audios that have the same sample rate as the data the model was trained on. Any file with a different sample rate must be resampled at the file level with sox or after loading with torchaudio. Sox has more capabilities than just listing audio metadata. It can read almost any file format and convert to others. If you have a mp3 file at 44.1 kHz, and want to convert into the usual wav format above, you can use: sox input_file.mp3 -r 16000 -c 1 -b 16 output_file.wav The flags used represent: -r 16000 : resample to a 16kHz sample rate -c 1 : convert to mono (1 channel) -b 16 : convert to PCM 16-bit output_file.wav : Sox understands that the output will be wav just by the file extension Ideally all the training and inference audio files should have the same characteristics, so it's a good idea to transform them into a common format before training. As the wav format does not have any compression, the resulting data will demand a huge HDD space. If that's a problem, you can instead convert the files to mp3, that way you lose a small percentage of the performance but can achieve up to 10x smaller dataset sizes. Now take a look at the labels. We are searching for a number of different problems here: Strange symbols: can easily find if you list all unique characters in the dataset Text in another language: remove these files Additional info that should not be there, like speaker identification as part of the transcription (common in subtitles) Regional/temporal differences that can cause the same words to have multiple written forms: mixing data from multiple countries that speak the same language, or using labels that came from old books Try to fix those label problems, or remove them from the training set if you have lots of data. Don't spend weeks just looking at the data, but have a small subset that you can trust is properly cleaned, even if that means manually labeling again. After you train the first couple of models, it's possible to use the model itself to help find problems in the training data. Loading the data The first step to train a speech recognition model is to load the collected data into the specific format the model expects. Usually, data from different sources will have unique ways that the label is encoded. Sometimes it's multiple .txt files, one for each audio. Another popular option is to have some .csv or .json file with the metadata of multiple examples. The recommendation here is that you convert all the labels to the same format before training. This will simplify the data loading, and any tools that you build to inspect the data can be shared between datasets. There's no obvious choice here, but the nemo manifest format has some pros: It's easy to save and version the manifest, to have fully reproducible training An increasing number of tools support it, including NeMo and Label Studio . The code to load the metadata is simple and intuitive It can store additional metadata as necessary In this format, each of the train/validation/test splits has one file containing the metadata. It has the extension .json , and contains one json in each line with the relevant data to one example, following the Json lines format: {\"audio_filepath\": \"commonvoice/pt/train/22026127.mp3\", \"duration\": 4.32, \"text\": \"Quatro\"} {\"audio_filepath\": \"commonvoice/pt/train/23920071.mp3\", \"duration\": 2.256, \"text\": \"Oito\"} {\"audio_filepath\": \"commonvoice/pt/train/20272843.mp3\", \"duration\": 2.544, \"text\": \"Eu vou desligar\"} These three keys for each example are required: audio_filepath : Contains the path to the input audio duration : has the duration of the audio, in seconds text : that's the corresponding label Make sure that each example starts and end on the same line. This is one example of invalid manifest: {\"audio_filepath\": \"example1.mp3\", \"duration\": 1.0, \"text\": \"This label starts in one line but has multiple line breaks making this manifest invalid\"} {\"audio_filepath\": \"example2.mp3\", \"duration\": 2.0, \"text\": \"this label is really long similar to the one above it, but it's contained into a single line making it valid\"} To load this data, the corresponding LightningModule is already implemented: from thunder.data.datamodule import ManifestDatamodule datamodule = ManifestDatamodule ( \"train_manifest.json\" , \"val_manifest.json\" , \"test_manifest.json\" , batch_size = 32 ) First train For this first train, you should only try to overfit one batch. This is the simplest test, and if you can't get past it then anything more complex that you try will be wasted time. To do it, try to load a training dataset with only one batch worth of data. The validation/test sets can be as usual, you will ignore them at this step. As we are using pytorch lightning, there's a trainer flag to limit the number of training batches ( overfit_batches=1 ) that can be used. Before you run the training, disable any augmentation, regularization and advanced stuff like learning rate scheduling. You can start with either a pretrained model, or a clean new one, but either way don't freeze any parameters, just let it all train. Start the training, and you should see the loss follow a pattern where, the more time you let it run, the lower the final value will be. This means that small bumps will happen, but it will always recover and keep going down. The ideal point is where you run the prediction on the batch that you overfit, and the model doesn't make a single mistake. Expected train loss: The validation loss is not important at this stage, we are trying to overfit on purpose to check if the model is learning correctly. It's possible to notice that it improved in the first epoch, but instantly went back up and keep increasing as the training goes. Expected validation loss: Note that if you're using the overfit_batches flag, the same training batch will be used during the validation step, and the validation loss/metrics will follow the same pattern as the training loss instead. Some problems that can happen: The train loss doesn't go below a certain value : Check if it' always the same batch being trained. It can happen when you forget to disable the shuffling in the train dataloader. The train loss is negative : There's a blank in the target text, find and remove it. Blanks should only be produced by the model, never at the labels. There are no predictions at all : let it train for more time Still, there are no predictions after a long time : Check if the target texts are being processed correctly. Inside the training step, decode the target text and assert that it returns what you expect The train loss just keep increasing : try to lower the learning rate Second train Now repeat the first training, but with around 10 hours of data. This number depends on the hardware that you have available, but something that gives you 2 minute epochs is a good amount. This time, you're not trying to overfit anymore. The validation loss will start to get lower, and the metrics will improve compared to the first training. Quickly, the model will reach the point where the data is enough, and it will start to overfit to the training data. Expected train loss: Expected validation loss: At this point the model is still overfitting, but it should be way less than the first train. The objective of this training round is to confirm that the model performance improves as the amount of training data increases, and also to find any problems that were not caught when using only a single batch. Scaling to the whole dataset Now that we confirmed the model is training properly, it's time to use all the training data available. Remember that speech recognition models are really sensitive to the quality of the labels, and transcription errors across the dataset can make the training unstable. The improvements from cleaning the labels and collecting more data can be an order of magnitude higher than using the new fancy SOTA model of the month with mislabeled data. Some tips at this step: break long audios : more than 25 seconds is usually bad for each train example, it can cause catastrophic forgetting on sequence based models, or out-of-memory errors Use the model to find problems : After the first few models are trained, they can be used to aid while cleaning the data: Sort by train loss descending and manually check the files. This will show the examples where the model is having the most difficult to learn, and that can be caused by bad data or outliers Sort by Character Error Rate (CER) descending and manually check the files. Similar to the test above, but this time sorting by the character error rate. Most of the top audios will be the same, but this test can also show new problematic ones. Sort by CER ascending on the validation/test set to find possible data leak. If there's some data repeated between train and validation/test, the metrics will be lower compared to samples that are new to the model. Watch for the loss spikes during training : Sometimes the loss will have spikes as show in the next figure, where it went from around 0.1 to 1.1 during a single step. This can be caused by mislabeled data, try to log the full batch when this happens and check manually. Reducing overfit To reduce the overfit in trained models, follow the recipe given by fast.ai , in this order of priority: Add more data Data augmentation Generalizable architectures Regularization Reduce architecture complexity While adding more data, try to follow the tips in the Scaling to the whole dataset section to use trained models to find problem in the new data. Also, when doing any of the other steps, it's best to repeat the Second train to quickly confirm that the modified model is still training properly. Deploy! Finally, it's time to deploy the trained model. This includes exporting the model, writing the data pipelines, building the server/mobile app where the model will run and monitoring. The recommended way to export models is using torchscript. This will ensure that they preserve correct behaviour, and every model implemented in this library is rigorously tested to be compatible with it. Pytorch tracing and onnx are also tested to export, but those methods have limitations around variable input shapes. The audio loading and preprocessing is also implemented as a nn.Module , so that it can be scripted and exported for inference. It exists as the .loader attribute inside the datasets, or can be directly instantiated from thunder.data.dataset.AudioFileLoader . This way, there's no need to reimplement the input pipeline when deploying the model. As a bonus, the exported models using torchscript remove the dependency on thunder, only pytorch is necessary to run the model, and optionally torchaudio to load the audio data using AudioFileLoader . It's possible to use the torchscript model inside a mobile app. Before exporting, one patch need to be applied so that operations that are not natively supported on mobile are changed to compatible implementations. One example of implementing a simple speech recognition app can be found here . To learn more about deployment and MLOps, there are free courses from made with ml and full stack deep learning that go in depth.","title":"The ultimate guide to speech recognition"},{"location":"Ultimate%20guide/#the-ultimate-guide-to-speech-recognition","text":"This guide is meant to give you all the steps necessary to achieve a decent (but not necessarily state-of-the-art) speech recognition system in a new language.","title":"The ultimate guide to speech recognition"},{"location":"Ultimate%20guide/#gathering-the-data","text":"Speech recognition systems are really sensitive to the quality of data used to train them. Also, they usually require from hundreds to thousands of hours depending on the quality expected. Some good sources for data are Mozilla commonvoice , the OpenSLR project or Tatoeba . After you download some initial data, there's a number of data quality problems that are expected and need to be fixed if you want to increase the performance of the trained models. First, list all the audio files by increasing size and check if there's any corrupted file (usually they're very small). Remove them from the training data. Then install sox , that's the best tool to inspect and convert audio files. It should come with a basic tool to inspect any file in the terminal, called soxi . As an example: $ soxi example_file.wav Input File : 'example_file.wav' Channels : 1 Sample Rate : 16000 Precision : 16-bit Duration : 00:00:04.27 = 94053 samples ~ 319.908 CDDA sectors File Size : 188k Bit Rate : 353k Sample Encoding: 16-bit Signed Integer PCM That's the usual format of files used in speech recognition research. Wav files, encoded with a 16-bit PCM codec and a sample rate of 16 kHz. The file format and codec can vary and will only affect the quality of the audio, but the sample rate is the essential one. Trained models will only predict well on audios that have the same sample rate as the data the model was trained on. Any file with a different sample rate must be resampled at the file level with sox or after loading with torchaudio. Sox has more capabilities than just listing audio metadata. It can read almost any file format and convert to others. If you have a mp3 file at 44.1 kHz, and want to convert into the usual wav format above, you can use: sox input_file.mp3 -r 16000 -c 1 -b 16 output_file.wav The flags used represent: -r 16000 : resample to a 16kHz sample rate -c 1 : convert to mono (1 channel) -b 16 : convert to PCM 16-bit output_file.wav : Sox understands that the output will be wav just by the file extension Ideally all the training and inference audio files should have the same characteristics, so it's a good idea to transform them into a common format before training. As the wav format does not have any compression, the resulting data will demand a huge HDD space. If that's a problem, you can instead convert the files to mp3, that way you lose a small percentage of the performance but can achieve up to 10x smaller dataset sizes. Now take a look at the labels. We are searching for a number of different problems here: Strange symbols: can easily find if you list all unique characters in the dataset Text in another language: remove these files Additional info that should not be there, like speaker identification as part of the transcription (common in subtitles) Regional/temporal differences that can cause the same words to have multiple written forms: mixing data from multiple countries that speak the same language, or using labels that came from old books Try to fix those label problems, or remove them from the training set if you have lots of data. Don't spend weeks just looking at the data, but have a small subset that you can trust is properly cleaned, even if that means manually labeling again. After you train the first couple of models, it's possible to use the model itself to help find problems in the training data.","title":"Gathering the data"},{"location":"Ultimate%20guide/#loading-the-data","text":"The first step to train a speech recognition model is to load the collected data into the specific format the model expects. Usually, data from different sources will have unique ways that the label is encoded. Sometimes it's multiple .txt files, one for each audio. Another popular option is to have some .csv or .json file with the metadata of multiple examples. The recommendation here is that you convert all the labels to the same format before training. This will simplify the data loading, and any tools that you build to inspect the data can be shared between datasets. There's no obvious choice here, but the nemo manifest format has some pros: It's easy to save and version the manifest, to have fully reproducible training An increasing number of tools support it, including NeMo and Label Studio . The code to load the metadata is simple and intuitive It can store additional metadata as necessary In this format, each of the train/validation/test splits has one file containing the metadata. It has the extension .json , and contains one json in each line with the relevant data to one example, following the Json lines format: {\"audio_filepath\": \"commonvoice/pt/train/22026127.mp3\", \"duration\": 4.32, \"text\": \"Quatro\"} {\"audio_filepath\": \"commonvoice/pt/train/23920071.mp3\", \"duration\": 2.256, \"text\": \"Oito\"} {\"audio_filepath\": \"commonvoice/pt/train/20272843.mp3\", \"duration\": 2.544, \"text\": \"Eu vou desligar\"} These three keys for each example are required: audio_filepath : Contains the path to the input audio duration : has the duration of the audio, in seconds text : that's the corresponding label Make sure that each example starts and end on the same line. This is one example of invalid manifest: {\"audio_filepath\": \"example1.mp3\", \"duration\": 1.0, \"text\": \"This label starts in one line but has multiple line breaks making this manifest invalid\"} {\"audio_filepath\": \"example2.mp3\", \"duration\": 2.0, \"text\": \"this label is really long similar to the one above it, but it's contained into a single line making it valid\"} To load this data, the corresponding LightningModule is already implemented: from thunder.data.datamodule import ManifestDatamodule datamodule = ManifestDatamodule ( \"train_manifest.json\" , \"val_manifest.json\" , \"test_manifest.json\" , batch_size = 32 )","title":"Loading the data"},{"location":"Ultimate%20guide/#first-train","text":"For this first train, you should only try to overfit one batch. This is the simplest test, and if you can't get past it then anything more complex that you try will be wasted time. To do it, try to load a training dataset with only one batch worth of data. The validation/test sets can be as usual, you will ignore them at this step. As we are using pytorch lightning, there's a trainer flag to limit the number of training batches ( overfit_batches=1 ) that can be used. Before you run the training, disable any augmentation, regularization and advanced stuff like learning rate scheduling. You can start with either a pretrained model, or a clean new one, but either way don't freeze any parameters, just let it all train. Start the training, and you should see the loss follow a pattern where, the more time you let it run, the lower the final value will be. This means that small bumps will happen, but it will always recover and keep going down. The ideal point is where you run the prediction on the batch that you overfit, and the model doesn't make a single mistake. Expected train loss: The validation loss is not important at this stage, we are trying to overfit on purpose to check if the model is learning correctly. It's possible to notice that it improved in the first epoch, but instantly went back up and keep increasing as the training goes. Expected validation loss: Note that if you're using the overfit_batches flag, the same training batch will be used during the validation step, and the validation loss/metrics will follow the same pattern as the training loss instead. Some problems that can happen: The train loss doesn't go below a certain value : Check if it' always the same batch being trained. It can happen when you forget to disable the shuffling in the train dataloader. The train loss is negative : There's a blank in the target text, find and remove it. Blanks should only be produced by the model, never at the labels. There are no predictions at all : let it train for more time Still, there are no predictions after a long time : Check if the target texts are being processed correctly. Inside the training step, decode the target text and assert that it returns what you expect The train loss just keep increasing : try to lower the learning rate","title":"First train"},{"location":"Ultimate%20guide/#second-train","text":"Now repeat the first training, but with around 10 hours of data. This number depends on the hardware that you have available, but something that gives you 2 minute epochs is a good amount. This time, you're not trying to overfit anymore. The validation loss will start to get lower, and the metrics will improve compared to the first training. Quickly, the model will reach the point where the data is enough, and it will start to overfit to the training data. Expected train loss: Expected validation loss: At this point the model is still overfitting, but it should be way less than the first train. The objective of this training round is to confirm that the model performance improves as the amount of training data increases, and also to find any problems that were not caught when using only a single batch.","title":"Second train"},{"location":"Ultimate%20guide/#scaling-to-the-whole-dataset","text":"Now that we confirmed the model is training properly, it's time to use all the training data available. Remember that speech recognition models are really sensitive to the quality of the labels, and transcription errors across the dataset can make the training unstable. The improvements from cleaning the labels and collecting more data can be an order of magnitude higher than using the new fancy SOTA model of the month with mislabeled data. Some tips at this step: break long audios : more than 25 seconds is usually bad for each train example, it can cause catastrophic forgetting on sequence based models, or out-of-memory errors Use the model to find problems : After the first few models are trained, they can be used to aid while cleaning the data: Sort by train loss descending and manually check the files. This will show the examples where the model is having the most difficult to learn, and that can be caused by bad data or outliers Sort by Character Error Rate (CER) descending and manually check the files. Similar to the test above, but this time sorting by the character error rate. Most of the top audios will be the same, but this test can also show new problematic ones. Sort by CER ascending on the validation/test set to find possible data leak. If there's some data repeated between train and validation/test, the metrics will be lower compared to samples that are new to the model. Watch for the loss spikes during training : Sometimes the loss will have spikes as show in the next figure, where it went from around 0.1 to 1.1 during a single step. This can be caused by mislabeled data, try to log the full batch when this happens and check manually.","title":"Scaling to the whole dataset"},{"location":"Ultimate%20guide/#reducing-overfit","text":"To reduce the overfit in trained models, follow the recipe given by fast.ai , in this order of priority: Add more data Data augmentation Generalizable architectures Regularization Reduce architecture complexity While adding more data, try to follow the tips in the Scaling to the whole dataset section to use trained models to find problem in the new data. Also, when doing any of the other steps, it's best to repeat the Second train to quickly confirm that the modified model is still training properly.","title":"Reducing overfit"},{"location":"Ultimate%20guide/#deploy","text":"Finally, it's time to deploy the trained model. This includes exporting the model, writing the data pipelines, building the server/mobile app where the model will run and monitoring. The recommended way to export models is using torchscript. This will ensure that they preserve correct behaviour, and every model implemented in this library is rigorously tested to be compatible with it. Pytorch tracing and onnx are also tested to export, but those methods have limitations around variable input shapes. The audio loading and preprocessing is also implemented as a nn.Module , so that it can be scripted and exported for inference. It exists as the .loader attribute inside the datasets, or can be directly instantiated from thunder.data.dataset.AudioFileLoader . This way, there's no need to reimplement the input pipeline when deploying the model. As a bonus, the exported models using torchscript remove the dependency on thunder, only pytorch is necessary to run the model, and optionally torchaudio to load the audio data using AudioFileLoader . It's possible to use the torchscript model inside a mobile app. Before exporting, one patch need to be applied so that operations that are not natively supported on mobile are changed to compatible implementations. One example of implementing a simple speech recognition app can be found here . To learn more about deployment and MLOps, there are free courses from made with ml and full stack deep learning that go in depth.","title":"Deploy!"},{"location":"quick%20reference%20guide/","text":"Quick reference guide How to load a Quartznet/Citrinet .nemo file? from thunder.quartznet.compatibility import load_quartznet_checkpoint from thunder.citrinet.compatibility import load_citrinet_checkpoint module = load_quartznet_checkpoint ( \"/path/to/quartznet.nemo\" ) module = load_citrinet_checkpoint ( \"/path/to/citrinet.nemo\" ) How to export models with special restrictions? Case 1: Using Quartznet or Citrinet on platforms that doesnt support FFT (android, onnx): from thunder.registry import load_pretrained from thunder.quartznet.transform import patch_stft import torch module = load_pretrained ( \"QuartzNet5x5LS_En\" ) module . audio_transform = patch_stft ( module . audio_transform ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) Case 2: Wav2vec 2.0 using torchscript from thunder.registry import load_pretrained from thunder.huggingface.compatibility import prepare_scriptable_wav2vec module = load_pretrained ( \"facebook/wav2vec2-large-960h\" ) module = prepare_scriptable_wav2vec ( module ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) What if I want the probabilities instead of the captions during inference? Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_transform . decode_prediction ( probs . argmax ( 1 )) How to finetune a model if I already have the nemo manifests prepared? import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.registry import load_pretrained from thunder.callbacks import FinetuneEncoderDecoder dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) model = load_pretrained ( \"QuartzNet5x5LS_En\" ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , callbacks = [ FinetuneEncoderDecoder ( unfreeze_encoder_at_epoch = 1 )], ) trainer . fit ( model = model , datamodule = dm ) How to get the tokens from my dataset? from thunder.text_processing.tokenizer import char_tokenizer , get_most_frequent_tokens my_datamodule = CustomDatamodule ( ... ) my_datamodule . prepare_data () my_datamodule . setup ( None ) train_corpus = \" \" . join ( my_datamodule . train_dataset . all_outputs ()) tokens = get_most_frequent_tokens ( train_corpus , char_tokenizer )","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#quick-reference-guide","text":"","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#how-to-load-a-quartznetcitrinet-nemo-file","text":"from thunder.quartznet.compatibility import load_quartznet_checkpoint from thunder.citrinet.compatibility import load_citrinet_checkpoint module = load_quartznet_checkpoint ( \"/path/to/quartznet.nemo\" ) module = load_citrinet_checkpoint ( \"/path/to/citrinet.nemo\" )","title":"How to load a Quartznet/Citrinet .nemo file?"},{"location":"quick%20reference%20guide/#how-to-export-models-with-special-restrictions","text":"Case 1: Using Quartznet or Citrinet on platforms that doesnt support FFT (android, onnx): from thunder.registry import load_pretrained from thunder.quartznet.transform import patch_stft import torch module = load_pretrained ( \"QuartzNet5x5LS_En\" ) module . audio_transform = patch_stft ( module . audio_transform ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) Case 2: Wav2vec 2.0 using torchscript from thunder.registry import load_pretrained from thunder.huggingface.compatibility import prepare_scriptable_wav2vec module = load_pretrained ( \"facebook/wav2vec2-large-960h\" ) module = prepare_scriptable_wav2vec ( module ) module . to_torchscript ( \"model_ready_for_inference.pt\" )","title":"How to export models with special restrictions?"},{"location":"quick%20reference%20guide/#what-if-i-want-the-probabilities-instead-of-the-captions-during-inference","text":"Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_transform . decode_prediction ( probs . argmax ( 1 ))","title":"What if I want the probabilities instead of the captions during inference?"},{"location":"quick%20reference%20guide/#how-to-finetune-a-model-if-i-already-have-the-nemo-manifests-prepared","text":"import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.registry import load_pretrained from thunder.callbacks import FinetuneEncoderDecoder dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) model = load_pretrained ( \"QuartzNet5x5LS_En\" ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , callbacks = [ FinetuneEncoderDecoder ( unfreeze_encoder_at_epoch = 1 )], ) trainer . fit ( model = model , datamodule = dm )","title":"How to finetune a model if I already have the nemo manifests prepared?"},{"location":"quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset","text":"from thunder.text_processing.tokenizer import char_tokenizer , get_most_frequent_tokens my_datamodule = CustomDatamodule ( ... ) my_datamodule . prepare_data () my_datamodule . setup ( None ) train_corpus = \" \" . join ( my_datamodule . train_dataset . all_outputs ()) tokens = get_most_frequent_tokens ( train_corpus , char_tokenizer )","title":"How to get the tokens from my dataset?"},{"location":"api/CTC%20Loss/","text":"Functionality to calculate the ctc loss. calculate_ctc ( probabilities , y , prob_lengths , y_lengths , blank_idx ) Calculates the ctc loss based on model probabilities (also called emissions) and labels. Parameters: Name Type Description Default probabilities Tensor Output of the model, before any softmax operation. Shape [batch, #vocab, time] required y Tensor Tensor containing the corresponding labels. Shape [batch] required prob_lengths Tensor Lengths of each element in the input. Shape [batch] required y_lengths Tensor Lenghts of each element in the output. Should NOT be normalized. required blank_idx int Index of the blank token in the vocab. required Returns: Type Description Tensor Loss tensor that can be backpropagated. Source code in thunder/ctc_loss.py def calculate_ctc ( probabilities : Tensor , y : Tensor , prob_lengths : Tensor , y_lengths : Tensor , blank_idx : int , ) -> Tensor : \"\"\"Calculates the ctc loss based on model probabilities (also called emissions) and labels. Args: probabilities: Output of the model, before any softmax operation. Shape [batch, #vocab, time] y: Tensor containing the corresponding labels. Shape [batch] prob_lengths: Lengths of each element in the input. Shape [batch] y_lengths: Lenghts of each element in the output. Should NOT be normalized. blank_idx: Index of the blank token in the vocab. Returns: Loss tensor that can be backpropagated. \"\"\" # Change from (batch, #vocab, time) to (time, batch, #vocab) probabilities = probabilities . permute ( 2 , 0 , 1 ) logprobs = log_softmax ( probabilities , dim = 2 ) return ctc_loss ( logprobs , y , prob_lengths . long (), y_lengths , blank = blank_idx , reduction = \"mean\" , zero_infinity = True , )","title":"CTC Loss"},{"location":"api/CTC%20Loss/#thunder.ctc_loss.calculate_ctc","text":"Calculates the ctc loss based on model probabilities (also called emissions) and labels. Parameters: Name Type Description Default probabilities Tensor Output of the model, before any softmax operation. Shape [batch, #vocab, time] required y Tensor Tensor containing the corresponding labels. Shape [batch] required prob_lengths Tensor Lengths of each element in the input. Shape [batch] required y_lengths Tensor Lenghts of each element in the output. Should NOT be normalized. required blank_idx int Index of the blank token in the vocab. required Returns: Type Description Tensor Loss tensor that can be backpropagated. Source code in thunder/ctc_loss.py def calculate_ctc ( probabilities : Tensor , y : Tensor , prob_lengths : Tensor , y_lengths : Tensor , blank_idx : int , ) -> Tensor : \"\"\"Calculates the ctc loss based on model probabilities (also called emissions) and labels. Args: probabilities: Output of the model, before any softmax operation. Shape [batch, #vocab, time] y: Tensor containing the corresponding labels. Shape [batch] prob_lengths: Lengths of each element in the input. Shape [batch] y_lengths: Lenghts of each element in the output. Should NOT be normalized. blank_idx: Index of the blank token in the vocab. Returns: Loss tensor that can be backpropagated. \"\"\" # Change from (batch, #vocab, time) to (time, batch, #vocab) probabilities = probabilities . permute ( 2 , 0 , 1 ) logprobs = log_softmax ( probabilities , dim = 2 ) return ctc_loss ( logprobs , y , prob_lengths . long (), y_lengths , blank = blank_idx , reduction = \"mean\" , zero_infinity = True , )","title":"calculate_ctc()"},{"location":"api/blocks/","text":"Building blocks that can be shared across all models. Masked ( Module ) Wrapper to mix normal modules with others that take 2 inputs Source code in thunder/blocks.py class Masked ( nn . Module ): \"\"\"Wrapper to mix normal modules with others that take 2 inputs\"\"\" def __init__ ( self , * layers ): super () . __init__ () self . layer = nn . Sequential ( * layers ) def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: return self . layer ( audio ), audio_lengths forward ( self , audio , audio_lengths ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/blocks.py def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: return self . layer ( audio ), audio_lengths MultiSequential ( Sequential ) nn.Sequential equivalent with 2 inputs/outputs Source code in thunder/blocks.py class MultiSequential ( nn . Sequential ): \"\"\"nn.Sequential equivalent with 2 inputs/outputs\"\"\" def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: for module in self . children (): audio , audio_lengths = module ( audio , audio_lengths ) return audio , audio_lengths forward ( self , audio , audio_lengths ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/blocks.py def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: for module in self . children (): audio , audio_lengths = module ( audio , audio_lengths ) return audio , audio_lengths SwapLastDimension ( Module ) Layer that swap the last two dimensions of the data. Source code in thunder/blocks.py class SwapLastDimension ( nn . Module ): \"\"\"Layer that swap the last two dimensions of the data.\"\"\" def forward ( self , x : Tensor ) -> Tensor : return x . transpose ( - 1 , - 2 ) forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/blocks.py def forward ( self , x : Tensor ) -> Tensor : return x . transpose ( - 1 , - 2 ) conv1d_decoder ( decoder_input_channels , num_classes ) Decoder that uses one conv1d layer Parameters: Name Type Description Default num_classes int Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. required decoder_input_channels int Number of input channels of the decoder. That is the number of channels of the features created by the encoder. required Returns: Type Description Module Pytorch model of the decoder Source code in thunder/blocks.py def conv1d_decoder ( decoder_input_channels : int , num_classes : int ) -> nn . Module : \"\"\"Decoder that uses one conv1d layer Args: num_classes: Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. decoder_input_channels: Number of input channels of the decoder. That is the number of channels of the features created by the encoder. Returns: Pytorch model of the decoder \"\"\" decoder = nn . Conv1d ( decoder_input_channels , num_classes , kernel_size = 1 , bias = True , ) nn . init . xavier_uniform_ ( decoder . weight , gain = 1.0 ) return decoder convolution_stft ( input_data , n_fft = 1024 , hop_length = 512 , win_length = 1024 , window = tensor ([ 0.0000e+00 , 9.4175e-06 , 3.7730e-05 , ... , 3.7730e-05 , 9.4175e-06 , 0.0000e+00 ]), center = True , return_complex = False ) Implements the stft operation using the convolution method. This is one alternative to make possible to export code using this operation to onnx and arm based environments. The signature shuld follow the same as torch.stft, making it possible to just swap the two. The code is based on https://github.com/pseeth/torch-stft Source code in thunder/blocks.py def convolution_stft ( input_data : torch . Tensor , n_fft : int = 1024 , hop_length : int = 512 , win_length : int = 1024 , window : torch . Tensor = torch . hann_window ( 1024 , periodic = False ), center : bool = True , return_complex : bool = False , ) -> torch . Tensor : \"\"\"Implements the stft operation using the convolution method. This is one alternative to make possible to export code using this operation to onnx and arm based environments. The signature shuld follow the same as torch.stft, making it possible to just swap the two. The code is based on https://github.com/pseeth/torch-stft \"\"\" assert n_fft >= win_length pad_amount = int ( n_fft / 2 ) window = window . to ( input_data . device ) fourier_basis = _fourier_matrix ( n_fft , device = input_data . device ) cutoff = int (( n_fft / 2 + 1 )) fourier_basis = torch . stack ( [ torch . real ( fourier_basis [: cutoff , :]), torch . imag ( fourier_basis [: cutoff , :])] ) . reshape ( - 1 , n_fft ) forward_basis = fourier_basis [:, None , :] . float () window_pad = ( n_fft - win_length ) // 2 window_pad2 = n_fft - ( window_pad + win_length ) fft_window = torch . nn . functional . pad ( window , [ window_pad , window_pad2 ]) # window the bases forward_basis *= fft_window forward_basis = forward_basis . float () num_batches = input_data . shape [ 0 ] num_samples = input_data . shape [ - 1 ] # similar to librosa, reflect-pad the input input_data = input_data . view ( num_batches , 1 , num_samples ) input_data = F . pad ( input_data . unsqueeze ( 1 ), ( pad_amount , pad_amount , 0 , 0 ), mode = \"reflect\" , ) input_data = input_data . squeeze ( 1 ) forward_transform = F . conv1d ( input_data , forward_basis , stride = hop_length , padding = 0 ) cutoff = int (( n_fft / 2 ) + 1 ) real_part = forward_transform [:, : cutoff , :] imag_part = forward_transform [:, cutoff :, :] return torch . stack (( real_part , imag_part ), dim =- 1 ) get_same_padding ( kernel_size , stride , dilation ) Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Parameters: Name Type Description Default kernel_size int convolution kernel size. Only tested to be correct with odd values. required stride int convolution stride required dilation int convolution dilation required Exceptions: Type Description ValueError Only stride or dilation may be greater than 1 Returns: Type Description int padding value to obtain same padding. Source code in thunder/blocks.py def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size: convolution kernel size. Only tested to be correct with odd values. stride: convolution stride dilation: convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2 lengths_to_mask ( lengths , max_length ) Convert from integer lengths of each element to mask representation Parameters: Name Type Description Default lengths Tensor lengths of each element in the batch required max_length int maximum length expected. Can be greater than lengths.max() required Returns: Type Description Tensor Corresponding boolean mask indicating the valid region of the tensor. Source code in thunder/blocks.py def lengths_to_mask ( lengths : torch . Tensor , max_length : int ) -> torch . Tensor : \"\"\"Convert from integer lengths of each element to mask representation Args: lengths: lengths of each element in the batch max_length: maximum length expected. Can be greater than lengths.max() Returns: Corresponding boolean mask indicating the valid region of the tensor. \"\"\" lengths = lengths . type ( torch . long ) mask = torch . arange ( max_length , device = lengths . device ) . expand ( lengths . shape [ 0 ], max_length ) < lengths . unsqueeze ( 1 ) return mask linear_decoder ( decoder_input_channels , num_classes , decoder_dropout ) Decoder that uses a linear layer with dropout Parameters: Name Type Description Default decoder_dropout float Amount of dropout to be used in the decoder required decoder_input_channels int Number of input channels of the decoder. That is the number of channels of the features created by the encoder. required num_classes int Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. required Returns: Type Description Module Module that represents the decoder. Source code in thunder/blocks.py def linear_decoder ( decoder_input_channels : int , num_classes : int , decoder_dropout : float ) -> nn . Module : \"\"\"Decoder that uses a linear layer with dropout Args: decoder_dropout: Amount of dropout to be used in the decoder decoder_input_channels: Number of input channels of the decoder. That is the number of channels of the features created by the encoder. num_classes: Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. Returns: Module that represents the decoder. \"\"\" # SwapLastDimension is necessary to # change from (batch, time, #vocab) to (batch, #vocab, time) # that is expected by the rest of the library return nn . Sequential ( SwapLastDimension (), nn . Dropout ( decoder_dropout ), nn . Linear ( decoder_input_channels , num_classes ), SwapLastDimension (), ) normalize_tensor ( input_values , mask = None , div_guard = 1e-07 , dim =- 1 ) Normalize tensor values, optionally using some mask to define the valid region. Parameters: Name Type Description Default input_values Tensor input tensor to be normalized required mask Optional[torch.Tensor] Optional mask describing the valid elements. None div_guard float value used to prevent division by zero when normalizing. 1e-07 dim int dimension used to calculate the mean and variance. -1 Returns: Type Description Tensor Normalized tensor Source code in thunder/blocks.py def normalize_tensor ( input_values : torch . Tensor , mask : Optional [ torch . Tensor ] = None , div_guard : float = 1e-7 , dim : int = - 1 , ) -> torch . Tensor : \"\"\"Normalize tensor values, optionally using some mask to define the valid region. Args: input_values: input tensor to be normalized mask: Optional mask describing the valid elements. div_guard: value used to prevent division by zero when normalizing. dim: dimension used to calculate the mean and variance. Returns: Normalized tensor \"\"\" # Vectorized implementation of (x - x.mean()) / x.std() considering only the valid mask if mask is not None : # Making sure the elements outside the mask are zero, to have the correct mean/std input_values = torch . masked_fill ( input_values , ~ mask . type ( torch . bool ), 0.0 ) # Number of valid elements num_elements = mask . sum ( dim = dim , keepdim = True ) . detach () # Mean is sum over number of elements x_mean = input_values . sum ( dim = dim , keepdim = True ) . detach () / num_elements # std numerator: sum of squared differences to the mean numerator = ( input_values - x_mean ) . pow ( 2 ) . sum ( dim = dim , keepdim = True ) . detach () x_std = ( numerator / num_elements ) . sqrt () # using the div_guard to prevent division by zero normalized = ( input_values - x_mean ) / ( x_std + div_guard ) # Cleaning elements outside of valid mask return torch . masked_fill ( normalized , ~ mask . type ( torch . bool ), 0.0 ) mean = input_values . mean ( dim = dim , keepdim = True ) . detach () std = ( input_values . var ( dim = dim , keepdim = True ) . detach () + div_guard ) . sqrt () return ( input_values - mean ) / std","title":"Blocks"},{"location":"api/blocks/#thunder.blocks.Masked","text":"Wrapper to mix normal modules with others that take 2 inputs Source code in thunder/blocks.py class Masked ( nn . Module ): \"\"\"Wrapper to mix normal modules with others that take 2 inputs\"\"\" def __init__ ( self , * layers ): super () . __init__ () self . layer = nn . Sequential ( * layers ) def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: return self . layer ( audio ), audio_lengths","title":"Masked"},{"location":"api/blocks/#thunder.blocks.Masked.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/blocks.py def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: return self . layer ( audio ), audio_lengths","title":"forward()"},{"location":"api/blocks/#thunder.blocks.MultiSequential","text":"nn.Sequential equivalent with 2 inputs/outputs Source code in thunder/blocks.py class MultiSequential ( nn . Sequential ): \"\"\"nn.Sequential equivalent with 2 inputs/outputs\"\"\" def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: for module in self . children (): audio , audio_lengths = module ( audio , audio_lengths ) return audio , audio_lengths","title":"MultiSequential"},{"location":"api/blocks/#thunder.blocks.MultiSequential.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/blocks.py def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: for module in self . children (): audio , audio_lengths = module ( audio , audio_lengths ) return audio , audio_lengths","title":"forward()"},{"location":"api/blocks/#thunder.blocks.SwapLastDimension","text":"Layer that swap the last two dimensions of the data. Source code in thunder/blocks.py class SwapLastDimension ( nn . Module ): \"\"\"Layer that swap the last two dimensions of the data.\"\"\" def forward ( self , x : Tensor ) -> Tensor : return x . transpose ( - 1 , - 2 )","title":"SwapLastDimension"},{"location":"api/blocks/#thunder.blocks.SwapLastDimension.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/blocks.py def forward ( self , x : Tensor ) -> Tensor : return x . transpose ( - 1 , - 2 )","title":"forward()"},{"location":"api/blocks/#thunder.blocks.conv1d_decoder","text":"Decoder that uses one conv1d layer Parameters: Name Type Description Default num_classes int Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. required decoder_input_channels int Number of input channels of the decoder. That is the number of channels of the features created by the encoder. required Returns: Type Description Module Pytorch model of the decoder Source code in thunder/blocks.py def conv1d_decoder ( decoder_input_channels : int , num_classes : int ) -> nn . Module : \"\"\"Decoder that uses one conv1d layer Args: num_classes: Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. decoder_input_channels: Number of input channels of the decoder. That is the number of channels of the features created by the encoder. Returns: Pytorch model of the decoder \"\"\" decoder = nn . Conv1d ( decoder_input_channels , num_classes , kernel_size = 1 , bias = True , ) nn . init . xavier_uniform_ ( decoder . weight , gain = 1.0 ) return decoder","title":"conv1d_decoder()"},{"location":"api/blocks/#thunder.blocks.convolution_stft","text":"Implements the stft operation using the convolution method. This is one alternative to make possible to export code using this operation to onnx and arm based environments. The signature shuld follow the same as torch.stft, making it possible to just swap the two. The code is based on https://github.com/pseeth/torch-stft Source code in thunder/blocks.py def convolution_stft ( input_data : torch . Tensor , n_fft : int = 1024 , hop_length : int = 512 , win_length : int = 1024 , window : torch . Tensor = torch . hann_window ( 1024 , periodic = False ), center : bool = True , return_complex : bool = False , ) -> torch . Tensor : \"\"\"Implements the stft operation using the convolution method. This is one alternative to make possible to export code using this operation to onnx and arm based environments. The signature shuld follow the same as torch.stft, making it possible to just swap the two. The code is based on https://github.com/pseeth/torch-stft \"\"\" assert n_fft >= win_length pad_amount = int ( n_fft / 2 ) window = window . to ( input_data . device ) fourier_basis = _fourier_matrix ( n_fft , device = input_data . device ) cutoff = int (( n_fft / 2 + 1 )) fourier_basis = torch . stack ( [ torch . real ( fourier_basis [: cutoff , :]), torch . imag ( fourier_basis [: cutoff , :])] ) . reshape ( - 1 , n_fft ) forward_basis = fourier_basis [:, None , :] . float () window_pad = ( n_fft - win_length ) // 2 window_pad2 = n_fft - ( window_pad + win_length ) fft_window = torch . nn . functional . pad ( window , [ window_pad , window_pad2 ]) # window the bases forward_basis *= fft_window forward_basis = forward_basis . float () num_batches = input_data . shape [ 0 ] num_samples = input_data . shape [ - 1 ] # similar to librosa, reflect-pad the input input_data = input_data . view ( num_batches , 1 , num_samples ) input_data = F . pad ( input_data . unsqueeze ( 1 ), ( pad_amount , pad_amount , 0 , 0 ), mode = \"reflect\" , ) input_data = input_data . squeeze ( 1 ) forward_transform = F . conv1d ( input_data , forward_basis , stride = hop_length , padding = 0 ) cutoff = int (( n_fft / 2 ) + 1 ) real_part = forward_transform [:, : cutoff , :] imag_part = forward_transform [:, cutoff :, :] return torch . stack (( real_part , imag_part ), dim =- 1 )","title":"convolution_stft()"},{"location":"api/blocks/#thunder.blocks.get_same_padding","text":"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Parameters: Name Type Description Default kernel_size int convolution kernel size. Only tested to be correct with odd values. required stride int convolution stride required dilation int convolution dilation required Exceptions: Type Description ValueError Only stride or dilation may be greater than 1 Returns: Type Description int padding value to obtain same padding. Source code in thunder/blocks.py def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size: convolution kernel size. Only tested to be correct with odd values. stride: convolution stride dilation: convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2","title":"get_same_padding()"},{"location":"api/blocks/#thunder.blocks.lengths_to_mask","text":"Convert from integer lengths of each element to mask representation Parameters: Name Type Description Default lengths Tensor lengths of each element in the batch required max_length int maximum length expected. Can be greater than lengths.max() required Returns: Type Description Tensor Corresponding boolean mask indicating the valid region of the tensor. Source code in thunder/blocks.py def lengths_to_mask ( lengths : torch . Tensor , max_length : int ) -> torch . Tensor : \"\"\"Convert from integer lengths of each element to mask representation Args: lengths: lengths of each element in the batch max_length: maximum length expected. Can be greater than lengths.max() Returns: Corresponding boolean mask indicating the valid region of the tensor. \"\"\" lengths = lengths . type ( torch . long ) mask = torch . arange ( max_length , device = lengths . device ) . expand ( lengths . shape [ 0 ], max_length ) < lengths . unsqueeze ( 1 ) return mask","title":"lengths_to_mask()"},{"location":"api/blocks/#thunder.blocks.linear_decoder","text":"Decoder that uses a linear layer with dropout Parameters: Name Type Description Default decoder_dropout float Amount of dropout to be used in the decoder required decoder_input_channels int Number of input channels of the decoder. That is the number of channels of the features created by the encoder. required num_classes int Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. required Returns: Type Description Module Module that represents the decoder. Source code in thunder/blocks.py def linear_decoder ( decoder_input_channels : int , num_classes : int , decoder_dropout : float ) -> nn . Module : \"\"\"Decoder that uses a linear layer with dropout Args: decoder_dropout: Amount of dropout to be used in the decoder decoder_input_channels: Number of input channels of the decoder. That is the number of channels of the features created by the encoder. num_classes: Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. Returns: Module that represents the decoder. \"\"\" # SwapLastDimension is necessary to # change from (batch, time, #vocab) to (batch, #vocab, time) # that is expected by the rest of the library return nn . Sequential ( SwapLastDimension (), nn . Dropout ( decoder_dropout ), nn . Linear ( decoder_input_channels , num_classes ), SwapLastDimension (), )","title":"linear_decoder()"},{"location":"api/blocks/#thunder.blocks.normalize_tensor","text":"Normalize tensor values, optionally using some mask to define the valid region. Parameters: Name Type Description Default input_values Tensor input tensor to be normalized required mask Optional[torch.Tensor] Optional mask describing the valid elements. None div_guard float value used to prevent division by zero when normalizing. 1e-07 dim int dimension used to calculate the mean and variance. -1 Returns: Type Description Tensor Normalized tensor Source code in thunder/blocks.py def normalize_tensor ( input_values : torch . Tensor , mask : Optional [ torch . Tensor ] = None , div_guard : float = 1e-7 , dim : int = - 1 , ) -> torch . Tensor : \"\"\"Normalize tensor values, optionally using some mask to define the valid region. Args: input_values: input tensor to be normalized mask: Optional mask describing the valid elements. div_guard: value used to prevent division by zero when normalizing. dim: dimension used to calculate the mean and variance. Returns: Normalized tensor \"\"\" # Vectorized implementation of (x - x.mean()) / x.std() considering only the valid mask if mask is not None : # Making sure the elements outside the mask are zero, to have the correct mean/std input_values = torch . masked_fill ( input_values , ~ mask . type ( torch . bool ), 0.0 ) # Number of valid elements num_elements = mask . sum ( dim = dim , keepdim = True ) . detach () # Mean is sum over number of elements x_mean = input_values . sum ( dim = dim , keepdim = True ) . detach () / num_elements # std numerator: sum of squared differences to the mean numerator = ( input_values - x_mean ) . pow ( 2 ) . sum ( dim = dim , keepdim = True ) . detach () x_std = ( numerator / num_elements ) . sqrt () # using the div_guard to prevent division by zero normalized = ( input_values - x_mean ) / ( x_std + div_guard ) # Cleaning elements outside of valid mask return torch . masked_fill ( normalized , ~ mask . type ( torch . bool ), 0.0 ) mean = input_values . mean ( dim = dim , keepdim = True ) . detach () std = ( input_values . var ( dim = dim , keepdim = True ) . detach () + div_guard ) . sqrt () return ( input_values - mean ) / std","title":"normalize_tensor()"},{"location":"api/callbacks/","text":"Helper callback functionality, not essential to research FinetuneEncoderDecoder ( BaseFinetuning ) Source code in thunder/callbacks.py class FinetuneEncoderDecoder ( BaseFinetuning ): def __init__ ( self , unfreeze_encoder_at_epoch : int = 1 , encoder_initial_lr_div : float = 10 , train_batchnorm : bool = True , ): \"\"\" Finetune a encoder model based on a learning rate. Args: unfreeze_encoder_at_epoch: Epoch at which the encoder will be unfreezed. encoder_initial_lr_div: Used to scale down the encoder learning rate compared to rest of model. train_batchnorm: Make Batch Normalization trainable at the beginning of train. \"\"\" super () . __init__ () self . unfreeze_encoder_at_epoch = unfreeze_encoder_at_epoch self . encoder_initial_lr_div = encoder_initial_lr_div self . train_batchnorm = train_batchnorm def on_fit_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ): \"\"\"Check if the LightningModule has the necessary attribute before the train starts Args: trainer: Lightning Trainer pl_module: Lightning Module used during train Raises: Exception: If LightningModule has no nn.Module `encoder` attribute. \"\"\" if hasattr ( pl_module , \"encoder\" ) and isinstance ( pl_module . encoder , nn . Module ): return raise Exception ( \"The LightningModule should have a nn.Module `encoder` attribute\" ) def freeze_before_training ( self , pl_module : pl . LightningModule ): \"\"\"Freeze the encoder initially before the train starts. Args: pl_module: Lightning Module \"\"\" self . freeze ( pl_module . encoder , train_bn = self . train_batchnorm ) def finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int , ): \"\"\"Unfreezes the encoder at the specified epoch Args: pl_module: Lightning Module epoch: epoch number optimizer: optimizer used during training opt_idx: optimizer index \"\"\" if epoch == self . unfreeze_encoder_at_epoch : self . unfreeze_and_add_param_group ( pl_module . encoder , optimizer , initial_denom_lr = self . encoder_initial_lr_div , train_bn = not self . train_batchnorm , ) __init__ ( self , unfreeze_encoder_at_epoch = 1 , encoder_initial_lr_div = 10 , train_batchnorm = True ) special Finetune a encoder model based on a learning rate. Parameters: Name Type Description Default unfreeze_encoder_at_epoch int Epoch at which the encoder will be unfreezed. 1 encoder_initial_lr_div float Used to scale down the encoder learning rate compared to rest of model. 10 train_batchnorm bool Make Batch Normalization trainable at the beginning of train. True Source code in thunder/callbacks.py def __init__ ( self , unfreeze_encoder_at_epoch : int = 1 , encoder_initial_lr_div : float = 10 , train_batchnorm : bool = True , ): \"\"\" Finetune a encoder model based on a learning rate. Args: unfreeze_encoder_at_epoch: Epoch at which the encoder will be unfreezed. encoder_initial_lr_div: Used to scale down the encoder learning rate compared to rest of model. train_batchnorm: Make Batch Normalization trainable at the beginning of train. \"\"\" super () . __init__ () self . unfreeze_encoder_at_epoch = unfreeze_encoder_at_epoch self . encoder_initial_lr_div = encoder_initial_lr_div self . train_batchnorm = train_batchnorm finetune_function ( self , pl_module , epoch , optimizer , opt_idx ) Unfreezes the encoder at the specified epoch Parameters: Name Type Description Default pl_module LightningModule Lightning Module required epoch int epoch number required optimizer Optimizer optimizer used during training required opt_idx int optimizer index required Source code in thunder/callbacks.py def finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int , ): \"\"\"Unfreezes the encoder at the specified epoch Args: pl_module: Lightning Module epoch: epoch number optimizer: optimizer used during training opt_idx: optimizer index \"\"\" if epoch == self . unfreeze_encoder_at_epoch : self . unfreeze_and_add_param_group ( pl_module . encoder , optimizer , initial_denom_lr = self . encoder_initial_lr_div , train_bn = not self . train_batchnorm , ) freeze_before_training ( self , pl_module ) Freeze the encoder initially before the train starts. Parameters: Name Type Description Default pl_module LightningModule Lightning Module required Source code in thunder/callbacks.py def freeze_before_training ( self , pl_module : pl . LightningModule ): \"\"\"Freeze the encoder initially before the train starts. Args: pl_module: Lightning Module \"\"\" self . freeze ( pl_module . encoder , train_bn = self . train_batchnorm ) on_fit_start ( self , trainer , pl_module ) Check if the LightningModule has the necessary attribute before the train starts Parameters: Name Type Description Default trainer Trainer Lightning Trainer required pl_module LightningModule Lightning Module used during train required Exceptions: Type Description Exception If LightningModule has no nn.Module encoder attribute. Source code in thunder/callbacks.py def on_fit_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ): \"\"\"Check if the LightningModule has the necessary attribute before the train starts Args: trainer: Lightning Trainer pl_module: Lightning Module used during train Raises: Exception: If LightningModule has no nn.Module `encoder` attribute. \"\"\" if hasattr ( pl_module , \"encoder\" ) and isinstance ( pl_module . encoder , nn . Module ): return raise Exception ( \"The LightningModule should have a nn.Module `encoder` attribute\" )","title":"Callbacks"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder","text":"Source code in thunder/callbacks.py class FinetuneEncoderDecoder ( BaseFinetuning ): def __init__ ( self , unfreeze_encoder_at_epoch : int = 1 , encoder_initial_lr_div : float = 10 , train_batchnorm : bool = True , ): \"\"\" Finetune a encoder model based on a learning rate. Args: unfreeze_encoder_at_epoch: Epoch at which the encoder will be unfreezed. encoder_initial_lr_div: Used to scale down the encoder learning rate compared to rest of model. train_batchnorm: Make Batch Normalization trainable at the beginning of train. \"\"\" super () . __init__ () self . unfreeze_encoder_at_epoch = unfreeze_encoder_at_epoch self . encoder_initial_lr_div = encoder_initial_lr_div self . train_batchnorm = train_batchnorm def on_fit_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ): \"\"\"Check if the LightningModule has the necessary attribute before the train starts Args: trainer: Lightning Trainer pl_module: Lightning Module used during train Raises: Exception: If LightningModule has no nn.Module `encoder` attribute. \"\"\" if hasattr ( pl_module , \"encoder\" ) and isinstance ( pl_module . encoder , nn . Module ): return raise Exception ( \"The LightningModule should have a nn.Module `encoder` attribute\" ) def freeze_before_training ( self , pl_module : pl . LightningModule ): \"\"\"Freeze the encoder initially before the train starts. Args: pl_module: Lightning Module \"\"\" self . freeze ( pl_module . encoder , train_bn = self . train_batchnorm ) def finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int , ): \"\"\"Unfreezes the encoder at the specified epoch Args: pl_module: Lightning Module epoch: epoch number optimizer: optimizer used during training opt_idx: optimizer index \"\"\" if epoch == self . unfreeze_encoder_at_epoch : self . unfreeze_and_add_param_group ( pl_module . encoder , optimizer , initial_denom_lr = self . encoder_initial_lr_div , train_bn = not self . train_batchnorm , )","title":"FinetuneEncoderDecoder"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder.__init__","text":"Finetune a encoder model based on a learning rate. Parameters: Name Type Description Default unfreeze_encoder_at_epoch int Epoch at which the encoder will be unfreezed. 1 encoder_initial_lr_div float Used to scale down the encoder learning rate compared to rest of model. 10 train_batchnorm bool Make Batch Normalization trainable at the beginning of train. True Source code in thunder/callbacks.py def __init__ ( self , unfreeze_encoder_at_epoch : int = 1 , encoder_initial_lr_div : float = 10 , train_batchnorm : bool = True , ): \"\"\" Finetune a encoder model based on a learning rate. Args: unfreeze_encoder_at_epoch: Epoch at which the encoder will be unfreezed. encoder_initial_lr_div: Used to scale down the encoder learning rate compared to rest of model. train_batchnorm: Make Batch Normalization trainable at the beginning of train. \"\"\" super () . __init__ () self . unfreeze_encoder_at_epoch = unfreeze_encoder_at_epoch self . encoder_initial_lr_div = encoder_initial_lr_div self . train_batchnorm = train_batchnorm","title":"__init__()"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder.finetune_function","text":"Unfreezes the encoder at the specified epoch Parameters: Name Type Description Default pl_module LightningModule Lightning Module required epoch int epoch number required optimizer Optimizer optimizer used during training required opt_idx int optimizer index required Source code in thunder/callbacks.py def finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int , ): \"\"\"Unfreezes the encoder at the specified epoch Args: pl_module: Lightning Module epoch: epoch number optimizer: optimizer used during training opt_idx: optimizer index \"\"\" if epoch == self . unfreeze_encoder_at_epoch : self . unfreeze_and_add_param_group ( pl_module . encoder , optimizer , initial_denom_lr = self . encoder_initial_lr_div , train_bn = not self . train_batchnorm , )","title":"finetune_function()"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder.freeze_before_training","text":"Freeze the encoder initially before the train starts. Parameters: Name Type Description Default pl_module LightningModule Lightning Module required Source code in thunder/callbacks.py def freeze_before_training ( self , pl_module : pl . LightningModule ): \"\"\"Freeze the encoder initially before the train starts. Args: pl_module: Lightning Module \"\"\" self . freeze ( pl_module . encoder , train_bn = self . train_batchnorm )","title":"freeze_before_training()"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder.on_fit_start","text":"Check if the LightningModule has the necessary attribute before the train starts Parameters: Name Type Description Default trainer Trainer Lightning Trainer required pl_module LightningModule Lightning Module used during train required Exceptions: Type Description Exception If LightningModule has no nn.Module encoder attribute. Source code in thunder/callbacks.py def on_fit_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ): \"\"\"Check if the LightningModule has the necessary attribute before the train starts Args: trainer: Lightning Trainer pl_module: Lightning Module used during train Raises: Exception: If LightningModule has no nn.Module `encoder` attribute. \"\"\" if hasattr ( pl_module , \"encoder\" ) and isinstance ( pl_module . encoder , nn . Module ): return raise Exception ( \"The LightningModule should have a nn.Module `encoder` attribute\" )","title":"on_fit_start()"},{"location":"api/finetune/","text":"Module that implements easy finetuning of any model in the library. FinetuneCTCModule ( BaseCTCModule ) Source code in thunder/finetune.py class FinetuneCTCModule ( BaseCTCModule ): def __init__ ( self , checkpoint_name : str , checkpoint_kwargs : Dict [ str , Any ] = None , decoder_class : ModuleBuilderType = None , decoder_kwargs : Dict [ str , Any ] = None , tokens : List [ str ] = None , text_kwargs : Dict [ str , Any ] = None , optimizer_class : OptimizerBuilderType = torch . optim . AdamW , optimizer_kwargs : Dict [ str , Any ] = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict [ str , Any ] = None , ): \"\"\"Generic finetune module, load any combination of encoder/decoder and custom tokens Args: checkpoint_name: Name of the base checkpoint to load checkpoint_kwargs: Additional kwargs to the checkpoint loading function. decoder_class: Optional class to override the loaded checkpoint. decoder_kwargs: Additional kwargs to the decoder_class. tokens: If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. text_kwargs: Additional kwargs to the text_tranform class, when tokens is not None. optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. \"\"\" self . save_hyperparameters () checkpoint_kwargs = checkpoint_kwargs or {} decoder_kwargs = decoder_kwargs or {} text_kwargs = text_kwargs or {} if tokens is not None and decoder_class is None : # Missing decoder raise ValueError ( \"New tokens were specified, but the module also needs to know the decoder class to initialize properly.\" ) if tokens is None and decoder_class is not None : # Missing tokens raise ValueError ( \"A new decoder was specified, but the module also needs to know the tokens to initialize properly.\" ) checkpoint_data = load_pretrained ( checkpoint_name , ** checkpoint_kwargs ) if decoder_class is None : # Keep original decoder/text processing text_transform = checkpoint_data . text_transform decoder = checkpoint_data . decoder else : # Changing the decoder layer and text processing text_transform = BatchTextTransformer ( tokens , ** text_kwargs ) decoder = decoder_class ( checkpoint_data . encoder_final_dimension , text_transform . num_tokens , ** decoder_kwargs , ) super () . __init__ ( encoder = checkpoint_data . encoder , decoder = decoder , audio_transform = checkpoint_data . audio_transform , text_transform = text_transform , optimizer_class = optimizer_class , optimizer_kwargs = optimizer_kwargs , lr_scheduler_class = lr_scheduler_class , lr_scheduler_kwargs = lr_scheduler_kwargs , ) __init__ ( self , checkpoint_name , checkpoint_kwargs = None , decoder_class = None , decoder_kwargs = None , tokens = None , text_kwargs = None , optimizer_class =< class ' torch . optim . adamw . AdamW '>, optimizer_kwargs=None, lr_scheduler_class=None, lr_scheduler_kwargs=None) special Generic finetune module, load any combination of encoder/decoder and custom tokens Parameters: Name Type Description Default checkpoint_name str Name of the base checkpoint to load required checkpoint_kwargs Dict[str, Any] Additional kwargs to the checkpoint loading function. None decoder_class Union[Type[torch.nn.modules.module.Module], Callable[..., torch.nn.modules.module.Module]] Optional class to override the loaded checkpoint. None decoder_kwargs Dict[str, Any] Additional kwargs to the decoder_class. None tokens List[str] If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. None text_kwargs Dict[str, Any] Additional kwargs to the text_tranform class, when tokens is not None. None optimizer_class Union[Type[torch.optim.optimizer.Optimizer], Callable[..., torch.optim.optimizer.Optimizer]] Optimizer to use during training. <class 'torch.optim.adamw.AdamW'> optimizer_kwargs Dict[str, Any] Optional extra kwargs to the optimizer. None lr_scheduler_class Union[Type[torch.optim.lr_scheduler._LRScheduler], Type[torch.optim.lr_scheduler.ReduceLROnPlateau], Callable[..., Union[torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler.ReduceLROnPlateau]]] Optional class to use a learning rate scheduler with the optimizer. None lr_scheduler_kwargs Dict[str, Any] Optional extra kwargs to the learning rate scheduler. None Source code in thunder/finetune.py def __init__ ( self , checkpoint_name : str , checkpoint_kwargs : Dict [ str , Any ] = None , decoder_class : ModuleBuilderType = None , decoder_kwargs : Dict [ str , Any ] = None , tokens : List [ str ] = None , text_kwargs : Dict [ str , Any ] = None , optimizer_class : OptimizerBuilderType = torch . optim . AdamW , optimizer_kwargs : Dict [ str , Any ] = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict [ str , Any ] = None , ): \"\"\"Generic finetune module, load any combination of encoder/decoder and custom tokens Args: checkpoint_name: Name of the base checkpoint to load checkpoint_kwargs: Additional kwargs to the checkpoint loading function. decoder_class: Optional class to override the loaded checkpoint. decoder_kwargs: Additional kwargs to the decoder_class. tokens: If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. text_kwargs: Additional kwargs to the text_tranform class, when tokens is not None. optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. \"\"\" self . save_hyperparameters () checkpoint_kwargs = checkpoint_kwargs or {} decoder_kwargs = decoder_kwargs or {} text_kwargs = text_kwargs or {} if tokens is not None and decoder_class is None : # Missing decoder raise ValueError ( \"New tokens were specified, but the module also needs to know the decoder class to initialize properly.\" ) if tokens is None and decoder_class is not None : # Missing tokens raise ValueError ( \"A new decoder was specified, but the module also needs to know the tokens to initialize properly.\" ) checkpoint_data = load_pretrained ( checkpoint_name , ** checkpoint_kwargs ) if decoder_class is None : # Keep original decoder/text processing text_transform = checkpoint_data . text_transform decoder = checkpoint_data . decoder else : # Changing the decoder layer and text processing text_transform = BatchTextTransformer ( tokens , ** text_kwargs ) decoder = decoder_class ( checkpoint_data . encoder_final_dimension , text_transform . num_tokens , ** decoder_kwargs , ) super () . __init__ ( encoder = checkpoint_data . encoder , decoder = decoder , audio_transform = checkpoint_data . audio_transform , text_transform = text_transform , optimizer_class = optimizer_class , optimizer_kwargs = optimizer_kwargs , lr_scheduler_class = lr_scheduler_class , lr_scheduler_kwargs = lr_scheduler_kwargs , )","title":"Finetune"},{"location":"api/finetune/#thunder.finetune.FinetuneCTCModule","text":"Source code in thunder/finetune.py class FinetuneCTCModule ( BaseCTCModule ): def __init__ ( self , checkpoint_name : str , checkpoint_kwargs : Dict [ str , Any ] = None , decoder_class : ModuleBuilderType = None , decoder_kwargs : Dict [ str , Any ] = None , tokens : List [ str ] = None , text_kwargs : Dict [ str , Any ] = None , optimizer_class : OptimizerBuilderType = torch . optim . AdamW , optimizer_kwargs : Dict [ str , Any ] = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict [ str , Any ] = None , ): \"\"\"Generic finetune module, load any combination of encoder/decoder and custom tokens Args: checkpoint_name: Name of the base checkpoint to load checkpoint_kwargs: Additional kwargs to the checkpoint loading function. decoder_class: Optional class to override the loaded checkpoint. decoder_kwargs: Additional kwargs to the decoder_class. tokens: If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. text_kwargs: Additional kwargs to the text_tranform class, when tokens is not None. optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. \"\"\" self . save_hyperparameters () checkpoint_kwargs = checkpoint_kwargs or {} decoder_kwargs = decoder_kwargs or {} text_kwargs = text_kwargs or {} if tokens is not None and decoder_class is None : # Missing decoder raise ValueError ( \"New tokens were specified, but the module also needs to know the decoder class to initialize properly.\" ) if tokens is None and decoder_class is not None : # Missing tokens raise ValueError ( \"A new decoder was specified, but the module also needs to know the tokens to initialize properly.\" ) checkpoint_data = load_pretrained ( checkpoint_name , ** checkpoint_kwargs ) if decoder_class is None : # Keep original decoder/text processing text_transform = checkpoint_data . text_transform decoder = checkpoint_data . decoder else : # Changing the decoder layer and text processing text_transform = BatchTextTransformer ( tokens , ** text_kwargs ) decoder = decoder_class ( checkpoint_data . encoder_final_dimension , text_transform . num_tokens , ** decoder_kwargs , ) super () . __init__ ( encoder = checkpoint_data . encoder , decoder = decoder , audio_transform = checkpoint_data . audio_transform , text_transform = text_transform , optimizer_class = optimizer_class , optimizer_kwargs = optimizer_kwargs , lr_scheduler_class = lr_scheduler_class , lr_scheduler_kwargs = lr_scheduler_kwargs , )","title":"FinetuneCTCModule"},{"location":"api/finetune/#thunder.finetune.FinetuneCTCModule.__init__","text":"Generic finetune module, load any combination of encoder/decoder and custom tokens Parameters: Name Type Description Default checkpoint_name str Name of the base checkpoint to load required checkpoint_kwargs Dict[str, Any] Additional kwargs to the checkpoint loading function. None decoder_class Union[Type[torch.nn.modules.module.Module], Callable[..., torch.nn.modules.module.Module]] Optional class to override the loaded checkpoint. None decoder_kwargs Dict[str, Any] Additional kwargs to the decoder_class. None tokens List[str] If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. None text_kwargs Dict[str, Any] Additional kwargs to the text_tranform class, when tokens is not None. None optimizer_class Union[Type[torch.optim.optimizer.Optimizer], Callable[..., torch.optim.optimizer.Optimizer]] Optimizer to use during training. <class 'torch.optim.adamw.AdamW'> optimizer_kwargs Dict[str, Any] Optional extra kwargs to the optimizer. None lr_scheduler_class Union[Type[torch.optim.lr_scheduler._LRScheduler], Type[torch.optim.lr_scheduler.ReduceLROnPlateau], Callable[..., Union[torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler.ReduceLROnPlateau]]] Optional class to use a learning rate scheduler with the optimizer. None lr_scheduler_kwargs Dict[str, Any] Optional extra kwargs to the learning rate scheduler. None Source code in thunder/finetune.py def __init__ ( self , checkpoint_name : str , checkpoint_kwargs : Dict [ str , Any ] = None , decoder_class : ModuleBuilderType = None , decoder_kwargs : Dict [ str , Any ] = None , tokens : List [ str ] = None , text_kwargs : Dict [ str , Any ] = None , optimizer_class : OptimizerBuilderType = torch . optim . AdamW , optimizer_kwargs : Dict [ str , Any ] = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict [ str , Any ] = None , ): \"\"\"Generic finetune module, load any combination of encoder/decoder and custom tokens Args: checkpoint_name: Name of the base checkpoint to load checkpoint_kwargs: Additional kwargs to the checkpoint loading function. decoder_class: Optional class to override the loaded checkpoint. decoder_kwargs: Additional kwargs to the decoder_class. tokens: If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. text_kwargs: Additional kwargs to the text_tranform class, when tokens is not None. optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. \"\"\" self . save_hyperparameters () checkpoint_kwargs = checkpoint_kwargs or {} decoder_kwargs = decoder_kwargs or {} text_kwargs = text_kwargs or {} if tokens is not None and decoder_class is None : # Missing decoder raise ValueError ( \"New tokens were specified, but the module also needs to know the decoder class to initialize properly.\" ) if tokens is None and decoder_class is not None : # Missing tokens raise ValueError ( \"A new decoder was specified, but the module also needs to know the tokens to initialize properly.\" ) checkpoint_data = load_pretrained ( checkpoint_name , ** checkpoint_kwargs ) if decoder_class is None : # Keep original decoder/text processing text_transform = checkpoint_data . text_transform decoder = checkpoint_data . decoder else : # Changing the decoder layer and text processing text_transform = BatchTextTransformer ( tokens , ** text_kwargs ) decoder = decoder_class ( checkpoint_data . encoder_final_dimension , text_transform . num_tokens , ** decoder_kwargs , ) super () . __init__ ( encoder = checkpoint_data . encoder , decoder = decoder , audio_transform = checkpoint_data . audio_transform , text_transform = text_transform , optimizer_class = optimizer_class , optimizer_kwargs = optimizer_kwargs , lr_scheduler_class = lr_scheduler_class , lr_scheduler_kwargs = lr_scheduler_kwargs , )","title":"__init__()"},{"location":"api/module/","text":"Base module to train ctc models BaseCTCModule ( LightningModule ) Source code in thunder/module.py class BaseCTCModule ( pl . LightningModule ): def __init__ ( self , encoder : nn . Module , decoder : nn . Module , audio_transform : nn . Module , text_transform : BatchTextTransformer , optimizer_class : OptimizerBuilderType = torch . optim . AdamW , optimizer_kwargs : Dict = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict = None , encoder_final_dimension : int = None , ): \"\"\"Base module for all systems that follow the same CTC training procedure. Args: encoder: Encoder part of the model decoder: Decoder part of the model audio_transform: Transforms raw audio into the features the encoder expects text_transform: Class that encodes and decodes all textual representation optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. encoder_final_dimension: number of features in the encoder output. \"\"\" super () . __init__ () self . encoder = encoder self . decoder = decoder self . audio_transform = audio_transform self . text_transform = text_transform self . optimizer_class = optimizer_class self . optimizer_kwargs = optimizer_kwargs or {} self . lr_scheduler_class = lr_scheduler_class self . lr_scheduler_kwargs = lr_scheduler_kwargs or {} self . lr_scheduler_interval = self . lr_scheduler_kwargs . pop ( \"interval\" , \"step\" ) self . encoder_final_dimension = encoder_final_dimension # Metrics self . validation_cer = CharErrorRate () self . validation_wer = WordErrorRate () self . example_input_array = ( torch . randn (( 10 , 16000 )), torch . randint ( 100 , 16000 , ( 10 ,)), ) def forward ( self , x : Tensor , lengths : Tensor ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Process the audio tensor to create the predictions. Args: x: Audio tensor of shape [batch_size, time] lengths: corresponding length of each element in the input tensor. Returns: Tensor with the predictions. \"\"\" features , feature_lengths = self . audio_transform ( x , lengths ) encoded , out_lengths = self . encoder ( features , feature_lengths ) return self . decoder ( encoded ), out_lengths @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x: Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" audio_lengths = torch . tensor ( x . shape [ 0 ] * [ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , audio_lengths ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, lengths and the corresponding text labels. batch_idx: Batch index Returns: Training loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) self . log ( \"loss/train_loss\" , loss ) return loss def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, lengths and the corresponding text labels. batch_idx: Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y , remove_repeated = False ) self . validation_cer ( decoded_preds , decoded_targets ) self . validation_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . validation_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . validation_wer , on_epoch = True ) return loss def _update_special_optimizer_arg ( self , original_kwargs : Dict ) -> Dict : updated_kwargs = original_kwargs . copy () total_steps_arg = updated_kwargs . pop ( \"total_steps_arg\" , None ) if total_steps_arg : updated_kwargs [ total_steps_arg ] = self . trainer . estimated_stepping_batches return updated_kwargs def configure_optimizers ( self ) -> Union [ torch . optim . Optimizer , Dict [ str , Any ]]: optim_kwargs = self . _update_special_optimizer_arg ( self . optimizer_kwargs ) optimizer = self . optimizer_class ( filter ( lambda p : p . requires_grad , self . parameters ()), ** optim_kwargs ) if not self . lr_scheduler_class : return optimizer scheduler_kwargs = self . _update_special_optimizer_arg ( self . lr_scheduler_kwargs ) lr_scheduler = self . lr_scheduler_class ( optimizer , ** scheduler_kwargs ) return { \"optimizer\" : optimizer , \"lr_scheduler\" : { \"scheduler\" : lr_scheduler , \"interval\" : self . lr_scheduler_interval , }, } __init__ ( self , encoder , decoder , audio_transform , text_transform , optimizer_class =< class ' torch . optim . adamw . AdamW '>, optimizer_kwargs=None, lr_scheduler_class=None, lr_scheduler_kwargs=None, encoder_final_dimension=None) special Base module for all systems that follow the same CTC training procedure. Parameters: Name Type Description Default encoder Module Encoder part of the model required decoder Module Decoder part of the model required audio_transform Module Transforms raw audio into the features the encoder expects required text_transform BatchTextTransformer Class that encodes and decodes all textual representation required optimizer_class Union[Type[torch.optim.optimizer.Optimizer], Callable[..., torch.optim.optimizer.Optimizer]] Optimizer to use during training. <class 'torch.optim.adamw.AdamW'> optimizer_kwargs Dict Optional extra kwargs to the optimizer. None lr_scheduler_class Union[Type[torch.optim.lr_scheduler._LRScheduler], Type[torch.optim.lr_scheduler.ReduceLROnPlateau], Callable[..., Union[torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler.ReduceLROnPlateau]]] Optional class to use a learning rate scheduler with the optimizer. None lr_scheduler_kwargs Dict Optional extra kwargs to the learning rate scheduler. None encoder_final_dimension int number of features in the encoder output. None Source code in thunder/module.py def __init__ ( self , encoder : nn . Module , decoder : nn . Module , audio_transform : nn . Module , text_transform : BatchTextTransformer , optimizer_class : OptimizerBuilderType = torch . optim . AdamW , optimizer_kwargs : Dict = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict = None , encoder_final_dimension : int = None , ): \"\"\"Base module for all systems that follow the same CTC training procedure. Args: encoder: Encoder part of the model decoder: Decoder part of the model audio_transform: Transforms raw audio into the features the encoder expects text_transform: Class that encodes and decodes all textual representation optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. encoder_final_dimension: number of features in the encoder output. \"\"\" super () . __init__ () self . encoder = encoder self . decoder = decoder self . audio_transform = audio_transform self . text_transform = text_transform self . optimizer_class = optimizer_class self . optimizer_kwargs = optimizer_kwargs or {} self . lr_scheduler_class = lr_scheduler_class self . lr_scheduler_kwargs = lr_scheduler_kwargs or {} self . lr_scheduler_interval = self . lr_scheduler_kwargs . pop ( \"interval\" , \"step\" ) self . encoder_final_dimension = encoder_final_dimension # Metrics self . validation_cer = CharErrorRate () self . validation_wer = WordErrorRate () self . example_input_array = ( torch . randn (( 10 , 16000 )), torch . randint ( 100 , 16000 , ( 10 ,)), ) configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Returns: Type Description Union[torch.optim.optimizer.Optimizer, Dict[str, Any]] Any of these 6 options. Single optimizer . List or Tuple of optimizers. Two lists - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple lr_scheduler_config ). Dictionary , with an \"optimizer\" key, and (optionally) a \"lr_scheduler\" key whose value is a single LR scheduler or lr_scheduler_config . Tuple of dictionaries as described above, with an optional \"frequency\" key. None - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the :class: torch.optim.lr_scheduler.ReduceLROnPlateau scheduler, Lightning requires that the lr_scheduler_config contains the keyword \"monitor\" set to the metric name that the scheduler should be conditioned on. .. testcode:: # The ReduceLROnPlateau scheduler requires a monitor def configure_optimizers(self): optimizer = Adam(...) return { \"optimizer\": optimizer, \"lr_scheduler\": { \"scheduler\": ReduceLROnPlateau(optimizer, ...), \"monitor\": \"metric_to_track\", \"frequency\": \"indicates how often the metric is updated\" # If \"monitor\" references validation metrics, then \"frequency\" should be set to a # multiple of \"trainer.check_val_every_n_epoch\". }, } # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler def configure_optimizers(self): optimizer1 = Adam(...) optimizer2 = SGD(...) scheduler1 = ReduceLROnPlateau(optimizer1, ...) scheduler2 = LambdaLR(optimizer2, ...) return ( { \"optimizer\": optimizer1, \"lr_scheduler\": { \"scheduler\": scheduler1, \"monitor\": \"metric_to_track\", }, }, {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2}, ) Metrics can be made available to monitor by simply logging it using self.log('metric_to_track', metric_val) in your :class: ~pytorch_lightning.core.lightning.LightningModule . Note The frequency value specified in a dict along with the optimizer key is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: - In the former case, all optimizers will operate on the given batch in each optimization step. - In the latter, only one optimizer will operate on the given batch at every step. This is different from the frequency value specified in the lr_scheduler_config mentioned above. .. code-block:: python def configure_optimizers(self): optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01) optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01) return [ {\"optimizer\": optimizer_one, \"frequency\": 5}, {\"optimizer\": optimizer_two, \"frequency\": 10}, ] In this example, the first optimizer will be used for the first 5 steps, the second optimizer for the next 10 steps and that cycle will continue. If an LR scheduler is specified for an optimizer using the lr_scheduler key in the above dict, the scheduler will only be updated when its optimizer is being used. Examples:: # most cases. no learning rate scheduler def configure_optimizers(self): return Adam(self.parameters(), lr=1e-3) # multiple optimizer case (e.g.: GAN) def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) return gen_opt, dis_opt # example with learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) dis_sch = CosineAnnealing(dis_opt, T_max=10) return [gen_opt, dis_opt], [dis_sch] # example with step-based learning rate schedulers # each optimizer has its own scheduler def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) gen_sch = { 'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step' # called after each training step } dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch return [gen_opt, dis_opt], [gen_sch, dis_sch] # example with optimizer frequencies # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1 # https://arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note Some things to know: Lightning calls .backward() and .step() on each optimizer and learning rate scheduler as needed. If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers. If you use multiple optimizers, :meth: training_step will have an additional optimizer_idx parameter. If you use :class: torch.optim.LBFGS , Lightning handles the closure function automatically for you. If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. If you need to control how often those optimizers step or override the default .step() schedule, override the :meth: optimizer_step hook. Source code in thunder/module.py def configure_optimizers ( self ) -> Union [ torch . optim . Optimizer , Dict [ str , Any ]]: optim_kwargs = self . _update_special_optimizer_arg ( self . optimizer_kwargs ) optimizer = self . optimizer_class ( filter ( lambda p : p . requires_grad , self . parameters ()), ** optim_kwargs ) if not self . lr_scheduler_class : return optimizer scheduler_kwargs = self . _update_special_optimizer_arg ( self . lr_scheduler_kwargs ) lr_scheduler = self . lr_scheduler_class ( optimizer , ** scheduler_kwargs ) return { \"optimizer\" : optimizer , \"lr_scheduler\" : { \"scheduler\" : lr_scheduler , \"interval\" : self . lr_scheduler_interval , }, } forward ( self , x , lengths ) Process the audio tensor to create the predictions. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required lengths Tensor corresponding length of each element in the input tensor. required Returns: Type Description Tuple[torch.Tensor, Optional[torch.Tensor]] Tensor with the predictions. Source code in thunder/module.py def forward ( self , x : Tensor , lengths : Tensor ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Process the audio tensor to create the predictions. Args: x: Audio tensor of shape [batch_size, time] lengths: corresponding length of each element in the input tensor. Returns: Tensor with the predictions. \"\"\" features , feature_lengths = self . audio_transform ( x , lengths ) encoded , out_lengths = self . encoder ( features , feature_lengths ) return self . decoder ( encoded ), out_lengths predict ( self , x ) Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x: Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" audio_lengths = torch . tensor ( x . shape [ 0 ] * [ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , audio_lengths ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) training_step ( self , batch , batch_idx ) Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, lengths and the corresponding text labels. batch_idx: Batch index Returns: Training loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) self . log ( \"loss/train_loss\" , loss ) return loss validation_step ( self , batch , batch_idx ) Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, lengths and the corresponding text labels. batch_idx: Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y , remove_repeated = False ) self . validation_cer ( decoded_preds , decoded_targets ) self . validation_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . validation_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . validation_wer , on_epoch = True ) return loss","title":"Module"},{"location":"api/module/#thunder.module.BaseCTCModule","text":"Source code in thunder/module.py class BaseCTCModule ( pl . LightningModule ): def __init__ ( self , encoder : nn . Module , decoder : nn . Module , audio_transform : nn . Module , text_transform : BatchTextTransformer , optimizer_class : OptimizerBuilderType = torch . optim . AdamW , optimizer_kwargs : Dict = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict = None , encoder_final_dimension : int = None , ): \"\"\"Base module for all systems that follow the same CTC training procedure. Args: encoder: Encoder part of the model decoder: Decoder part of the model audio_transform: Transforms raw audio into the features the encoder expects text_transform: Class that encodes and decodes all textual representation optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. encoder_final_dimension: number of features in the encoder output. \"\"\" super () . __init__ () self . encoder = encoder self . decoder = decoder self . audio_transform = audio_transform self . text_transform = text_transform self . optimizer_class = optimizer_class self . optimizer_kwargs = optimizer_kwargs or {} self . lr_scheduler_class = lr_scheduler_class self . lr_scheduler_kwargs = lr_scheduler_kwargs or {} self . lr_scheduler_interval = self . lr_scheduler_kwargs . pop ( \"interval\" , \"step\" ) self . encoder_final_dimension = encoder_final_dimension # Metrics self . validation_cer = CharErrorRate () self . validation_wer = WordErrorRate () self . example_input_array = ( torch . randn (( 10 , 16000 )), torch . randint ( 100 , 16000 , ( 10 ,)), ) def forward ( self , x : Tensor , lengths : Tensor ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Process the audio tensor to create the predictions. Args: x: Audio tensor of shape [batch_size, time] lengths: corresponding length of each element in the input tensor. Returns: Tensor with the predictions. \"\"\" features , feature_lengths = self . audio_transform ( x , lengths ) encoded , out_lengths = self . encoder ( features , feature_lengths ) return self . decoder ( encoded ), out_lengths @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x: Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" audio_lengths = torch . tensor ( x . shape [ 0 ] * [ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , audio_lengths ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, lengths and the corresponding text labels. batch_idx: Batch index Returns: Training loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) self . log ( \"loss/train_loss\" , loss ) return loss def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, lengths and the corresponding text labels. batch_idx: Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y , remove_repeated = False ) self . validation_cer ( decoded_preds , decoded_targets ) self . validation_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . validation_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . validation_wer , on_epoch = True ) return loss def _update_special_optimizer_arg ( self , original_kwargs : Dict ) -> Dict : updated_kwargs = original_kwargs . copy () total_steps_arg = updated_kwargs . pop ( \"total_steps_arg\" , None ) if total_steps_arg : updated_kwargs [ total_steps_arg ] = self . trainer . estimated_stepping_batches return updated_kwargs def configure_optimizers ( self ) -> Union [ torch . optim . Optimizer , Dict [ str , Any ]]: optim_kwargs = self . _update_special_optimizer_arg ( self . optimizer_kwargs ) optimizer = self . optimizer_class ( filter ( lambda p : p . requires_grad , self . parameters ()), ** optim_kwargs ) if not self . lr_scheduler_class : return optimizer scheduler_kwargs = self . _update_special_optimizer_arg ( self . lr_scheduler_kwargs ) lr_scheduler = self . lr_scheduler_class ( optimizer , ** scheduler_kwargs ) return { \"optimizer\" : optimizer , \"lr_scheduler\" : { \"scheduler\" : lr_scheduler , \"interval\" : self . lr_scheduler_interval , }, }","title":"BaseCTCModule"},{"location":"api/module/#thunder.module.BaseCTCModule.__init__","text":"Base module for all systems that follow the same CTC training procedure. Parameters: Name Type Description Default encoder Module Encoder part of the model required decoder Module Decoder part of the model required audio_transform Module Transforms raw audio into the features the encoder expects required text_transform BatchTextTransformer Class that encodes and decodes all textual representation required optimizer_class Union[Type[torch.optim.optimizer.Optimizer], Callable[..., torch.optim.optimizer.Optimizer]] Optimizer to use during training. <class 'torch.optim.adamw.AdamW'> optimizer_kwargs Dict Optional extra kwargs to the optimizer. None lr_scheduler_class Union[Type[torch.optim.lr_scheduler._LRScheduler], Type[torch.optim.lr_scheduler.ReduceLROnPlateau], Callable[..., Union[torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler.ReduceLROnPlateau]]] Optional class to use a learning rate scheduler with the optimizer. None lr_scheduler_kwargs Dict Optional extra kwargs to the learning rate scheduler. None encoder_final_dimension int number of features in the encoder output. None Source code in thunder/module.py def __init__ ( self , encoder : nn . Module , decoder : nn . Module , audio_transform : nn . Module , text_transform : BatchTextTransformer , optimizer_class : OptimizerBuilderType = torch . optim . AdamW , optimizer_kwargs : Dict = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict = None , encoder_final_dimension : int = None , ): \"\"\"Base module for all systems that follow the same CTC training procedure. Args: encoder: Encoder part of the model decoder: Decoder part of the model audio_transform: Transforms raw audio into the features the encoder expects text_transform: Class that encodes and decodes all textual representation optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. encoder_final_dimension: number of features in the encoder output. \"\"\" super () . __init__ () self . encoder = encoder self . decoder = decoder self . audio_transform = audio_transform self . text_transform = text_transform self . optimizer_class = optimizer_class self . optimizer_kwargs = optimizer_kwargs or {} self . lr_scheduler_class = lr_scheduler_class self . lr_scheduler_kwargs = lr_scheduler_kwargs or {} self . lr_scheduler_interval = self . lr_scheduler_kwargs . pop ( \"interval\" , \"step\" ) self . encoder_final_dimension = encoder_final_dimension # Metrics self . validation_cer = CharErrorRate () self . validation_wer = WordErrorRate () self . example_input_array = ( torch . randn (( 10 , 16000 )), torch . randint ( 100 , 16000 , ( 10 ,)), )","title":"__init__()"},{"location":"api/module/#thunder.module.BaseCTCModule.configure_optimizers","text":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Returns: Type Description Union[torch.optim.optimizer.Optimizer, Dict[str, Any]] Any of these 6 options. Single optimizer . List or Tuple of optimizers. Two lists - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple lr_scheduler_config ). Dictionary , with an \"optimizer\" key, and (optionally) a \"lr_scheduler\" key whose value is a single LR scheduler or lr_scheduler_config . Tuple of dictionaries as described above, with an optional \"frequency\" key. None - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the :class: torch.optim.lr_scheduler.ReduceLROnPlateau scheduler, Lightning requires that the lr_scheduler_config contains the keyword \"monitor\" set to the metric name that the scheduler should be conditioned on. .. testcode:: # The ReduceLROnPlateau scheduler requires a monitor def configure_optimizers(self): optimizer = Adam(...) return { \"optimizer\": optimizer, \"lr_scheduler\": { \"scheduler\": ReduceLROnPlateau(optimizer, ...), \"monitor\": \"metric_to_track\", \"frequency\": \"indicates how often the metric is updated\" # If \"monitor\" references validation metrics, then \"frequency\" should be set to a # multiple of \"trainer.check_val_every_n_epoch\". }, } # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler def configure_optimizers(self): optimizer1 = Adam(...) optimizer2 = SGD(...) scheduler1 = ReduceLROnPlateau(optimizer1, ...) scheduler2 = LambdaLR(optimizer2, ...) return ( { \"optimizer\": optimizer1, \"lr_scheduler\": { \"scheduler\": scheduler1, \"monitor\": \"metric_to_track\", }, }, {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2}, ) Metrics can be made available to monitor by simply logging it using self.log('metric_to_track', metric_val) in your :class: ~pytorch_lightning.core.lightning.LightningModule . Note The frequency value specified in a dict along with the optimizer key is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: - In the former case, all optimizers will operate on the given batch in each optimization step. - In the latter, only one optimizer will operate on the given batch at every step. This is different from the frequency value specified in the lr_scheduler_config mentioned above. .. code-block:: python def configure_optimizers(self): optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01) optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01) return [ {\"optimizer\": optimizer_one, \"frequency\": 5}, {\"optimizer\": optimizer_two, \"frequency\": 10}, ] In this example, the first optimizer will be used for the first 5 steps, the second optimizer for the next 10 steps and that cycle will continue. If an LR scheduler is specified for an optimizer using the lr_scheduler key in the above dict, the scheduler will only be updated when its optimizer is being used. Examples:: # most cases. no learning rate scheduler def configure_optimizers(self): return Adam(self.parameters(), lr=1e-3) # multiple optimizer case (e.g.: GAN) def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) return gen_opt, dis_opt # example with learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) dis_sch = CosineAnnealing(dis_opt, T_max=10) return [gen_opt, dis_opt], [dis_sch] # example with step-based learning rate schedulers # each optimizer has its own scheduler def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) gen_sch = { 'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step' # called after each training step } dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch return [gen_opt, dis_opt], [gen_sch, dis_sch] # example with optimizer frequencies # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1 # https://arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note Some things to know: Lightning calls .backward() and .step() on each optimizer and learning rate scheduler as needed. If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers. If you use multiple optimizers, :meth: training_step will have an additional optimizer_idx parameter. If you use :class: torch.optim.LBFGS , Lightning handles the closure function automatically for you. If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. If you need to control how often those optimizers step or override the default .step() schedule, override the :meth: optimizer_step hook. Source code in thunder/module.py def configure_optimizers ( self ) -> Union [ torch . optim . Optimizer , Dict [ str , Any ]]: optim_kwargs = self . _update_special_optimizer_arg ( self . optimizer_kwargs ) optimizer = self . optimizer_class ( filter ( lambda p : p . requires_grad , self . parameters ()), ** optim_kwargs ) if not self . lr_scheduler_class : return optimizer scheduler_kwargs = self . _update_special_optimizer_arg ( self . lr_scheduler_kwargs ) lr_scheduler = self . lr_scheduler_class ( optimizer , ** scheduler_kwargs ) return { \"optimizer\" : optimizer , \"lr_scheduler\" : { \"scheduler\" : lr_scheduler , \"interval\" : self . lr_scheduler_interval , }, }","title":"configure_optimizers()"},{"location":"api/module/#thunder.module.BaseCTCModule.forward","text":"Process the audio tensor to create the predictions. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required lengths Tensor corresponding length of each element in the input tensor. required Returns: Type Description Tuple[torch.Tensor, Optional[torch.Tensor]] Tensor with the predictions. Source code in thunder/module.py def forward ( self , x : Tensor , lengths : Tensor ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Process the audio tensor to create the predictions. Args: x: Audio tensor of shape [batch_size, time] lengths: corresponding length of each element in the input tensor. Returns: Tensor with the predictions. \"\"\" features , feature_lengths = self . audio_transform ( x , lengths ) encoded , out_lengths = self . encoder ( features , feature_lengths ) return self . decoder ( encoded ), out_lengths","title":"forward()"},{"location":"api/module/#thunder.module.BaseCTCModule.predict","text":"Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x: Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" audio_lengths = torch . tensor ( x . shape [ 0 ] * [ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , audio_lengths ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/module/#thunder.module.BaseCTCModule.training_step","text":"Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, lengths and the corresponding text labels. batch_idx: Batch index Returns: Training loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) self . log ( \"loss/train_loss\" , loss ) return loss","title":"training_step()"},{"location":"api/module/#thunder.module.BaseCTCModule.validation_step","text":"Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, lengths and the corresponding text labels. batch_idx: Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y , remove_repeated = False ) self . validation_cer ( decoded_preds , decoded_targets ) self . validation_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . validation_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . validation_wer , on_epoch = True ) return loss","title":"validation_step()"},{"location":"api/registry/","text":"Functionality to register the multiple checkpoints and provide a unified loading interface. load_pretrained ( checkpoint_name , ** load_kwargs ) Load data from any registered checkpoint Parameters: Name Type Description Default checkpoint_name Union[str, thunder.utils.BaseCheckpoint] Base checkpoint name, like \"QuartzNet5x5LS_En\" or \"facebook/wav2vec2-large-960h\" required Returns: Type Description BaseCTCModule Object containing the checkpoint data (encoder, decoder, transforms and additional data). Source code in thunder/registry.py def load_pretrained ( checkpoint_name : Union [ str , BaseCheckpoint ], ** load_kwargs ) -> BaseCTCModule : \"\"\"Load data from any registered checkpoint Args: checkpoint_name: Base checkpoint name, like \"QuartzNet5x5LS_En\" or \"facebook/wav2vec2-large-960h\" Returns: Object containing the checkpoint data (encoder, decoder, transforms and additional data). \"\"\" if isinstance ( checkpoint_name , BaseCheckpoint ): checkpoint_name = checkpoint_name . name # Special case when dealing with any huggingface model if \"/\" in checkpoint_name : model_data = load_huggingface_checkpoint ( checkpoint_name , ** load_kwargs ) else : load_fn = CHECKPOINT_REGISTRY [ checkpoint_name ] model_data = load_fn ( ** load_kwargs ) return model_data register_checkpoint_enum ( checkpoints , load_function ) Register all variations of some checkpoint enum with the corresponding loading function Parameters: Name Type Description Default checkpoints Type[thunder.utils.BaseCheckpoint] Base checkpoint class required load_function Callable[..., thunder.module.BaseCTCModule] function to load the checkpoint, must receive one instance of checkpoints as first argument required Source code in thunder/registry.py def register_checkpoint_enum ( checkpoints : Type [ BaseCheckpoint ], load_function : CHECKPOINT_LOAD_FUNC_TYPE ): \"\"\"Register all variations of some checkpoint enum with the corresponding loading function Args: checkpoints: Base checkpoint class load_function: function to load the checkpoint, must receive one instance of `checkpoints` as first argument\"\"\" for checkpoint in checkpoints : CHECKPOINT_REGISTRY . update ( { checkpoint . name : partial ( load_function , checkpoint )} )","title":"Registry"},{"location":"api/registry/#thunder.registry.load_pretrained","text":"Load data from any registered checkpoint Parameters: Name Type Description Default checkpoint_name Union[str, thunder.utils.BaseCheckpoint] Base checkpoint name, like \"QuartzNet5x5LS_En\" or \"facebook/wav2vec2-large-960h\" required Returns: Type Description BaseCTCModule Object containing the checkpoint data (encoder, decoder, transforms and additional data). Source code in thunder/registry.py def load_pretrained ( checkpoint_name : Union [ str , BaseCheckpoint ], ** load_kwargs ) -> BaseCTCModule : \"\"\"Load data from any registered checkpoint Args: checkpoint_name: Base checkpoint name, like \"QuartzNet5x5LS_En\" or \"facebook/wav2vec2-large-960h\" Returns: Object containing the checkpoint data (encoder, decoder, transforms and additional data). \"\"\" if isinstance ( checkpoint_name , BaseCheckpoint ): checkpoint_name = checkpoint_name . name # Special case when dealing with any huggingface model if \"/\" in checkpoint_name : model_data = load_huggingface_checkpoint ( checkpoint_name , ** load_kwargs ) else : load_fn = CHECKPOINT_REGISTRY [ checkpoint_name ] model_data = load_fn ( ** load_kwargs ) return model_data","title":"load_pretrained()"},{"location":"api/registry/#thunder.registry.register_checkpoint_enum","text":"Register all variations of some checkpoint enum with the corresponding loading function Parameters: Name Type Description Default checkpoints Type[thunder.utils.BaseCheckpoint] Base checkpoint class required load_function Callable[..., thunder.module.BaseCTCModule] function to load the checkpoint, must receive one instance of checkpoints as first argument required Source code in thunder/registry.py def register_checkpoint_enum ( checkpoints : Type [ BaseCheckpoint ], load_function : CHECKPOINT_LOAD_FUNC_TYPE ): \"\"\"Register all variations of some checkpoint enum with the corresponding loading function Args: checkpoints: Base checkpoint class load_function: function to load the checkpoint, must receive one instance of `checkpoints` as first argument\"\"\" for checkpoint in checkpoints : CHECKPOINT_REGISTRY . update ( { checkpoint . name : partial ( load_function , checkpoint )} )","title":"register_checkpoint_enum()"},{"location":"api/utils/","text":"Utility functions BaseCheckpoint ( str , Enum ) Base class that represents a pretrained model checkpoint. Source code in thunder/utils.py class BaseCheckpoint ( str , Enum ): \"\"\"Base class that represents a pretrained model checkpoint.\"\"\" @classmethod def from_string ( cls , name : str ) -> \"BaseCheckpoint\" : \"\"\"Creates enum value from string. Helper to use with argparse/hydra Args: name: Name of the checkpoint Raises: ValueError: Name provided is not a valid checkpoint Returns: Enum value corresponding to the name \"\"\" try : return cls [ name ] except KeyError as option_does_not_exist : raise ValueError ( \"Name provided is not a valid checkpoint\" ) from option_does_not_exist audio_len ( item ) Returns the length of the audio file Parameters: Name Type Description Default item Union[pathlib.Path, str] Audio path required Returns: Type Description float Lenght in seconds of the audio Source code in thunder/utils.py def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item: Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate chain_calls ( * funcs ) Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Examples: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 Returns: Type Description Callable Single chained function Source code in thunder/utils.py def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner download_checkpoint ( name , checkpoint_folder = None ) Download checkpoint by identifier. Parameters: Name Type Description Default name BaseCheckpoint Model identifier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. None Returns: Type Description Path Path to the saved checkpoint file. Source code in thunder/utils.py def download_checkpoint ( name : BaseCheckpoint , checkpoint_folder : str = None ) -> Path : \"\"\"Download checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = name . value filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path get_default_cache_folder () Get the default folder where the cached stuff will be saved. Returns: Type Description Path Path of the cache folder. Source code in thunder/utils.py def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder get_files ( directory , extension ) Find all files in directory with extension. Parameters: Name Type Description Default directory Union[str, pathlib.Path] Directory to recursively find the files required extension str File extension to search for required Returns: Type Description List[pathlib.Path] List of all the files that match the extension Source code in thunder/utils.py def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory: Directory to recursively find the files extension: File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"Utils"},{"location":"api/utils/#thunder.utils.BaseCheckpoint","text":"Base class that represents a pretrained model checkpoint. Source code in thunder/utils.py class BaseCheckpoint ( str , Enum ): \"\"\"Base class that represents a pretrained model checkpoint.\"\"\" @classmethod def from_string ( cls , name : str ) -> \"BaseCheckpoint\" : \"\"\"Creates enum value from string. Helper to use with argparse/hydra Args: name: Name of the checkpoint Raises: ValueError: Name provided is not a valid checkpoint Returns: Enum value corresponding to the name \"\"\" try : return cls [ name ] except KeyError as option_does_not_exist : raise ValueError ( \"Name provided is not a valid checkpoint\" ) from option_does_not_exist","title":"BaseCheckpoint"},{"location":"api/utils/#thunder.utils.audio_len","text":"Returns the length of the audio file Parameters: Name Type Description Default item Union[pathlib.Path, str] Audio path required Returns: Type Description float Lenght in seconds of the audio Source code in thunder/utils.py def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item: Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate","title":"audio_len()"},{"location":"api/utils/#thunder.utils.chain_calls","text":"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Examples: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 Returns: Type Description Callable Single chained function Source code in thunder/utils.py def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner","title":"chain_calls()"},{"location":"api/utils/#thunder.utils.download_checkpoint","text":"Download checkpoint by identifier. Parameters: Name Type Description Default name BaseCheckpoint Model identifier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. None Returns: Type Description Path Path to the saved checkpoint file. Source code in thunder/utils.py def download_checkpoint ( name : BaseCheckpoint , checkpoint_folder : str = None ) -> Path : \"\"\"Download checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = name . value filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path","title":"download_checkpoint()"},{"location":"api/utils/#thunder.utils.get_default_cache_folder","text":"Get the default folder where the cached stuff will be saved. Returns: Type Description Path Path of the cache folder. Source code in thunder/utils.py def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder","title":"get_default_cache_folder()"},{"location":"api/utils/#thunder.utils.get_files","text":"Find all files in directory with extension. Parameters: Name Type Description Default directory Union[str, pathlib.Path] Directory to recursively find the files required extension str File extension to search for required Returns: Type Description List[pathlib.Path] List of all the files that match the extension Source code in thunder/utils.py def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory: Directory to recursively find the files extension: File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"get_files()"},{"location":"api/Citrinet/blocks/","text":"Basic building blocks to create the Citrinet model CitrinetBlock ( Module ) Source code in thunder/citrinet/blocks.py class CitrinetBlock ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( Masked ( SqueezeExcite ( out_channels , reduction_ratio = 8 ))) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes lengths: corresponding length of each element in the input tensor. Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out __init__ ( self , in_channels , out_channels , repeat = 5 , kernel_size = ( 11 ,), stride = ( 1 ,), dilation = ( 1 ,), dropout = 0.0 , residual = True , separable = False ) special Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size Union[int, Tuple[int]] Kernel size. (11,) stride Union[int, Tuple[int]] Stride of each repetition. (1,) dilation Union[int, Tuple[int]] Dilation of each repetition. (1,) dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/citrinet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( Masked ( SqueezeExcite ( out_channels , reduction_ratio = 8 ))) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) forward ( self , x , lengths ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required lengths Tensor corresponding length of each element in the input tensor. required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Result of applying the block on the input, and corresponding output lengths Source code in thunder/citrinet/blocks.py def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes lengths: corresponding length of each element in the input tensor. Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out SqueezeExcite ( Module ) Source code in thunder/citrinet/blocks.py class SqueezeExcite ( nn . Module ): def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super () . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y __init__ ( self , channels , reduction_ratio ) special Squeeze-and-Excitation sub-module. Parameters: Name Type Description Default channels int Input number of channels. required reduction_ratio int Reduction ratio for \"squeeze\" layer. required Source code in thunder/citrinet/blocks.py def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super () . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), ) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape [batch, channels, time] required Returns: Type Description Tensor Tensor of shape [batch, channels, time] Source code in thunder/citrinet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y CitrinetEncoder ( filters , kernel_sizes , strides , feat_in = 80 ) Basic Citrinet encoder setup. Parameters: Name Type Description Default filters List[int] List of filter sizes used to create the encoder blocks. required kernel_sizes List[int] List of kernel sizes corresponding to each filter size. required strides List[int] List of stride corresponding to each filter size. required feat_in int Number of input features to the model. 80 Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/citrinet/blocks.py def CitrinetEncoder ( filters : List [ int ], kernel_sizes : List [ int ], strides : List [ int ], feat_in : int = 80 ) -> nn . Module : \"\"\"Basic Citrinet encoder setup. Args: filters: List of filter sizes used to create the encoder blocks. kernel_sizes: List of kernel sizes corresponding to each filter size. strides: List of stride corresponding to each filter size. feat_in: Number of input features to the model. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , strides ), ) body ( filters , kernel_size , strides ) Creates the body of the Citrinet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required strides List[int] Corresponding list of strides for each block. Should have the same length as the first argument. required Returns: Type Description List[thunder.citrinet.blocks.CitrinetBlock] List of layers that form the body of the network. Source code in thunder/citrinet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], strides : List [ int ], ) -> List [ CitrinetBlock ]: \"\"\"Creates the body of the Citrinet model. That is the middle part. Args: filters: List of filters inside each block in the body. kernel_size: Corresponding list of kernel sizes for each block. Should have the same length as the first argument. strides: Corresponding list of strides for each block. Should have the same length as the first argument. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k , s in zip ( filters , kernel_size , strides ): layers . append ( CitrinetBlock ( f_in , f , kernel_size = ( k ,), stride = ( s ,), separable = True ) ) f_in = f layers . append ( CitrinetBlock ( f_in , 640 , repeat = 1 , kernel_size = ( 41 ,), residual = False , separable = True , ) ) return layers stem ( feat_in ) Creates the Citrinet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description CitrinetBlock Citrinet stem block Source code in thunder/citrinet/blocks.py def stem ( feat_in : int ) -> CitrinetBlock : \"\"\"Creates the Citrinet stem. That is the first block of the model, that process the input directly. Args: feat_in: Number of input features Returns: Citrinet stem block \"\"\" return CitrinetBlock ( feat_in , 256 , repeat = 1 , kernel_size = ( 5 ,), residual = False , separable = True , )","title":"Blocks"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock","text":"Source code in thunder/citrinet/blocks.py class CitrinetBlock ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( Masked ( SqueezeExcite ( out_channels , reduction_ratio = 8 ))) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes lengths: corresponding length of each element in the input tensor. Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out","title":"CitrinetBlock"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock.__init__","text":"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size Union[int, Tuple[int]] Kernel size. (11,) stride Union[int, Tuple[int]] Stride of each repetition. (1,) dilation Union[int, Tuple[int]] Dilation of each repetition. (1,) dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/citrinet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( Masked ( SqueezeExcite ( out_channels , reduction_ratio = 8 ))) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required lengths Tensor corresponding length of each element in the input tensor. required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Result of applying the block on the input, and corresponding output lengths Source code in thunder/citrinet/blocks.py def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes lengths: corresponding length of each element in the input tensor. Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out","title":"forward()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite","text":"Source code in thunder/citrinet/blocks.py class SqueezeExcite ( nn . Module ): def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super () . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y","title":"SqueezeExcite"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite.__init__","text":"Squeeze-and-Excitation sub-module. Parameters: Name Type Description Default channels int Input number of channels. required reduction_ratio int Reduction ratio for \"squeeze\" layer. required Source code in thunder/citrinet/blocks.py def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super () . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), )","title":"__init__()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape [batch, channels, time] required Returns: Type Description Tensor Tensor of shape [batch, channels, time] Source code in thunder/citrinet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y","title":"forward()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetEncoder","text":"Basic Citrinet encoder setup. Parameters: Name Type Description Default filters List[int] List of filter sizes used to create the encoder blocks. required kernel_sizes List[int] List of kernel sizes corresponding to each filter size. required strides List[int] List of stride corresponding to each filter size. required feat_in int Number of input features to the model. 80 Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/citrinet/blocks.py def CitrinetEncoder ( filters : List [ int ], kernel_sizes : List [ int ], strides : List [ int ], feat_in : int = 80 ) -> nn . Module : \"\"\"Basic Citrinet encoder setup. Args: filters: List of filter sizes used to create the encoder blocks. kernel_sizes: List of kernel sizes corresponding to each filter size. strides: List of stride corresponding to each filter size. feat_in: Number of input features to the model. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , strides ), )","title":"CitrinetEncoder()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.body","text":"Creates the body of the Citrinet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required strides List[int] Corresponding list of strides for each block. Should have the same length as the first argument. required Returns: Type Description List[thunder.citrinet.blocks.CitrinetBlock] List of layers that form the body of the network. Source code in thunder/citrinet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], strides : List [ int ], ) -> List [ CitrinetBlock ]: \"\"\"Creates the body of the Citrinet model. That is the middle part. Args: filters: List of filters inside each block in the body. kernel_size: Corresponding list of kernel sizes for each block. Should have the same length as the first argument. strides: Corresponding list of strides for each block. Should have the same length as the first argument. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k , s in zip ( filters , kernel_size , strides ): layers . append ( CitrinetBlock ( f_in , f , kernel_size = ( k ,), stride = ( s ,), separable = True ) ) f_in = f layers . append ( CitrinetBlock ( f_in , 640 , repeat = 1 , kernel_size = ( 41 ,), residual = False , separable = True , ) ) return layers","title":"body()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.stem","text":"Creates the Citrinet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description CitrinetBlock Citrinet stem block Source code in thunder/citrinet/blocks.py def stem ( feat_in : int ) -> CitrinetBlock : \"\"\"Creates the Citrinet stem. That is the first block of the model, that process the input directly. Args: feat_in: Number of input features Returns: Citrinet stem block \"\"\" return CitrinetBlock ( feat_in , 256 , repeat = 1 , kernel_size = ( 5 ,), residual = False , separable = True , )","title":"stem()"},{"location":"api/Citrinet/compatibility/","text":"Helper functions to load the Citrinet model from original Nemo released checkpoint files. CitrinetCheckpoint ( BaseCheckpoint ) Trained model weight checkpoints. Used by download_checkpoint and load_citrinet_checkpoint . Note Possible values are stt_en_citrinet_256 , stt_en_citrinet_512 , stt_en_citrinet_1024 , stt_es_citrinet_512 Source code in thunder/citrinet/compatibility.py class CitrinetCheckpoint ( BaseCheckpoint ): \"\"\"Trained model weight checkpoints. Used by [`download_checkpoint`][thunder.utils.download_checkpoint] and [`load_citrinet_checkpoint`][thunder.citrinet.compatibility.load_citrinet_checkpoint]. Note: Possible values are `stt_en_citrinet_256`,`stt_en_citrinet_512`,`stt_en_citrinet_1024`, `stt_es_citrinet_512` \"\"\" stt_en_citrinet_256 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_256/versions/1.0.0rc1/files/stt_en_citrinet_256.nemo\" stt_en_citrinet_512 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_512/versions/1.0.0rc1/files/stt_en_citrinet_512.nemo\" stt_en_citrinet_1024 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_1024/versions/1.0.0rc1/files/stt_en_citrinet_1024.nemo\" stt_es_citrinet_512 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_citrinet_512/versions/1.0.0/files/stt_es_citrinet_512.nemo\" fix_vocab ( vocab_tokens ) Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Parameters: Name Type Description Default vocab_tokens List[str] List of tokens in the vocabulary required Returns: Type Description List[str] New list of tokens with the new prefix Source code in thunder/citrinet/compatibility.py def fix_vocab ( vocab_tokens : List [ str ]) -> List [ str ]: \"\"\"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Args: vocab_tokens: List of tokens in the vocabulary Returns: New list of tokens with the new prefix \"\"\" out_tokens = [] for token in vocab_tokens : if token . startswith ( \"##\" ): out_tokens . append ( token [ 2 :]) else : out_tokens . append ( \"\u2581\" + token ) return out_tokens load_citrinet_checkpoint ( checkpoint , save_folder = None ) Load from the original nemo checkpoint. Parameters: Name Type Description Default checkpoint Union[str, thunder.citrinet.compatibility.CitrinetCheckpoint] Path to local .nemo file or checkpoint to be downloaded locally and lodaded. required save_folder str Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. None Returns: Type Description BaseCTCModule The model loaded from the checkpoint Source code in thunder/citrinet/compatibility.py def load_citrinet_checkpoint ( checkpoint : Union [ str , CitrinetCheckpoint ], save_folder : str = None ) -> BaseCTCModule : \"\"\"Load from the original nemo checkpoint. Args: checkpoint: Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder: Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , CitrinetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = checkpoint with TemporaryDirectory () as extract_folder : extract_archive ( str ( nemo_filepath ), extract_folder ) extract_path = Path ( extract_folder ) config_path = extract_path / \"model_config.yaml\" sentencepiece_path = str ( extract_path / \"tokenizer.model\" ) ( encoder , audio_transform , text_transform , ) = load_components_from_citrinet_config ( config_path , sentencepiece_path ) decoder = conv1d_decoder ( 640 , num_classes = text_transform . num_tokens ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( encoder , decoder , str ( weights_path )) module = BaseCTCModule ( encoder , decoder , audio_transform , text_transform , encoder_final_dimension = 640 , ) return module . eval () load_components_from_citrinet_config ( config_path , sentencepiece_path ) Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path Union[str, pathlib.Path] Path to the .yaml file, usually called model_config.yaml required sentencepiece_path Union[str, pathlib.Path] Path to the sentencepiece model used to tokenize, usually called tokenizer.model required Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module, thunder.text_processing.transform.BatchTextTransformer] A tuple containing, in this order, the encoder, the audio transform and the text transform Source code in thunder/citrinet/compatibility.py def load_components_from_citrinet_config ( config_path : Union [ str , Path ], sentencepiece_path : Union [ str , Path ] ) -> Tuple [ nn . Module , nn . Module , BatchTextTransformer ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path: Path to the .yaml file, usually called model_config.yaml sentencepiece_path: Path to the sentencepiece model used to tokenize, usually called tokenizer.model Returns: A tuple containing, in this order, the encoder, the audio transform and the text transform \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 1 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] strides = [ cfg [ \"stride\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , \"strides\" : strides , } preprocess = conf [ \"preprocessor\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"vocabulary\" ] encoder = CitrinetEncoder ( ** encoder_cfg ) text_transform = BatchTextTransformer ( tokens = fix_vocab ( labels ), sentencepiece_model = sentencepiece_path , ) audio_transform = FilterbankFeatures ( ** preprocess_cfg ) return ( encoder , audio_transform , text_transform , )","title":"Compatibility"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.CitrinetCheckpoint","text":"Trained model weight checkpoints. Used by download_checkpoint and load_citrinet_checkpoint . Note Possible values are stt_en_citrinet_256 , stt_en_citrinet_512 , stt_en_citrinet_1024 , stt_es_citrinet_512 Source code in thunder/citrinet/compatibility.py class CitrinetCheckpoint ( BaseCheckpoint ): \"\"\"Trained model weight checkpoints. Used by [`download_checkpoint`][thunder.utils.download_checkpoint] and [`load_citrinet_checkpoint`][thunder.citrinet.compatibility.load_citrinet_checkpoint]. Note: Possible values are `stt_en_citrinet_256`,`stt_en_citrinet_512`,`stt_en_citrinet_1024`, `stt_es_citrinet_512` \"\"\" stt_en_citrinet_256 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_256/versions/1.0.0rc1/files/stt_en_citrinet_256.nemo\" stt_en_citrinet_512 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_512/versions/1.0.0rc1/files/stt_en_citrinet_512.nemo\" stt_en_citrinet_1024 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_1024/versions/1.0.0rc1/files/stt_en_citrinet_1024.nemo\" stt_es_citrinet_512 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_citrinet_512/versions/1.0.0/files/stt_es_citrinet_512.nemo\"","title":"CitrinetCheckpoint"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.fix_vocab","text":"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Parameters: Name Type Description Default vocab_tokens List[str] List of tokens in the vocabulary required Returns: Type Description List[str] New list of tokens with the new prefix Source code in thunder/citrinet/compatibility.py def fix_vocab ( vocab_tokens : List [ str ]) -> List [ str ]: \"\"\"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Args: vocab_tokens: List of tokens in the vocabulary Returns: New list of tokens with the new prefix \"\"\" out_tokens = [] for token in vocab_tokens : if token . startswith ( \"##\" ): out_tokens . append ( token [ 2 :]) else : out_tokens . append ( \"\u2581\" + token ) return out_tokens","title":"fix_vocab()"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.load_citrinet_checkpoint","text":"Load from the original nemo checkpoint. Parameters: Name Type Description Default checkpoint Union[str, thunder.citrinet.compatibility.CitrinetCheckpoint] Path to local .nemo file or checkpoint to be downloaded locally and lodaded. required save_folder str Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. None Returns: Type Description BaseCTCModule The model loaded from the checkpoint Source code in thunder/citrinet/compatibility.py def load_citrinet_checkpoint ( checkpoint : Union [ str , CitrinetCheckpoint ], save_folder : str = None ) -> BaseCTCModule : \"\"\"Load from the original nemo checkpoint. Args: checkpoint: Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder: Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , CitrinetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = checkpoint with TemporaryDirectory () as extract_folder : extract_archive ( str ( nemo_filepath ), extract_folder ) extract_path = Path ( extract_folder ) config_path = extract_path / \"model_config.yaml\" sentencepiece_path = str ( extract_path / \"tokenizer.model\" ) ( encoder , audio_transform , text_transform , ) = load_components_from_citrinet_config ( config_path , sentencepiece_path ) decoder = conv1d_decoder ( 640 , num_classes = text_transform . num_tokens ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( encoder , decoder , str ( weights_path )) module = BaseCTCModule ( encoder , decoder , audio_transform , text_transform , encoder_final_dimension = 640 , ) return module . eval ()","title":"load_citrinet_checkpoint()"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.load_components_from_citrinet_config","text":"Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path Union[str, pathlib.Path] Path to the .yaml file, usually called model_config.yaml required sentencepiece_path Union[str, pathlib.Path] Path to the sentencepiece model used to tokenize, usually called tokenizer.model required Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module, thunder.text_processing.transform.BatchTextTransformer] A tuple containing, in this order, the encoder, the audio transform and the text transform Source code in thunder/citrinet/compatibility.py def load_components_from_citrinet_config ( config_path : Union [ str , Path ], sentencepiece_path : Union [ str , Path ] ) -> Tuple [ nn . Module , nn . Module , BatchTextTransformer ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path: Path to the .yaml file, usually called model_config.yaml sentencepiece_path: Path to the sentencepiece model used to tokenize, usually called tokenizer.model Returns: A tuple containing, in this order, the encoder, the audio transform and the text transform \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 1 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] strides = [ cfg [ \"stride\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , \"strides\" : strides , } preprocess = conf [ \"preprocessor\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"vocabulary\" ] encoder = CitrinetEncoder ( ** encoder_cfg ) text_transform = BatchTextTransformer ( tokens = fix_vocab ( labels ), sentencepiece_model = sentencepiece_path , ) audio_transform = FilterbankFeatures ( ** preprocess_cfg ) return ( encoder , audio_transform , text_transform , )","title":"load_components_from_citrinet_config()"},{"location":"api/Data/dataloader%20utils/","text":"Helper functions used by the speech dataloaders. asr_collate ( samples ) Function that collect samples and adds padding. Parameters: Name Type Description Default samples List[Tuple[torch.Tensor, str]] Samples produced by dataloader required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing padded audios, audio lengths and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples: Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"Dataloader utils"},{"location":"api/Data/dataloader%20utils/#thunder.data.dataloader_utils.asr_collate","text":"Function that collect samples and adds padding. Parameters: Name Type Description Default samples List[Tuple[torch.Tensor, str]] Samples produced by dataloader required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing padded audios, audio lengths and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples: Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"asr_collate()"},{"location":"api/Data/datamodule/","text":"Implements pytorch lightning's Datamodule for audio datasets. BaseDataModule ( LightningDataModule ) Source code in thunder/data/datamodule.py class BaseDataModule ( LightningDataModule ): def __init__ ( self , batch_size : int = 10 , num_workers : int = 8 , ): super () . __init__ () self . batch_size = batch_size self . num_workers = num_workers def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split: One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () def setup ( self , stage : Optional [ str ] = None ): if stage in ( None , \"fit\" ): self . train_dataset = self . get_dataset ( split = \"train\" ) self . val_dataset = self . get_dataset ( split = \"valid\" ) if stage in ( None , \"test\" ): self . test_dataset = self . get_dataset ( split = \"test\" ) def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , ) def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) @property def steps_per_epoch ( self ) -> int : \"\"\"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Number of steps \"\"\" return len ( self . train_dataset ) // self . batch_size steps_per_epoch : int property readonly Number of steps for each training epoch. Used for learning rate scheduling. Returns: Type Description int Number of steps get_dataset ( self , split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split: One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () setup ( self , stage = None ) Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(self, stage): data = load_data(...) self.l1 = nn.Linear(28, data.num_classes) Source code in thunder/data/datamodule.py def setup ( self , stage : Optional [ str ] = None ): if stage in ( None , \"fit\" ): self . train_dataset = self . get_dataset ( split = \"train\" ) self . val_dataset = self . get_dataset ( split = \"valid\" ) if stage in ( None , \"test\" ): self . test_dataset = self . get_dataset ( split = \"test\" ) test_dataloader ( self ) Implement one or multiple PyTorch DataLoaders for testing. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.trainer.Trainer.test :meth: prepare_data :meth: setup Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) train_dataloader ( self ) Implement one or more PyTorch DataLoaders for training. Returns: Type Description A collection of class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: section <multiple-dataloaders> . The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.trainer.Trainer.fit :meth: prepare_data :meth: setup Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in thunder/data/datamodule.py def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , ) val_dataloader ( self ) Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.trainer.Trainer.fit :meth: ~pytorch_lightning.trainer.trainer.Trainer.validate :meth: prepare_data :meth: setup Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) ManifestDatamodule ( BaseDataModule ) Source code in thunder/data/datamodule.py class ManifestDatamodule ( BaseDataModule ): def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): \"\"\"Datamodule compatible with the NEMO manifest data format. Args: train_manifest: Training manifest file val_manifest: Validation manifest file test_manifest: Test manifest file force_mono: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] sample_rate: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] batch_size: Batch size used by dataloader num_workers: Number of workers used by dataloader \"\"\" super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate ) __init__ ( self , train_manifest , val_manifest , test_manifest , force_mono = True , sample_rate = 16000 , batch_size = 10 , num_workers = 8 ) special Datamodule compatible with the NEMO manifest data format. Parameters: Name Type Description Default train_manifest str Training manifest file required val_manifest str Validation manifest file required test_manifest str Test manifest file required force_mono bool Check ManifestSpeechDataset True sample_rate int Check ManifestSpeechDataset 16000 batch_size int Batch size used by dataloader 10 num_workers int Number of workers used by dataloader 8 Source code in thunder/data/datamodule.py def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): \"\"\"Datamodule compatible with the NEMO manifest data format. Args: train_manifest: Training manifest file val_manifest: Validation manifest file test_manifest: Test manifest file force_mono: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] sample_rate: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] batch_size: Batch size used by dataloader num_workers: Number of workers used by dataloader \"\"\" super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate get_dataset ( self , split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description ManifestSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"Datamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule","text":"Source code in thunder/data/datamodule.py class BaseDataModule ( LightningDataModule ): def __init__ ( self , batch_size : int = 10 , num_workers : int = 8 , ): super () . __init__ () self . batch_size = batch_size self . num_workers = num_workers def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split: One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () def setup ( self , stage : Optional [ str ] = None ): if stage in ( None , \"fit\" ): self . train_dataset = self . get_dataset ( split = \"train\" ) self . val_dataset = self . get_dataset ( split = \"valid\" ) if stage in ( None , \"test\" ): self . test_dataset = self . get_dataset ( split = \"test\" ) def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , ) def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) @property def steps_per_epoch ( self ) -> int : \"\"\"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Number of steps \"\"\" return len ( self . train_dataset ) // self . batch_size","title":"BaseDataModule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.steps_per_epoch","text":"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Type Description int Number of steps","title":"steps_per_epoch"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split: One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError ()","title":"get_dataset()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.setup","text":"Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(self, stage): data = load_data(...) self.l1 = nn.Linear(28, data.num_classes) Source code in thunder/data/datamodule.py def setup ( self , stage : Optional [ str ] = None ): if stage in ( None , \"fit\" ): self . train_dataset = self . get_dataset ( split = \"train\" ) self . val_dataset = self . get_dataset ( split = \"valid\" ) if stage in ( None , \"test\" ): self . test_dataset = self . get_dataset ( split = \"test\" )","title":"setup()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.test_dataloader","text":"Implement one or multiple PyTorch DataLoaders for testing. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.trainer.Trainer.test :meth: prepare_data :meth: setup Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , )","title":"test_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.train_dataloader","text":"Implement one or more PyTorch DataLoaders for training. Returns: Type Description A collection of class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: section <multiple-dataloaders> . The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.trainer.Trainer.fit :meth: prepare_data :meth: setup Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in thunder/data/datamodule.py def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , )","title":"train_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.val_dataloader","text":"Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.trainer.Trainer.fit :meth: ~pytorch_lightning.trainer.trainer.Trainer.validate :meth: prepare_data :meth: setup Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , )","title":"val_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule","text":"Source code in thunder/data/datamodule.py class ManifestDatamodule ( BaseDataModule ): def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): \"\"\"Datamodule compatible with the NEMO manifest data format. Args: train_manifest: Training manifest file val_manifest: Validation manifest file test_manifest: Test manifest file force_mono: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] sample_rate: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] batch_size: Batch size used by dataloader num_workers: Number of workers used by dataloader \"\"\" super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"ManifestDatamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule.__init__","text":"Datamodule compatible with the NEMO manifest data format. Parameters: Name Type Description Default train_manifest str Training manifest file required val_manifest str Validation manifest file required test_manifest str Test manifest file required force_mono bool Check ManifestSpeechDataset True sample_rate int Check ManifestSpeechDataset 16000 batch_size int Batch size used by dataloader 10 num_workers int Number of workers used by dataloader 8 Source code in thunder/data/datamodule.py def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): \"\"\"Datamodule compatible with the NEMO manifest data format. Args: train_manifest: Training manifest file val_manifest: Validation manifest file test_manifest: Test manifest file force_mono: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] sample_rate: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] batch_size: Batch size used by dataloader num_workers: Number of workers used by dataloader \"\"\" super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate","title":"__init__()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description ManifestSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"get_dataset()"},{"location":"api/Data/dataset/","text":"Speech recognition datasets AudioFileLoader ( Module ) Source code in thunder/data/dataset.py class AudioFileLoader ( nn . Module ): def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = int ( sample_rate ), new_freq = int ( self . sample_rate ) ) return audio def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item: Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate ) __init__ ( self , force_mono = True , sample_rate = 16000 ) special Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Parameters: Name Type Description Default force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate forward ( self , item ) Opens audio item and do basic preprocessing Parameters: Name Type Description Default item str Path to the audio to be opened required Returns: Type Description Tensor Audio tensor after preprocessing Source code in thunder/data/dataset.py def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item: Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate ) open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item str Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) preprocess_audio ( self , audio , sample_rate ) Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = int ( sample_rate ), new_freq = int ( self . sample_rate ) ) return audio BaseSpeechDataset ( Dataset ) Source code in thunder/data/dataset.py class BaseSpeechDataset ( Dataset ): def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items: Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate ) def __len__ ( self ): return len ( self . items ) def __getitem__ ( self , index : int ) -> Tuple [ Tensor , str ]: item = self . get_item ( index ) # Dealing with input audio , sr = self . open_audio ( item ) audio = self . preprocess_audio ( audio , sr ) # Dealing with output text = self . open_text ( item ) text = self . preprocess_text ( text ) return audio , text def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index: Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item ) def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate ) def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item: The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text: Label text Returns: Label text after processing \"\"\" return text __init__ ( self , items , force_mono = True , sample_rate = 16000 ) special This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Parameters: Name Type Description Default items Sequence Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. required force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items: Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate ) all_outputs ( self ) Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: Type Description List[str] All of the outputs of the dataset, with the corresponding preprocessing applied. Source code in thunder/data/dataset.py def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs get_item ( self , index ) Get the item source specified by the index. Parameters: Name Type Description Default index int Indicates what item it needs to return information about. required Returns: Type Description Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index: Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item Any Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item ) open_text ( self , item ) Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item Any The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item: The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () preprocess_audio ( self , audio , sample_rate ) Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate ) preprocess_text ( self , text ) Add here preprocessing steps to remove some common problems in the text. Parameters: Name Type Description Default text str Label text required Returns: Type Description str Label text after processing Source code in thunder/data/dataset.py def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text: Label text Returns: Label text after processing \"\"\" return text ManifestSpeechDataset ( BaseSpeechDataset ) Source code in thunder/data/dataset.py class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file: Nemo manifest file. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ] __init__ ( self , file , force_mono , sample_rate ) special Dataset that loads from nemo manifest files. Parameters: Name Type Description Default file Union[str, pathlib.Path] Nemo manifest file. required force_mono bool If true, convert all the loaded samples to mono. required sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. required Source code in thunder/data/dataset.py def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file: Nemo manifest file. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item dict Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) open_text ( self , item ) Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item dict The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"Dataset"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader","text":"Source code in thunder/data/dataset.py class AudioFileLoader ( nn . Module ): def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = int ( sample_rate ), new_freq = int ( self . sample_rate ) ) return audio def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item: Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate )","title":"AudioFileLoader"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.__init__","text":"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Parameters: Name Type Description Default force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.forward","text":"Opens audio item and do basic preprocessing Parameters: Name Type Description Default item str Path to the audio to be opened required Returns: Type Description Tensor Audio tensor after preprocessing Source code in thunder/data/dataset.py def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item: Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate )","title":"forward()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item str Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item )","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.preprocess_audio","text":"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = int ( sample_rate ), new_freq = int ( self . sample_rate ) ) return audio","title":"preprocess_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset","text":"Source code in thunder/data/dataset.py class BaseSpeechDataset ( Dataset ): def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items: Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate ) def __len__ ( self ): return len ( self . items ) def __getitem__ ( self , index : int ) -> Tuple [ Tensor , str ]: item = self . get_item ( index ) # Dealing with input audio , sr = self . open_audio ( item ) audio = self . preprocess_audio ( audio , sr ) # Dealing with output text = self . open_text ( item ) text = self . preprocess_text ( text ) return audio , text def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index: Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item ) def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate ) def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item: The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text: Label text Returns: Label text after processing \"\"\" return text","title":"BaseSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.__init__","text":"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Parameters: Name Type Description Default items Sequence Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. required force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items: Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate )","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.all_outputs","text":"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: Type Description List[str] All of the outputs of the dataset, with the corresponding preprocessing applied. Source code in thunder/data/dataset.py def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs","title":"all_outputs()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.get_item","text":"Get the item source specified by the index. Parameters: Name Type Description Default index int Indicates what item it needs to return information about. required Returns: Type Description Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index: Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ]","title":"get_item()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item Any Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item )","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item Any The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item: The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError ()","title":"open_text()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.preprocess_audio","text":"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate )","title":"preprocess_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.preprocess_text","text":"Add here preprocessing steps to remove some common problems in the text. Parameters: Name Type Description Default text str Label text required Returns: Type Description str Label text after processing Source code in thunder/data/dataset.py def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text: Label text Returns: Label text after processing \"\"\" return text","title":"preprocess_text()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset","text":"Source code in thunder/data/dataset.py class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file: Nemo manifest file. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"ManifestSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.__init__","text":"Dataset that loads from nemo manifest files. Parameters: Name Type Description Default file Union[str, pathlib.Path] Nemo manifest file. required force_mono bool If true, convert all the loaded samples to mono. required sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. required Source code in thunder/data/dataset.py def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file: Nemo manifest file. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate )","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item dict Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ])","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item dict The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"open_text()"},{"location":"api/Huggingface/compatibility/","text":"Helper functions to load huggingface speech recognition models. load_huggingface_checkpoint ( model_name , ** model_kwargs ) Load huggingface model and convert to thunder BaseCTCModule Parameters: Name Type Description Default model_name str huggingface identifier of the model, like \"facebook/wav2vec2-large-960h\" required model_kwargs Dict[str, Any] extra keyword arguments to be passed to AutoModelForCTC.from_pretrained {} Returns: Type Description BaseCTCModule Thunder module containing the huggingface model. Source code in thunder/huggingface/compatibility.py def load_huggingface_checkpoint ( model_name : str , ** model_kwargs : Dict [ str , Any ] ) -> BaseCTCModule : \"\"\"Load huggingface model and convert to thunder [`BaseCTCModule`][thunder.module.BaseCTCModule] Args: model_name: huggingface identifier of the model, like \"facebook/wav2vec2-large-960h\" model_kwargs: extra keyword arguments to be passed to `AutoModelForCTC.from_pretrained` Returns: Thunder module containing the huggingface model. \"\"\" model = AutoModelForCTC . from_pretrained ( model_name , ** model_kwargs ) feature_extractor = AutoFeatureExtractor . from_pretrained ( model_name ) # Some models only contain the encoder, and no tokenizer # In that case we need to warn the user to fix it before training try : tokenizer = AutoTokenizer . from_pretrained ( model_name ) text_transform = _tok_to_transform ( tokenizer ) decoder = linear_decoder ( model . base_model . config . hidden_size , text_transform . num_tokens , decoder_dropout = 0.0 , ) if hasattr ( model , \"lm_head\" ): decoder [ 2 ] . load_state_dict ( model . lm_head . state_dict ()) except OSError : warn ( UserWarning ( \"Huggingface model is missing the tokenizer! decoder and text_transform were not initialized\" ) ) text_transform = None decoder = None module = BaseCTCModule ( encoder = _HuggingFaceEncoderAdapt ( model . base_model , mask_input = feature_extractor . return_attention_mask , ), decoder = decoder , text_transform = text_transform , audio_transform = Wav2Vec2Preprocess ( mask_input = feature_extractor . return_attention_mask , ), encoder_final_dimension = model . base_model . config . hidden_size , ) return module . eval () prepare_scriptable_wav2vec ( module , quantized = False ) Converts thunder module containing a wav2vec2 model to be scriptable. Parameters: Name Type Description Default module BaseCTCModule Module containing wav2vec2 required quantized bool If true, also performs quantization of the model False Returns: Type Description BaseCTCModule Modified module ready to call torch.jit.script(module) or module.to_torchscript() Source code in thunder/huggingface/compatibility.py def prepare_scriptable_wav2vec ( module : BaseCTCModule , quantized : bool = False ) -> BaseCTCModule : \"\"\"Converts thunder module containing a wav2vec2 model to be scriptable. Args: module: Module containing wav2vec2 quantized: If true, also performs quantization of the model Returns: Modified module ready to call torch.jit.script(module) or module.to_torchscript() \"\"\" imported = import_huggingface_model ( module . encoder . original_encoder ) if quantized : imported . encoder . transformer . pos_conv_embed . __prepare_scriptable__ () imported = torch . quantization . quantize_dynamic ( imported , qconfig_spec = { torch . nn . Linear }, dtype = torch . qint8 ) module . encoder = imported module . decoder = nn . Sequential ( * module . decoder [ 1 :]) return module","title":"Compatibility"},{"location":"api/Huggingface/compatibility/#thunder.huggingface.compatibility.load_huggingface_checkpoint","text":"Load huggingface model and convert to thunder BaseCTCModule Parameters: Name Type Description Default model_name str huggingface identifier of the model, like \"facebook/wav2vec2-large-960h\" required model_kwargs Dict[str, Any] extra keyword arguments to be passed to AutoModelForCTC.from_pretrained {} Returns: Type Description BaseCTCModule Thunder module containing the huggingface model. Source code in thunder/huggingface/compatibility.py def load_huggingface_checkpoint ( model_name : str , ** model_kwargs : Dict [ str , Any ] ) -> BaseCTCModule : \"\"\"Load huggingface model and convert to thunder [`BaseCTCModule`][thunder.module.BaseCTCModule] Args: model_name: huggingface identifier of the model, like \"facebook/wav2vec2-large-960h\" model_kwargs: extra keyword arguments to be passed to `AutoModelForCTC.from_pretrained` Returns: Thunder module containing the huggingface model. \"\"\" model = AutoModelForCTC . from_pretrained ( model_name , ** model_kwargs ) feature_extractor = AutoFeatureExtractor . from_pretrained ( model_name ) # Some models only contain the encoder, and no tokenizer # In that case we need to warn the user to fix it before training try : tokenizer = AutoTokenizer . from_pretrained ( model_name ) text_transform = _tok_to_transform ( tokenizer ) decoder = linear_decoder ( model . base_model . config . hidden_size , text_transform . num_tokens , decoder_dropout = 0.0 , ) if hasattr ( model , \"lm_head\" ): decoder [ 2 ] . load_state_dict ( model . lm_head . state_dict ()) except OSError : warn ( UserWarning ( \"Huggingface model is missing the tokenizer! decoder and text_transform were not initialized\" ) ) text_transform = None decoder = None module = BaseCTCModule ( encoder = _HuggingFaceEncoderAdapt ( model . base_model , mask_input = feature_extractor . return_attention_mask , ), decoder = decoder , text_transform = text_transform , audio_transform = Wav2Vec2Preprocess ( mask_input = feature_extractor . return_attention_mask , ), encoder_final_dimension = model . base_model . config . hidden_size , ) return module . eval ()","title":"load_huggingface_checkpoint()"},{"location":"api/Huggingface/compatibility/#thunder.huggingface.compatibility.prepare_scriptable_wav2vec","text":"Converts thunder module containing a wav2vec2 model to be scriptable. Parameters: Name Type Description Default module BaseCTCModule Module containing wav2vec2 required quantized bool If true, also performs quantization of the model False Returns: Type Description BaseCTCModule Modified module ready to call torch.jit.script(module) or module.to_torchscript() Source code in thunder/huggingface/compatibility.py def prepare_scriptable_wav2vec ( module : BaseCTCModule , quantized : bool = False ) -> BaseCTCModule : \"\"\"Converts thunder module containing a wav2vec2 model to be scriptable. Args: module: Module containing wav2vec2 quantized: If true, also performs quantization of the model Returns: Modified module ready to call torch.jit.script(module) or module.to_torchscript() \"\"\" imported = import_huggingface_model ( module . encoder . original_encoder ) if quantized : imported . encoder . transformer . pos_conv_embed . __prepare_scriptable__ () imported = torch . quantization . quantize_dynamic ( imported , qconfig_spec = { torch . nn . Linear }, dtype = torch . qint8 ) module . encoder = imported module . decoder = nn . Sequential ( * module . decoder [ 1 :]) return module","title":"prepare_scriptable_wav2vec()"},{"location":"api/Huggingface/transform/","text":"Implementation of data preprocessing transform compatible with the huggingface wav2vec2 one Wav2Vec2Preprocess ( Module ) Source code in thunder/huggingface/transform.py class Wav2Vec2Preprocess ( nn . Module ): def __init__ ( self , div_guard : float = 1e-7 , mask_input : bool = False , ): \"\"\"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Args: div_guard: Guard value to prevent division by zero. mask_input: controls the use of masking in the input tensor. \"\"\" super () . __init__ () self . div_guard = div_guard self . mask_input = mask_input def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , Optional [ torch . Tensor ]]: \"\"\"Applies the normalization Args: audio: Audio tensor of shape [batch_size, time] audio_lengths: corresponding length of each element in the input tensor. Returns: Normalized audio tensor with same shape as input. Optionally the valid mask \"\"\" attention_mask : Optional [ torch . Tensor ] = None if self . mask_input : attention_mask = lengths_to_mask ( audio_lengths , max_length = audio . size ( - 1 ) ) . int () return ( normalize_tensor ( audio , attention_mask , div_guard = self . div_guard ), audio_lengths , ) __init__ ( self , div_guard = 1e-07 , mask_input = False ) special Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Parameters: Name Type Description Default div_guard float Guard value to prevent division by zero. 1e-07 mask_input bool controls the use of masking in the input tensor. False Source code in thunder/huggingface/transform.py def __init__ ( self , div_guard : float = 1e-7 , mask_input : bool = False , ): \"\"\"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Args: div_guard: Guard value to prevent division by zero. mask_input: controls the use of masking in the input tensor. \"\"\" super () . __init__ () self . div_guard = div_guard self . mask_input = mask_input forward ( self , audio , audio_lengths ) Applies the normalization Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required audio_lengths Tensor corresponding length of each element in the input tensor. required Returns: Type Description Tuple[torch.Tensor, Optional[torch.Tensor]] Normalized audio tensor with same shape as input. Optionally the valid mask Source code in thunder/huggingface/transform.py def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , Optional [ torch . Tensor ]]: \"\"\"Applies the normalization Args: audio: Audio tensor of shape [batch_size, time] audio_lengths: corresponding length of each element in the input tensor. Returns: Normalized audio tensor with same shape as input. Optionally the valid mask \"\"\" attention_mask : Optional [ torch . Tensor ] = None if self . mask_input : attention_mask = lengths_to_mask ( audio_lengths , max_length = audio . size ( - 1 ) ) . int () return ( normalize_tensor ( audio , attention_mask , div_guard = self . div_guard ), audio_lengths , )","title":"Transform"},{"location":"api/Huggingface/transform/#thunder.huggingface.transform.Wav2Vec2Preprocess","text":"Source code in thunder/huggingface/transform.py class Wav2Vec2Preprocess ( nn . Module ): def __init__ ( self , div_guard : float = 1e-7 , mask_input : bool = False , ): \"\"\"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Args: div_guard: Guard value to prevent division by zero. mask_input: controls the use of masking in the input tensor. \"\"\" super () . __init__ () self . div_guard = div_guard self . mask_input = mask_input def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , Optional [ torch . Tensor ]]: \"\"\"Applies the normalization Args: audio: Audio tensor of shape [batch_size, time] audio_lengths: corresponding length of each element in the input tensor. Returns: Normalized audio tensor with same shape as input. Optionally the valid mask \"\"\" attention_mask : Optional [ torch . Tensor ] = None if self . mask_input : attention_mask = lengths_to_mask ( audio_lengths , max_length = audio . size ( - 1 ) ) . int () return ( normalize_tensor ( audio , attention_mask , div_guard = self . div_guard ), audio_lengths , )","title":"Wav2Vec2Preprocess"},{"location":"api/Huggingface/transform/#thunder.huggingface.transform.Wav2Vec2Preprocess.__init__","text":"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Parameters: Name Type Description Default div_guard float Guard value to prevent division by zero. 1e-07 mask_input bool controls the use of masking in the input tensor. False Source code in thunder/huggingface/transform.py def __init__ ( self , div_guard : float = 1e-7 , mask_input : bool = False , ): \"\"\"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Args: div_guard: Guard value to prevent division by zero. mask_input: controls the use of masking in the input tensor. \"\"\" super () . __init__ () self . div_guard = div_guard self . mask_input = mask_input","title":"__init__()"},{"location":"api/Huggingface/transform/#thunder.huggingface.transform.Wav2Vec2Preprocess.forward","text":"Applies the normalization Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required audio_lengths Tensor corresponding length of each element in the input tensor. required Returns: Type Description Tuple[torch.Tensor, Optional[torch.Tensor]] Normalized audio tensor with same shape as input. Optionally the valid mask Source code in thunder/huggingface/transform.py def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , Optional [ torch . Tensor ]]: \"\"\"Applies the normalization Args: audio: Audio tensor of shape [batch_size, time] audio_lengths: corresponding length of each element in the input tensor. Returns: Normalized audio tensor with same shape as input. Optionally the valid mask \"\"\" attention_mask : Optional [ torch . Tensor ] = None if self . mask_input : attention_mask = lengths_to_mask ( audio_lengths , max_length = audio . size ( - 1 ) ) . int () return ( normalize_tensor ( audio , attention_mask , div_guard = self . div_guard ), audio_lengths , )","title":"forward()"},{"location":"api/Quartznet/blocks/","text":"Basic building blocks to create the Quartznet model InitMode ( str , Enum ) Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal Source code in thunder/quartznet/blocks.py class InitMode ( str , Enum ): \"\"\"Weight init methods. Used by [`init_weights`][thunder.quartznet.blocks.init_weights]. Note: Possible values are `xavier_uniform`,`xavier_normal`,`kaiming_uniform` and `kaiming_normal` \"\"\" xavier_uniform = \"xavier_uniform\" xavier_normal = \"xavier_normal\" kaiming_uniform = \"kaiming_uniform\" kaiming_normal = \"kaiming_normal\" MaskedConv1d ( Module ) Source code in thunder/quartznet/blocks.py class MaskedConv1d ( nn . Module ): __constants__ = [ \"use_mask\" , \"padding\" , \"dilation\" , \"kernel_size\" , \"stride\" ] def __init__ ( self , in_channels : int , out_channels : int , kernel_size : _size_1_t , stride : _size_1_t = 1 , padding : _size_1_t = 0 , dilation : _size_1_t = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels: Same as nn.Conv1d out_channels: Same as nn.Conv1d kernel_size: Same as nn.Conv1d stride: Same as nn.Conv1d padding: Same as nn.Conv1d dilation: Same as nn.Conv1d groups: Same as nn.Conv1d bias: Same as nn.Conv1d use_mask: Controls the masking of input before the convolution during the forward. \"\"\" super () . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ] def get_seq_len ( self , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lengths: Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( torch . div ( lengths + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 , self . stride , rounding_mode = \"floor\" , ) + 1 ) def mask_fill ( self , x : torch . Tensor , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return x . masked_fill ( ~ mask . unsqueeze ( 1 ), 0 ) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lengths ) out = self . conv ( x ) return out , self . get_seq_len ( lengths ) __init__ ( self , in_channels , out_channels , kernel_size , stride = 1 , padding = 0 , dilation = 1 , groups = 1 , bias = False , use_mask = True ) special Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Parameters: Name Type Description Default in_channels int Same as nn.Conv1d required out_channels int Same as nn.Conv1d required kernel_size Union[int, Tuple[int]] Same as nn.Conv1d required stride Union[int, Tuple[int]] Same as nn.Conv1d 1 padding Union[int, Tuple[int]] Same as nn.Conv1d 0 dilation Union[int, Tuple[int]] Same as nn.Conv1d 1 groups int Same as nn.Conv1d 1 bias bool Same as nn.Conv1d False use_mask bool Controls the masking of input before the convolution during the forward. True Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , kernel_size : _size_1_t , stride : _size_1_t = 1 , padding : _size_1_t = 0 , dilation : _size_1_t = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels: Same as nn.Conv1d out_channels: Same as nn.Conv1d kernel_size: Same as nn.Conv1d stride: Same as nn.Conv1d padding: Same as nn.Conv1d dilation: Same as nn.Conv1d groups: Same as nn.Conv1d bias: Same as nn.Conv1d use_mask: Controls the masking of input before the convolution during the forward. \"\"\" super () . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ] forward ( self , x , lengths ) Forward method Parameters: Name Type Description Default x Tensor Signal to be processed, of shape (batch, features, time) required lengths Tensor Lenghts of each element in the batch of x, with shape (batch) required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Both the signal processed by the convolution and the resulting lengths Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lengths ) out = self . conv ( x ) return out , self . get_seq_len ( lengths ) get_seq_len ( self , lengths ) Get the lengths of the inputs after the convolution operation is applied. Parameters: Name Type Description Default lengths Tensor Original lengths of the inputs required Returns: Type Description Tensor Resulting lengths after the convolution Source code in thunder/quartznet/blocks.py def get_seq_len ( self , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lengths: Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( torch . div ( lengths + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 , self . stride , rounding_mode = \"floor\" , ) + 1 ) mask_fill ( self , x , lengths ) Mask the input based on it's respective lengths. Parameters: Name Type Description Default x Tensor Signal to be processed, of shape (batch, features, time) required lengths Tensor Lenghts of each element in the batch of x, with shape (batch) required Returns: Type Description Tensor The masked signal Source code in thunder/quartznet/blocks.py def mask_fill ( self , x : torch . Tensor , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return x . masked_fill ( ~ mask . unsqueeze ( 1 ), 0 ) QuartznetBlock ( Module ) Source code in thunder/quartznet/blocks.py class QuartznetBlock ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out __init__ ( self , in_channels , out_channels , repeat = 5 , kernel_size = ( 11 ,), stride = ( 1 ,), dilation = ( 1 ,), dropout = 0.0 , residual = True , separable = False ) special Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size Union[int, Tuple[int]] Kernel size. (11,) stride Union[int, Tuple[int]] Stride of each repetition. (1,) dilation Union[int, Tuple[int]] Dilation of each repetition. (1,) dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) forward ( self , x , lengths ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Result of applying the block on the input, and corresponding output lengths Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out QuartznetEncoder ( feat_in = 64 , filters = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks = 1 ) Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Parameters: Name Type Description Default feat_in int Number of input features to the model. 64 filters List[int] List of filter sizes used to create the encoder blocks. [256, 256, 512, 512, 512] kernel_sizes List[int] List of kernel sizes corresponding to each filter size. [33, 39, 51, 63, 75] repeat_blocks int Number of repetitions of each block. 1 Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/blocks.py def QuartznetEncoder ( feat_in : int = 64 , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , ) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: feat_in: Number of input features to the model. filters: List of filter sizes used to create the encoder blocks. kernel_sizes: List of kernel sizes corresponding to each filter size. repeat_blocks: Number of repetitions of each block. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , repeat_blocks ), ) body ( filters , kernel_size , repeat_blocks = 1 ) Creates the body of the Quartznet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required repeat_blocks int Number of repetitions of each block inside the body. 1 Returns: Type Description List[thunder.quartznet.blocks.QuartznetBlock] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters: List of filters inside each block in the body. kernel_size: Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks: Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = ( k ,), separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = ( 2 ,), kernel_size = ( 87 ,), residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = ( 1 ,), residual = False , separable = False ), ] ) return layers init_weights ( m , mode =< InitMode . xavier_uniform : 'xavier_uniform' > ) Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , MaskedConv1d ): init_weights ( m . conv , mode ) if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( f \"Unknown Initialization mode: { mode } \" ) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias ) stem ( feat_in ) Creates the Quartznet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in: Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = ( 2 ,), kernel_size = ( 33 ,), residual = False , separable = True , )","title":"Blocks"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.InitMode","text":"Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal Source code in thunder/quartznet/blocks.py class InitMode ( str , Enum ): \"\"\"Weight init methods. Used by [`init_weights`][thunder.quartznet.blocks.init_weights]. Note: Possible values are `xavier_uniform`,`xavier_normal`,`kaiming_uniform` and `kaiming_normal` \"\"\" xavier_uniform = \"xavier_uniform\" xavier_normal = \"xavier_normal\" kaiming_uniform = \"kaiming_uniform\" kaiming_normal = \"kaiming_normal\"","title":"InitMode"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d","text":"Source code in thunder/quartznet/blocks.py class MaskedConv1d ( nn . Module ): __constants__ = [ \"use_mask\" , \"padding\" , \"dilation\" , \"kernel_size\" , \"stride\" ] def __init__ ( self , in_channels : int , out_channels : int , kernel_size : _size_1_t , stride : _size_1_t = 1 , padding : _size_1_t = 0 , dilation : _size_1_t = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels: Same as nn.Conv1d out_channels: Same as nn.Conv1d kernel_size: Same as nn.Conv1d stride: Same as nn.Conv1d padding: Same as nn.Conv1d dilation: Same as nn.Conv1d groups: Same as nn.Conv1d bias: Same as nn.Conv1d use_mask: Controls the masking of input before the convolution during the forward. \"\"\" super () . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ] def get_seq_len ( self , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lengths: Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( torch . div ( lengths + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 , self . stride , rounding_mode = \"floor\" , ) + 1 ) def mask_fill ( self , x : torch . Tensor , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return x . masked_fill ( ~ mask . unsqueeze ( 1 ), 0 ) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lengths ) out = self . conv ( x ) return out , self . get_seq_len ( lengths )","title":"MaskedConv1d"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.__init__","text":"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Parameters: Name Type Description Default in_channels int Same as nn.Conv1d required out_channels int Same as nn.Conv1d required kernel_size Union[int, Tuple[int]] Same as nn.Conv1d required stride Union[int, Tuple[int]] Same as nn.Conv1d 1 padding Union[int, Tuple[int]] Same as nn.Conv1d 0 dilation Union[int, Tuple[int]] Same as nn.Conv1d 1 groups int Same as nn.Conv1d 1 bias bool Same as nn.Conv1d False use_mask bool Controls the masking of input before the convolution during the forward. True Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , kernel_size : _size_1_t , stride : _size_1_t = 1 , padding : _size_1_t = 0 , dilation : _size_1_t = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels: Same as nn.Conv1d out_channels: Same as nn.Conv1d kernel_size: Same as nn.Conv1d stride: Same as nn.Conv1d padding: Same as nn.Conv1d dilation: Same as nn.Conv1d groups: Same as nn.Conv1d bias: Same as nn.Conv1d use_mask: Controls the masking of input before the convolution during the forward. \"\"\" super () . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ]","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.forward","text":"Forward method Parameters: Name Type Description Default x Tensor Signal to be processed, of shape (batch, features, time) required lengths Tensor Lenghts of each element in the batch of x, with shape (batch) required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Both the signal processed by the convolution and the resulting lengths Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lengths ) out = self . conv ( x ) return out , self . get_seq_len ( lengths )","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.get_seq_len","text":"Get the lengths of the inputs after the convolution operation is applied. Parameters: Name Type Description Default lengths Tensor Original lengths of the inputs required Returns: Type Description Tensor Resulting lengths after the convolution Source code in thunder/quartznet/blocks.py def get_seq_len ( self , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lengths: Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( torch . div ( lengths + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 , self . stride , rounding_mode = \"floor\" , ) + 1 )","title":"get_seq_len()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.mask_fill","text":"Mask the input based on it's respective lengths. Parameters: Name Type Description Default x Tensor Signal to be processed, of shape (batch, features, time) required lengths Tensor Lenghts of each element in the batch of x, with shape (batch) required Returns: Type Description Tensor The masked signal Source code in thunder/quartznet/blocks.py def mask_fill ( self , x : torch . Tensor , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return x . masked_fill ( ~ mask . unsqueeze ( 1 ), 0 )","title":"mask_fill()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock","text":"Source code in thunder/quartznet/blocks.py class QuartznetBlock ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out","title":"QuartznetBlock"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.__init__","text":"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size Union[int, Tuple[int]] Kernel size. (11,) stride Union[int, Tuple[int]] Stride of each repetition. (1,) dilation Union[int, Tuple[int]] Dilation of each repetition. (1,) dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Result of applying the block on the input, and corresponding output lengths Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetEncoder","text":"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Parameters: Name Type Description Default feat_in int Number of input features to the model. 64 filters List[int] List of filter sizes used to create the encoder blocks. [256, 256, 512, 512, 512] kernel_sizes List[int] List of kernel sizes corresponding to each filter size. [33, 39, 51, 63, 75] repeat_blocks int Number of repetitions of each block. 1 Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/blocks.py def QuartznetEncoder ( feat_in : int = 64 , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , ) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: feat_in: Number of input features to the model. filters: List of filter sizes used to create the encoder blocks. kernel_sizes: List of kernel sizes corresponding to each filter size. repeat_blocks: Number of repetitions of each block. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , repeat_blocks ), )","title":"QuartznetEncoder()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.body","text":"Creates the body of the Quartznet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required repeat_blocks int Number of repetitions of each block inside the body. 1 Returns: Type Description List[thunder.quartznet.blocks.QuartznetBlock] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters: List of filters inside each block in the body. kernel_size: Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks: Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = ( k ,), separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = ( 2 ,), kernel_size = ( 87 ,), residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = ( 1 ,), residual = False , separable = False ), ] ) return layers","title":"body()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.init_weights","text":"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , MaskedConv1d ): init_weights ( m . conv , mode ) if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( f \"Unknown Initialization mode: { mode } \" ) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias )","title":"init_weights()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.stem","text":"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in: Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = ( 2 ,), kernel_size = ( 33 ,), residual = False , separable = True , )","title":"stem()"},{"location":"api/Quartznet/compatibility/","text":"Helper functions to load the Quartznet model from original Nemo released checkpoint files. QuartznetCheckpoint ( BaseCheckpoint ) Trained model weight checkpoints. Used by download_checkpoint and load_quartznet_checkpoint . Note Possible values are QuartzNet15x5Base_En , QuartzNet15x5Base_Zh , QuartzNet5x5LS_En , QuartzNet15x5NR_En , stt_ca_quartznet15x5 , stt_it_quartznet15x5 , stt_fr_quartznet15x5 , stt_es_quartznet15x5 , stt_de_quartznet15x5 , stt_pl_quartznet15x5 , stt_ru_quartznet15x5 , stt_en_quartznet15x5 , stt_zh_quartznet15x5 Source code in thunder/quartznet/compatibility.py class QuartznetCheckpoint ( BaseCheckpoint ): \"\"\"Trained model weight checkpoints. Used by [`download_checkpoint`][thunder.utils.download_checkpoint] and [`load_quartznet_checkpoint`][thunder.quartznet.compatibility.load_quartznet_checkpoint]. Note: Possible values are `QuartzNet15x5Base_En`,`QuartzNet15x5Base_Zh`,`QuartzNet5x5LS_En`, `QuartzNet15x5NR_En`, `stt_ca_quartznet15x5`,`stt_it_quartznet15x5`,`stt_fr_quartznet15x5`,`stt_es_quartznet15x5`, `stt_de_quartznet15x5`,`stt_pl_quartznet15x5`,`stt_ru_quartznet15x5`,`stt_en_quartznet15x5`, `stt_zh_quartznet15x5` \"\"\" QuartzNet15x5Base_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-En.nemo\" QuartzNet15x5Base_Zh = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-Zh.nemo\" QuartzNet5x5LS_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet5x5LS-En.nemo\" QuartzNet15x5NR_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5NR-En.nemo\" stt_ca_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ca_quartznet15x5/versions/1.0.0rc1/files/stt_ca_quartznet15x5.nemo\" stt_it_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_it_quartznet15x5/versions/1.0.0rc1/files/stt_it_quartznet15x5.nemo\" stt_fr_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_fr_quartznet15x5/versions/1.0.0rc1/files/stt_fr_quartznet15x5.nemo\" stt_es_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_quartznet15x5/versions/1.0.0rc1/files/stt_es_quartznet15x5.nemo\" stt_de_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_de_quartznet15x5/versions/1.0.0rc1/files/stt_de_quartznet15x5.nemo\" stt_pl_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_pl_quartznet15x5/versions/1.0.0rc1/files/stt_pl_quartznet15x5.nemo\" stt_ru_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ru_quartznet15x5/versions/1.0.0rc1/files/stt_ru_quartznet15x5.nemo\" stt_en_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_quartznet15x5/versions/1.0.0rc1/files/stt_en_quartznet15x5.nemo\" stt_zh_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_zh_quartznet15x5/versions/1.0.0rc1/files/stt_zh_quartznet15x5.nemo\" load_components_from_quartznet_config ( config_path ) Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path Union[str, pathlib.Path] Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module, thunder.text_processing.transform.BatchTextTransformer] A tuple containing, in this order, the encoder, the audio transform and the text transform Source code in thunder/quartznet/compatibility.py def load_components_from_quartznet_config ( config_path : Union [ str , Path ] ) -> Tuple [ nn . Module , nn . Module , BatchTextTransformer ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path: Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder, the audio transform and the text transform \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = ( conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"params\" ][ \"vocabulary\" ] ) audio_transform = FilterbankFeatures ( ** preprocess_cfg ) encoder = QuartznetEncoder ( ** encoder_cfg ) text_transform = BatchTextTransformer ( tokens = OmegaConf . to_container ( labels ), ) return ( encoder , audio_transform , text_transform , ) load_quartznet_checkpoint ( checkpoint , save_folder = None ) Load from the original nemo checkpoint. Parameters: Name Type Description Default checkpoint Union[str, thunder.quartznet.compatibility.QuartznetCheckpoint] Path to local .nemo file or checkpoint to be downloaded locally and lodaded. required save_folder str Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. None Returns: Type Description BaseCTCModule The model loaded from the checkpoint Source code in thunder/quartznet/compatibility.py def load_quartznet_checkpoint ( checkpoint : Union [ str , QuartznetCheckpoint ], save_folder : str = None ) -> BaseCTCModule : \"\"\"Load from the original nemo checkpoint. Args: checkpoint: Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder: Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , QuartznetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = Path ( checkpoint ) with TemporaryDirectory () as extract_folder : extract_archive ( str ( nemo_filepath ), extract_folder ) extract_path = Path ( extract_folder ) config_path = extract_path / \"model_config.yaml\" ( encoder , audio_transform , text_transform , ) = load_components_from_quartznet_config ( config_path ) decoder = conv1d_decoder ( 1024 , text_transform . num_tokens ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( encoder , decoder , str ( weights_path )) module = BaseCTCModule ( encoder , decoder , audio_transform , text_transform , encoder_final_dimension = 1024 , ) return module . eval () load_quartznet_weights ( encoder , decoder , weights_path ) Load Quartznet model weights from data present inside .nemo file Parameters: Name Type Description Default encoder Module Encoder module to load the weights into required decoder Module Decoder module to load the weights into required weights_path str Path to the pytorch weights checkpoint required Source code in thunder/quartznet/compatibility.py def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path: Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) def fix_encoder_name ( x : str ) -> str : x = x . replace ( \"encoder.\" , \"\" ) . replace ( \".res.0\" , \".res\" ) # Add another abstraction layer if it's not a masked conv # This is caused by the new Masked wrapper if \".conv\" not in x : parts = x . split ( \".\" ) x = \".\" . join ( parts [: 3 ] + [ \"layer\" , \"0\" ] + parts [ 3 :]) return x # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { fix_encoder_name ( k ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True )","title":"Compatibility"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.QuartznetCheckpoint","text":"Trained model weight checkpoints. Used by download_checkpoint and load_quartznet_checkpoint . Note Possible values are QuartzNet15x5Base_En , QuartzNet15x5Base_Zh , QuartzNet5x5LS_En , QuartzNet15x5NR_En , stt_ca_quartznet15x5 , stt_it_quartznet15x5 , stt_fr_quartznet15x5 , stt_es_quartznet15x5 , stt_de_quartznet15x5 , stt_pl_quartznet15x5 , stt_ru_quartznet15x5 , stt_en_quartznet15x5 , stt_zh_quartznet15x5 Source code in thunder/quartznet/compatibility.py class QuartznetCheckpoint ( BaseCheckpoint ): \"\"\"Trained model weight checkpoints. Used by [`download_checkpoint`][thunder.utils.download_checkpoint] and [`load_quartznet_checkpoint`][thunder.quartznet.compatibility.load_quartznet_checkpoint]. Note: Possible values are `QuartzNet15x5Base_En`,`QuartzNet15x5Base_Zh`,`QuartzNet5x5LS_En`, `QuartzNet15x5NR_En`, `stt_ca_quartznet15x5`,`stt_it_quartznet15x5`,`stt_fr_quartznet15x5`,`stt_es_quartznet15x5`, `stt_de_quartznet15x5`,`stt_pl_quartznet15x5`,`stt_ru_quartznet15x5`,`stt_en_quartznet15x5`, `stt_zh_quartznet15x5` \"\"\" QuartzNet15x5Base_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-En.nemo\" QuartzNet15x5Base_Zh = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-Zh.nemo\" QuartzNet5x5LS_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet5x5LS-En.nemo\" QuartzNet15x5NR_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5NR-En.nemo\" stt_ca_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ca_quartznet15x5/versions/1.0.0rc1/files/stt_ca_quartznet15x5.nemo\" stt_it_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_it_quartznet15x5/versions/1.0.0rc1/files/stt_it_quartznet15x5.nemo\" stt_fr_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_fr_quartznet15x5/versions/1.0.0rc1/files/stt_fr_quartznet15x5.nemo\" stt_es_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_quartznet15x5/versions/1.0.0rc1/files/stt_es_quartznet15x5.nemo\" stt_de_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_de_quartznet15x5/versions/1.0.0rc1/files/stt_de_quartznet15x5.nemo\" stt_pl_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_pl_quartznet15x5/versions/1.0.0rc1/files/stt_pl_quartznet15x5.nemo\" stt_ru_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ru_quartznet15x5/versions/1.0.0rc1/files/stt_ru_quartznet15x5.nemo\" stt_en_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_quartznet15x5/versions/1.0.0rc1/files/stt_en_quartznet15x5.nemo\" stt_zh_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_zh_quartznet15x5/versions/1.0.0rc1/files/stt_zh_quartznet15x5.nemo\"","title":"QuartznetCheckpoint"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_components_from_quartznet_config","text":"Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path Union[str, pathlib.Path] Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module, thunder.text_processing.transform.BatchTextTransformer] A tuple containing, in this order, the encoder, the audio transform and the text transform Source code in thunder/quartznet/compatibility.py def load_components_from_quartznet_config ( config_path : Union [ str , Path ] ) -> Tuple [ nn . Module , nn . Module , BatchTextTransformer ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path: Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder, the audio transform and the text transform \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = ( conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"params\" ][ \"vocabulary\" ] ) audio_transform = FilterbankFeatures ( ** preprocess_cfg ) encoder = QuartznetEncoder ( ** encoder_cfg ) text_transform = BatchTextTransformer ( tokens = OmegaConf . to_container ( labels ), ) return ( encoder , audio_transform , text_transform , )","title":"load_components_from_quartznet_config()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_quartznet_checkpoint","text":"Load from the original nemo checkpoint. Parameters: Name Type Description Default checkpoint Union[str, thunder.quartznet.compatibility.QuartznetCheckpoint] Path to local .nemo file or checkpoint to be downloaded locally and lodaded. required save_folder str Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. None Returns: Type Description BaseCTCModule The model loaded from the checkpoint Source code in thunder/quartznet/compatibility.py def load_quartznet_checkpoint ( checkpoint : Union [ str , QuartznetCheckpoint ], save_folder : str = None ) -> BaseCTCModule : \"\"\"Load from the original nemo checkpoint. Args: checkpoint: Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder: Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , QuartznetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = Path ( checkpoint ) with TemporaryDirectory () as extract_folder : extract_archive ( str ( nemo_filepath ), extract_folder ) extract_path = Path ( extract_folder ) config_path = extract_path / \"model_config.yaml\" ( encoder , audio_transform , text_transform , ) = load_components_from_quartznet_config ( config_path ) decoder = conv1d_decoder ( 1024 , text_transform . num_tokens ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( encoder , decoder , str ( weights_path )) module = BaseCTCModule ( encoder , decoder , audio_transform , text_transform , encoder_final_dimension = 1024 , ) return module . eval ()","title":"load_quartznet_checkpoint()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_quartznet_weights","text":"Load Quartznet model weights from data present inside .nemo file Parameters: Name Type Description Default encoder Module Encoder module to load the weights into required decoder Module Decoder module to load the weights into required weights_path str Path to the pytorch weights checkpoint required Source code in thunder/quartznet/compatibility.py def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path: Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) def fix_encoder_name ( x : str ) -> str : x = x . replace ( \"encoder.\" , \"\" ) . replace ( \".res.0\" , \".res\" ) # Add another abstraction layer if it's not a masked conv # This is caused by the new Masked wrapper if \".conv\" not in x : parts = x . split ( \".\" ) x = \".\" . join ( parts [: 3 ] + [ \"layer\" , \"0\" ] + parts [ 3 :]) return x # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { fix_encoder_name ( k ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True )","title":"load_quartznet_weights()"},{"location":"api/Quartznet/transform/","text":"Functionality to transform the audio input in the same way that the Quartznet model expects it. DitherAudio ( Module ) Source code in thunder/quartznet/transform.py class DitherAudio ( nn . Module ): def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither: Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" if self . training : return x + ( self . dither * torch . randn_like ( x )) else : return x __init__ ( self , dither = 1e-05 ) special Add some dithering to the audio tensor. Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Parameters: Name Type Description Default dither float Amount of dither to add. 1e-05 Source code in thunder/quartznet/transform.py def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither: Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" if self . training : return x + ( self . dither * torch . randn_like ( x )) else : return x FeatureBatchNormalizer ( Module ) Source code in thunder/quartznet/transform.py class FeatureBatchNormalizer ( nn . Module ): def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) lengths: corresponding length of each element in the input tensor. \"\"\" # https://github.com/pytorch/pytorch/issues/45208 # https://github.com/pytorch/pytorch/issues/44768 with torch . no_grad (): mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return ( normalize_tensor ( x , mask . unsqueeze ( 1 ), div_guard = self . div_guard ), lengths , ) __init__ ( self ) special Normalize batch at the feature dimension. Source code in thunder/quartznet/transform.py def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 forward ( self , x , lengths ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required lengths Tensor corresponding length of each element in the input tensor. required Source code in thunder/quartznet/transform.py def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) lengths: corresponding length of each element in the input tensor. \"\"\" # https://github.com/pytorch/pytorch/issues/45208 # https://github.com/pytorch/pytorch/issues/44768 with torch . no_grad (): mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return ( normalize_tensor ( x , mask . unsqueeze ( 1 ), div_guard = self . div_guard ), lengths , ) MelScale ( Module ) Source code in thunder/quartznet/transform.py class MelScale ( nn . Module ): def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate: Sampling rate of the signal n_fft: Number of fourier features nfilt: Number of output mel filters to use log_scale: Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( melscale_fbanks ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 **- 24 ) return x __init__ ( self , sample_rate , n_fft , nfilt , log_scale = True ) special Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal required n_fft int Number of fourier features required nfilt int Number of output mel filters to use required log_scale bool Controls if the output should also be applied a log scale. True Source code in thunder/quartznet/transform.py def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate: Sampling rate of the signal n_fft: Number of fourier features nfilt: Number of output mel filters to use log_scale: Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( melscale_fbanks ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 **- 24 ) return x PowerSpectrum ( Module ) Source code in thunder/quartznet/transform.py class PowerSpectrum ( nn . Module ): def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) # This way so that the torch.stft can be changed to the patched version # before scripting. That way it works correctly when the export option # doesnt support fft, like mobile or onnx. self . stft_func = torch . stft def get_sequence_length ( self , lengths : torch . Tensor ) -> torch . Tensor : seq_len = torch . floor ( lengths / self . hop_length ) + 1 return seq_len . to ( dtype = torch . long ) @torch . no_grad () def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" x = self . stft_func ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x , self . get_sequence_length ( lengths ) __init__ ( self , n_window_size = 320 , n_window_stride = 160 , n_fft = None ) special Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Parameters: Name Type Description Default n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft Optional[int] Number of fourier features. None Exceptions: Type Description ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/transform.py def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) # This way so that the torch.stft can be changed to the patched version # before scripting. That way it works correctly when the export option # doesnt support fft, like mobile or onnx. self . stft_func = torch . stft forward ( self , x , lengths ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" x = self . stft_func ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x , self . get_sequence_length ( lengths ) PreEmphasisFilter ( Module ) Source code in thunder/quartznet/transform.py class PreEmphasisFilter ( nn . Module ): def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph: Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 ) __init__ ( self , preemph = 0.97 ) special Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] Parameters: Name Type Description Default preemph float Filter control factor. 0.97 Source code in thunder/quartznet/transform.py def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph: Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 ) FilterbankFeatures ( sample_rate = 16000 , n_window_size = 320 , n_window_stride = 160 , n_fft = 512 , preemph = 0.97 , nfilt = 64 , dither = 1e-05 ) Creates the Filterbank features used in the Quartznet model. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal. 16000 n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft int Number of fourier features. 512 preemph float Preemphasis filtering control factor. 0.97 nfilt int Number of output mel filters to use. 64 dither float Amount of dither to add. 1e-05 Returns: Type Description Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/transform.py def FilterbankFeatures ( sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: sample_rate: Sampling rate of the signal. n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. preemph: Preemphasis filtering control factor. nfilt: Number of output mel filters to use. dither: Amount of dither to add. Returns: Module that computes the features based on raw audio tensor. \"\"\" return MultiSequential ( Masked ( DitherAudio ( dither = dither ), PreEmphasisFilter ( preemph = preemph )), PowerSpectrum ( n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , ), Masked ( MelScale ( sample_rate = sample_rate , n_fft = n_fft , nfilt = nfilt )), FeatureBatchNormalizer (), ) patch_stft ( filterbank ) This function applies a patch to the FilterbankFeatures to use instead a convolution layer based stft. That makes possible to export to onnx and use the scripted model directly on arm cpu's, inside mobile applications. Parameters: Name Type Description Default filterbank Module the FilterbankFeatures layer to be patched required Returns: Type Description Module Layer with the stft operation patched. Source code in thunder/quartznet/transform.py def patch_stft ( filterbank : nn . Module ) -> nn . Module : \"\"\"This function applies a patch to the FilterbankFeatures to use instead a convolution layer based stft. That makes possible to export to onnx and use the scripted model directly on arm cpu's, inside mobile applications. Args: filterbank: the FilterbankFeatures layer to be patched Returns: Layer with the stft operation patched. \"\"\" filterbank [ 1 ] . stft_func = convolution_stft return filterbank","title":"Transform"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio","text":"Source code in thunder/quartznet/transform.py class DitherAudio ( nn . Module ): def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither: Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" if self . training : return x + ( self . dither * torch . randn_like ( x )) else : return x","title":"DitherAudio"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio.__init__","text":"Add some dithering to the audio tensor. Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Parameters: Name Type Description Default dither float Amount of dither to add. 1e-05 Source code in thunder/quartznet/transform.py def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither: Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" if self . training : return x + ( self . dither * torch . randn_like ( x )) else : return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer","text":"Source code in thunder/quartznet/transform.py class FeatureBatchNormalizer ( nn . Module ): def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) lengths: corresponding length of each element in the input tensor. \"\"\" # https://github.com/pytorch/pytorch/issues/45208 # https://github.com/pytorch/pytorch/issues/44768 with torch . no_grad (): mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return ( normalize_tensor ( x , mask . unsqueeze ( 1 ), div_guard = self . div_guard ), lengths , )","title":"FeatureBatchNormalizer"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer.__init__","text":"Normalize batch at the feature dimension. Source code in thunder/quartznet/transform.py def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required lengths Tensor corresponding length of each element in the input tensor. required Source code in thunder/quartznet/transform.py def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) lengths: corresponding length of each element in the input tensor. \"\"\" # https://github.com/pytorch/pytorch/issues/45208 # https://github.com/pytorch/pytorch/issues/44768 with torch . no_grad (): mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return ( normalize_tensor ( x , mask . unsqueeze ( 1 ), div_guard = self . div_guard ), lengths , )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale","text":"Source code in thunder/quartznet/transform.py class MelScale ( nn . Module ): def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate: Sampling rate of the signal n_fft: Number of fourier features nfilt: Number of output mel filters to use log_scale: Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( melscale_fbanks ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 **- 24 ) return x","title":"MelScale"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale.__init__","text":"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal required n_fft int Number of fourier features required nfilt int Number of output mel filters to use required log_scale bool Controls if the output should also be applied a log scale. True Source code in thunder/quartznet/transform.py def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate: Sampling rate of the signal n_fft: Number of fourier features nfilt: Number of output mel filters to use log_scale: Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( melscale_fbanks ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 **- 24 ) return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum","text":"Source code in thunder/quartznet/transform.py class PowerSpectrum ( nn . Module ): def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) # This way so that the torch.stft can be changed to the patched version # before scripting. That way it works correctly when the export option # doesnt support fft, like mobile or onnx. self . stft_func = torch . stft def get_sequence_length ( self , lengths : torch . Tensor ) -> torch . Tensor : seq_len = torch . floor ( lengths / self . hop_length ) + 1 return seq_len . to ( dtype = torch . long ) @torch . no_grad () def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" x = self . stft_func ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x , self . get_sequence_length ( lengths )","title":"PowerSpectrum"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum.__init__","text":"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Parameters: Name Type Description Default n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft Optional[int] Number of fourier features. None Exceptions: Type Description ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/transform.py def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) # This way so that the torch.stft can be changed to the patched version # before scripting. That way it works correctly when the export option # doesnt support fft, like mobile or onnx. self . stft_func = torch . stft","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" x = self . stft_func ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x , self . get_sequence_length ( lengths )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter","text":"Source code in thunder/quartznet/transform.py class PreEmphasisFilter ( nn . Module ): def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph: Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 )","title":"PreEmphasisFilter"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter.__init__","text":"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] Parameters: Name Type Description Default preemph float Filter control factor. 0.97 Source code in thunder/quartznet/transform.py def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph: Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FilterbankFeatures","text":"Creates the Filterbank features used in the Quartznet model. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal. 16000 n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft int Number of fourier features. 512 preemph float Preemphasis filtering control factor. 0.97 nfilt int Number of output mel filters to use. 64 dither float Amount of dither to add. 1e-05 Returns: Type Description Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/transform.py def FilterbankFeatures ( sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: sample_rate: Sampling rate of the signal. n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. preemph: Preemphasis filtering control factor. nfilt: Number of output mel filters to use. dither: Amount of dither to add. Returns: Module that computes the features based on raw audio tensor. \"\"\" return MultiSequential ( Masked ( DitherAudio ( dither = dither ), PreEmphasisFilter ( preemph = preemph )), PowerSpectrum ( n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , ), Masked ( MelScale ( sample_rate = sample_rate , n_fft = n_fft , nfilt = nfilt )), FeatureBatchNormalizer (), )","title":"FilterbankFeatures()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.patch_stft","text":"This function applies a patch to the FilterbankFeatures to use instead a convolution layer based stft. That makes possible to export to onnx and use the scripted model directly on arm cpu's, inside mobile applications. Parameters: Name Type Description Default filterbank Module the FilterbankFeatures layer to be patched required Returns: Type Description Module Layer with the stft operation patched. Source code in thunder/quartznet/transform.py def patch_stft ( filterbank : nn . Module ) -> nn . Module : \"\"\"This function applies a patch to the FilterbankFeatures to use instead a convolution layer based stft. That makes possible to export to onnx and use the scripted model directly on arm cpu's, inside mobile applications. Args: filterbank: the FilterbankFeatures layer to be patched Returns: Layer with the stft operation patched. \"\"\" filterbank [ 1 ] . stft_func = convolution_stft return filterbank","title":"patch_stft()"},{"location":"api/Text%20Processing/preprocess/","text":"Text preprocessing functionality expand_numbers ( text , language = 'en' ) Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Parameters: Name Type Description Default text str Input text required language str Language used to expand the numbers. Defaults to \"en\". 'en' Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text: Input text language: Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text lower_text ( text ) Transform all the text to lowercase. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text: Input text Returns: Output text \"\"\" return text . lower () normalize_text ( text ) Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text: Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"Preprocess"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.expand_numbers","text":"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Parameters: Name Type Description Default text str Input text required language str Language used to expand the numbers. Defaults to \"en\". 'en' Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text: Input text language: Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text","title":"expand_numbers()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.lower_text","text":"Transform all the text to lowercase. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text: Input text Returns: Output text \"\"\" return text . lower ()","title":"lower_text()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.normalize_text","text":"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text: Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"normalize_text()"},{"location":"api/Text%20Processing/tokenize/","text":"Text tokenization including character, word or sentencepiece char_tokenizer ( text ) Tokenize input text splitting into characters Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text: Input text Returns: Tokenized text \"\"\" return list ( text ) get_most_frequent_tokens ( corpus , tokenize_function , minimum_frequency = 1 , max_number_of_tokens = None ) Helper function to get the most frequent tokens from a text corpus. Parameters: Name Type Description Default corpus str Text corpus to be used, this is a long string containing all of your text required tokenize_function Callable Same tokenizer function that will be used during training required minimum_frequency int Remove any token with frequency less than that. Defaults to 1. 1 max_number_of_tokens Optional[int] Optionally limit to the K most frequent tokens. Defaults to None. None Returns: Type Description List[str] All of the unique, most frequent tokens, ordered by frequency. Source code in thunder/text_processing/tokenizer.py def get_most_frequent_tokens ( corpus : str , tokenize_function : Callable , minimum_frequency : int = 1 , max_number_of_tokens : Optional [ int ] = None , ) -> List [ str ]: \"\"\"Helper function to get the most frequent tokens from a text corpus. Args: corpus: Text corpus to be used, this is a long string containing all of your text tokenize_function: Same tokenizer function that will be used during training minimum_frequency: Remove any token with frequency less than that. Defaults to 1. max_number_of_tokens: Optionally limit to the K most frequent tokens. Defaults to None. Returns: All of the unique, most frequent tokens, ordered by frequency. \"\"\" tokenized = tokenize_function ( corpus ) token_counter = Counter ( tokenized ) output_tokens = [] for token , count in token_counter . most_common ( max_number_of_tokens ): if count >= minimum_frequency : output_tokens . append ( token ) return output_tokens train_sentencepiece_model ( data_file , vocab_size , output_dir , sample_size =- 1 , do_lower_case = True , tokenizer_type = 'unigram' , character_coverage = 1.0 , train_extremely_large_corpus = False , max_sentencepiece_length =- 1 ) Creates sentence piece tokenizer model from data file. This is a direct port of create_spt_model present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Parameters: Name Type Description Default data_file str text file containing the sentences that will be used to train the model required vocab_size int maximum vocabulary size required output_dir str folder to save created tokenizer model and vocab required sample_size int maximum number of sentences the trainer loads. -1 means to use all the data. -1 do_lower_case bool if text should be lower cased before tokenizer model is created True tokenizer_type str controls the sentencepiece model type. 'unigram' character_coverage float float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 1.0 train_extremely_large_corpus bool If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. False max_sentencepiece_length int Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. -1 Source code in thunder/text_processing/tokenizer.py def train_sentencepiece_model ( data_file : str , vocab_size : int , output_dir : str , sample_size : int = - 1 , do_lower_case : bool = True , tokenizer_type : str = \"unigram\" , character_coverage : float = 1.0 , train_extremely_large_corpus : bool = False , max_sentencepiece_length : int = - 1 , ) -> str : \"\"\" Creates sentence piece tokenizer model from data file. This is a direct port of `create_spt_model` present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Args: data_file: text file containing the sentences that will be used to train the model vocab_size: maximum vocabulary size output_dir: folder to save created tokenizer model and vocab sample_size: maximum number of sentences the trainer loads. -1 means to use all the data. do_lower_case: if text should be lower cased before tokenizer model is created tokenizer_type: controls the sentencepiece model type. character_coverage: float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 train_extremely_large_corpus: If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. max_sentencepiece_length: Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. \"\"\" data_file = Path ( data_file ) if not data_file or not data_file . exists (): raise ValueError ( f \"data_file must be valid file path, but got { data_file } \" ) output_dir = Path ( output_dir ) if ( output_dir / \"tokenizer.model\" ) . exists (): warn ( \"There's already a trained sentencepiece model at the output directory. Skipping train.\" ) return str ( output_dir ) output_dir . mkdir ( exist_ok = True ) cmd = ( f \"--input= { data_file } --model_prefix= { output_dir } /tokenizer \" f \"--vocab_size= { vocab_size } \" f \"--shuffle_input_sentence=true --hard_vocab_limit=false \" f \"--model_type= { tokenizer_type } \" f \"--character_coverage= { character_coverage } \" ) if do_lower_case : cmd += \" --normalization_rule_name=nmt_nfkc_cf\" if sample_size > 0 : cmd += f \" --input_sentence_size= { sample_size } \" if train_extremely_large_corpus : cmd += \" --train_extremely_large_corpus=true\" if max_sentencepiece_length >= 0 : cmd += f \" --max_sentencepiece_length= { max_sentencepiece_length } \" sentencepiece . SentencePieceTrainer . Train ( cmd ) return str ( output_dir ) word_tokenizer ( text ) Tokenize input text splitting into words Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text: Input text Returns: Tokenized text \"\"\" return text . split ()","title":"Tokenize"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.char_tokenizer","text":"Tokenize input text splitting into characters Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text: Input text Returns: Tokenized text \"\"\" return list ( text )","title":"char_tokenizer()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.get_most_frequent_tokens","text":"Helper function to get the most frequent tokens from a text corpus. Parameters: Name Type Description Default corpus str Text corpus to be used, this is a long string containing all of your text required tokenize_function Callable Same tokenizer function that will be used during training required minimum_frequency int Remove any token with frequency less than that. Defaults to 1. 1 max_number_of_tokens Optional[int] Optionally limit to the K most frequent tokens. Defaults to None. None Returns: Type Description List[str] All of the unique, most frequent tokens, ordered by frequency. Source code in thunder/text_processing/tokenizer.py def get_most_frequent_tokens ( corpus : str , tokenize_function : Callable , minimum_frequency : int = 1 , max_number_of_tokens : Optional [ int ] = None , ) -> List [ str ]: \"\"\"Helper function to get the most frequent tokens from a text corpus. Args: corpus: Text corpus to be used, this is a long string containing all of your text tokenize_function: Same tokenizer function that will be used during training minimum_frequency: Remove any token with frequency less than that. Defaults to 1. max_number_of_tokens: Optionally limit to the K most frequent tokens. Defaults to None. Returns: All of the unique, most frequent tokens, ordered by frequency. \"\"\" tokenized = tokenize_function ( corpus ) token_counter = Counter ( tokenized ) output_tokens = [] for token , count in token_counter . most_common ( max_number_of_tokens ): if count >= minimum_frequency : output_tokens . append ( token ) return output_tokens","title":"get_most_frequent_tokens()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.train_sentencepiece_model","text":"Creates sentence piece tokenizer model from data file. This is a direct port of create_spt_model present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Parameters: Name Type Description Default data_file str text file containing the sentences that will be used to train the model required vocab_size int maximum vocabulary size required output_dir str folder to save created tokenizer model and vocab required sample_size int maximum number of sentences the trainer loads. -1 means to use all the data. -1 do_lower_case bool if text should be lower cased before tokenizer model is created True tokenizer_type str controls the sentencepiece model type. 'unigram' character_coverage float float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 1.0 train_extremely_large_corpus bool If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. False max_sentencepiece_length int Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. -1 Source code in thunder/text_processing/tokenizer.py def train_sentencepiece_model ( data_file : str , vocab_size : int , output_dir : str , sample_size : int = - 1 , do_lower_case : bool = True , tokenizer_type : str = \"unigram\" , character_coverage : float = 1.0 , train_extremely_large_corpus : bool = False , max_sentencepiece_length : int = - 1 , ) -> str : \"\"\" Creates sentence piece tokenizer model from data file. This is a direct port of `create_spt_model` present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Args: data_file: text file containing the sentences that will be used to train the model vocab_size: maximum vocabulary size output_dir: folder to save created tokenizer model and vocab sample_size: maximum number of sentences the trainer loads. -1 means to use all the data. do_lower_case: if text should be lower cased before tokenizer model is created tokenizer_type: controls the sentencepiece model type. character_coverage: float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 train_extremely_large_corpus: If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. max_sentencepiece_length: Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. \"\"\" data_file = Path ( data_file ) if not data_file or not data_file . exists (): raise ValueError ( f \"data_file must be valid file path, but got { data_file } \" ) output_dir = Path ( output_dir ) if ( output_dir / \"tokenizer.model\" ) . exists (): warn ( \"There's already a trained sentencepiece model at the output directory. Skipping train.\" ) return str ( output_dir ) output_dir . mkdir ( exist_ok = True ) cmd = ( f \"--input= { data_file } --model_prefix= { output_dir } /tokenizer \" f \"--vocab_size= { vocab_size } \" f \"--shuffle_input_sentence=true --hard_vocab_limit=false \" f \"--model_type= { tokenizer_type } \" f \"--character_coverage= { character_coverage } \" ) if do_lower_case : cmd += \" --normalization_rule_name=nmt_nfkc_cf\" if sample_size > 0 : cmd += f \" --input_sentence_size= { sample_size } \" if train_extremely_large_corpus : cmd += \" --train_extremely_large_corpus=true\" if max_sentencepiece_length >= 0 : cmd += f \" --max_sentencepiece_length= { max_sentencepiece_length } \" sentencepiece . SentencePieceTrainer . Train ( cmd ) return str ( output_dir )","title":"train_sentencepiece_model()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.word_tokenizer","text":"Tokenize input text splitting into words Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text: Input text Returns: Tokenized text \"\"\" return text . split ()","title":"word_tokenizer()"},{"location":"api/Text%20Processing/transform/","text":"Process batched text BatchTextTransformer ( Module ) Source code in thunder/text_processing/transform.py class BatchTextTransformer ( nn . Module ): def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : str = None , unknown_token : str = None , start_token : str = None , end_token : str = None , sentencepiece_model : Optional [ str ] = None , custom_tokenizer_function : Callable [[ str ], List [ str ]] = None , ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: tokens: Basic list of tokens that will be part of the vocabulary. blank_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] pad_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] unknown_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] start_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] end_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] sentencepiece_model: Path to sentencepiece .model file, if applicable. custom_tokenizer_function: Allows the use of a custom function to tokenize the input. \"\"\" super () . __init__ () self . vocab = Vocabulary ( tokens , blank_token , pad_token , unknown_token , start_token , end_token , ) if custom_tokenizer_function : self . tokenizer = custom_tokenizer_function elif sentencepiece_model : self . tokenizer = BPETokenizer ( sentencepiece_model ) else : self . tokenizer = char_tokenizer def encode ( self , items : List [ str ], return_length : bool = True , device = None ) -> Union [ Tensor , Tuple [ Tensor , Tensor ]]: \"\"\"Encode a list of texts to a padded pytorch tensor Args: items: List of texts to be processed return_length: optionally also return the length of each element in the encoded tensor device: optional device to create the tensors with Returns: Either the encoded tensor, or a tuple with tensor and lengths. \"\"\" tokenized = [ self . tokenizer ( x ) for x in items ] expanded_tokenized = [ self . vocab . add_special_tokens ( x ) for x in tokenized ] encoded = [ self . vocab . numericalize ( x ) . to ( device = device ) for x in expanded_tokenized ] encoded_batched = pad_sequence ( encoded , batch_first = True , padding_value = self . vocab . pad_idx ) if return_length : lengths = torch . LongTensor ([ len ( it ) for it in encoded ]) . to ( device = device ) return encoded_batched , lengths else : return encoded_batched @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions: Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) # | is a special char used by huggingface as space out = out . replace ( \"|\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"BatchTextTransformer\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir: Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `BatchTextTransformer` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , ) @property def num_tokens ( self ): return len ( self . vocab . itos ) __init__ ( self , tokens , blank_token = '<blank>' , pad_token = None , unknown_token = None , start_token = None , end_token = None , sentencepiece_model = None , custom_tokenizer_function = None ) special That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Parameters: Name Type Description Default tokens List[str] Basic list of tokens that will be part of the vocabulary. required blank_token str Check Vocabulary '<blank>' pad_token str Check Vocabulary None unknown_token str Check Vocabulary None start_token str Check Vocabulary None end_token str Check Vocabulary None sentencepiece_model Optional[str] Path to sentencepiece .model file, if applicable. None custom_tokenizer_function Callable[[str], List[str]] Allows the use of a custom function to tokenize the input. None Source code in thunder/text_processing/transform.py def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : str = None , unknown_token : str = None , start_token : str = None , end_token : str = None , sentencepiece_model : Optional [ str ] = None , custom_tokenizer_function : Callable [[ str ], List [ str ]] = None , ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: tokens: Basic list of tokens that will be part of the vocabulary. blank_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] pad_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] unknown_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] start_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] end_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] sentencepiece_model: Path to sentencepiece .model file, if applicable. custom_tokenizer_function: Allows the use of a custom function to tokenize the input. \"\"\" super () . __init__ () self . vocab = Vocabulary ( tokens , blank_token , pad_token , unknown_token , start_token , end_token , ) if custom_tokenizer_function : self . tokenizer = custom_tokenizer_function elif sentencepiece_model : self . tokenizer = BPETokenizer ( sentencepiece_model ) else : self . tokenizer = char_tokenizer decode_prediction ( self , predictions , remove_repeated = True ) Parameters: Name Type Description Default predictions Tensor Tensor of shape (batch, time) required remove_repeated bool controls if repeated elements without a blank between them will be removed while decoding True Returns: Type Description List[str] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions: Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) # | is a special char used by huggingface as space out = out . replace ( \"|\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list encode ( self , items , return_length = True , device = None ) Encode a list of texts to a padded pytorch tensor Parameters: Name Type Description Default items List[str] List of texts to be processed required return_length bool optionally also return the length of each element in the encoded tensor True device optional device to create the tensors with None Returns: Type Description Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] Either the encoded tensor, or a tuple with tensor and lengths. Source code in thunder/text_processing/transform.py def encode ( self , items : List [ str ], return_length : bool = True , device = None ) -> Union [ Tensor , Tuple [ Tensor , Tensor ]]: \"\"\"Encode a list of texts to a padded pytorch tensor Args: items: List of texts to be processed return_length: optionally also return the length of each element in the encoded tensor device: optional device to create the tensors with Returns: Either the encoded tensor, or a tuple with tensor and lengths. \"\"\" tokenized = [ self . tokenizer ( x ) for x in items ] expanded_tokenized = [ self . vocab . add_special_tokens ( x ) for x in tokenized ] encoded = [ self . vocab . numericalize ( x ) . to ( device = device ) for x in expanded_tokenized ] encoded_batched = pad_sequence ( encoded , batch_first = True , padding_value = self . vocab . pad_idx ) if return_length : lengths = torch . LongTensor ([ len ( it ) for it in encoded ]) . to ( device = device ) return encoded_batched , lengths else : return encoded_batched from_sentencepiece ( output_dir ) classmethod Load the data from a folder that contains the tokenizer.vocab and tokenizer.model outputs from sentencepiece. Parameters: Name Type Description Default output_dir str Output directory of the sentencepiece training, that contains the required files. required Returns: Type Description BatchTextTransformer Instance of BatchTextTransformer with the corresponding data loaded. Source code in thunder/text_processing/transform.py @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"BatchTextTransformer\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir: Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `BatchTextTransformer` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , )","title":"Transform"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer","text":"Source code in thunder/text_processing/transform.py class BatchTextTransformer ( nn . Module ): def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : str = None , unknown_token : str = None , start_token : str = None , end_token : str = None , sentencepiece_model : Optional [ str ] = None , custom_tokenizer_function : Callable [[ str ], List [ str ]] = None , ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: tokens: Basic list of tokens that will be part of the vocabulary. blank_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] pad_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] unknown_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] start_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] end_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] sentencepiece_model: Path to sentencepiece .model file, if applicable. custom_tokenizer_function: Allows the use of a custom function to tokenize the input. \"\"\" super () . __init__ () self . vocab = Vocabulary ( tokens , blank_token , pad_token , unknown_token , start_token , end_token , ) if custom_tokenizer_function : self . tokenizer = custom_tokenizer_function elif sentencepiece_model : self . tokenizer = BPETokenizer ( sentencepiece_model ) else : self . tokenizer = char_tokenizer def encode ( self , items : List [ str ], return_length : bool = True , device = None ) -> Union [ Tensor , Tuple [ Tensor , Tensor ]]: \"\"\"Encode a list of texts to a padded pytorch tensor Args: items: List of texts to be processed return_length: optionally also return the length of each element in the encoded tensor device: optional device to create the tensors with Returns: Either the encoded tensor, or a tuple with tensor and lengths. \"\"\" tokenized = [ self . tokenizer ( x ) for x in items ] expanded_tokenized = [ self . vocab . add_special_tokens ( x ) for x in tokenized ] encoded = [ self . vocab . numericalize ( x ) . to ( device = device ) for x in expanded_tokenized ] encoded_batched = pad_sequence ( encoded , batch_first = True , padding_value = self . vocab . pad_idx ) if return_length : lengths = torch . LongTensor ([ len ( it ) for it in encoded ]) . to ( device = device ) return encoded_batched , lengths else : return encoded_batched @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions: Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) # | is a special char used by huggingface as space out = out . replace ( \"|\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"BatchTextTransformer\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir: Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `BatchTextTransformer` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , ) @property def num_tokens ( self ): return len ( self . vocab . itos )","title":"BatchTextTransformer"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.__init__","text":"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Parameters: Name Type Description Default tokens List[str] Basic list of tokens that will be part of the vocabulary. required blank_token str Check Vocabulary '<blank>' pad_token str Check Vocabulary None unknown_token str Check Vocabulary None start_token str Check Vocabulary None end_token str Check Vocabulary None sentencepiece_model Optional[str] Path to sentencepiece .model file, if applicable. None custom_tokenizer_function Callable[[str], List[str]] Allows the use of a custom function to tokenize the input. None Source code in thunder/text_processing/transform.py def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : str = None , unknown_token : str = None , start_token : str = None , end_token : str = None , sentencepiece_model : Optional [ str ] = None , custom_tokenizer_function : Callable [[ str ], List [ str ]] = None , ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: tokens: Basic list of tokens that will be part of the vocabulary. blank_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] pad_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] unknown_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] start_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] end_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] sentencepiece_model: Path to sentencepiece .model file, if applicable. custom_tokenizer_function: Allows the use of a custom function to tokenize the input. \"\"\" super () . __init__ () self . vocab = Vocabulary ( tokens , blank_token , pad_token , unknown_token , start_token , end_token , ) if custom_tokenizer_function : self . tokenizer = custom_tokenizer_function elif sentencepiece_model : self . tokenizer = BPETokenizer ( sentencepiece_model ) else : self . tokenizer = char_tokenizer","title":"__init__()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.decode_prediction","text":"Parameters: Name Type Description Default predictions Tensor Tensor of shape (batch, time) required remove_repeated bool controls if repeated elements without a blank between them will be removed while decoding True Returns: Type Description List[str] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions: Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) # | is a special char used by huggingface as space out = out . replace ( \"|\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list","title":"decode_prediction()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.encode","text":"Encode a list of texts to a padded pytorch tensor Parameters: Name Type Description Default items List[str] List of texts to be processed required return_length bool optionally also return the length of each element in the encoded tensor True device optional device to create the tensors with None Returns: Type Description Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] Either the encoded tensor, or a tuple with tensor and lengths. Source code in thunder/text_processing/transform.py def encode ( self , items : List [ str ], return_length : bool = True , device = None ) -> Union [ Tensor , Tuple [ Tensor , Tensor ]]: \"\"\"Encode a list of texts to a padded pytorch tensor Args: items: List of texts to be processed return_length: optionally also return the length of each element in the encoded tensor device: optional device to create the tensors with Returns: Either the encoded tensor, or a tuple with tensor and lengths. \"\"\" tokenized = [ self . tokenizer ( x ) for x in items ] expanded_tokenized = [ self . vocab . add_special_tokens ( x ) for x in tokenized ] encoded = [ self . vocab . numericalize ( x ) . to ( device = device ) for x in expanded_tokenized ] encoded_batched = pad_sequence ( encoded , batch_first = True , padding_value = self . vocab . pad_idx ) if return_length : lengths = torch . LongTensor ([ len ( it ) for it in encoded ]) . to ( device = device ) return encoded_batched , lengths else : return encoded_batched","title":"encode()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.from_sentencepiece","text":"Load the data from a folder that contains the tokenizer.vocab and tokenizer.model outputs from sentencepiece. Parameters: Name Type Description Default output_dir str Output directory of the sentencepiece training, that contains the required files. required Returns: Type Description BatchTextTransformer Instance of BatchTextTransformer with the corresponding data loaded. Source code in thunder/text_processing/transform.py @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"BatchTextTransformer\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir: Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `BatchTextTransformer` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , )","title":"from_sentencepiece()"},{"location":"api/Text%20Processing/vocab/","text":"Classes that represent the vocabulary used by the model. Vocabulary ( Module ) Source code in thunder/text_processing/vocab.py class Vocabulary ( nn . Module ): def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : Optional [ str ] = None , unknown_token : Optional [ str ] = None , start_token : Optional [ str ] = None , end_token : Optional [ str ] = None , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: tokens: Basic list of tokens that will be part of the vocabulary. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset) blank_token: Token that will represent the ctc blank. pad_token: Token that will represent padding, might also act as the ctc blank. unknown_token: Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token: Token that will represent the beginning of the sequence. end_token: Token that will represent the end of the sequence. \"\"\" super () . __init__ () self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token self . blank_token = blank_token self . pad_token = pad_token or blank_token self . itos = tokens self . _maybe_add_token ( blank_token ) self . _maybe_add_token ( pad_token ) self . _maybe_add_token ( unknown_token ) self . _maybe_add_token ( start_token ) self . _maybe_add_token ( end_token ) self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . itos . index ( self . pad_token ) self . _unk_idx = - 1 if self . unknown_token is not None : self . _unk_idx = self . itos . index ( self . unknown_token ) def _maybe_add_token ( self , token : Optional [ str ]): # Only adds tokens if they are not optional # and are not included in the vocabulary already if token and ( token not in self . itos ): self . itos = self . itos + [ token ] def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens: A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . unknown_token is None : # When in there's no unknown token # we filter out all of the tokens not in the vocab tokens = [ t for t in tokens if t in self . itos ] return torch . tensor ( [ self . stoi . get ( it , self . _unk_idx ) for it in tokens ], dtype = torch . long ) @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices: Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens: Tokenized text Returns: Text with the special tokens added. \"\"\" if self . start_token is not None : tokens = [ self . start_token ] + tokens if self . end_token is not None : tokens = tokens + [ self . end_token ] return tokens @torch . jit . export def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text: Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) if self . start_token is not None : text = text . replace ( self . start_token , \"\" ) if self . end_token is not None : text = text . replace ( self . end_token , \"\" ) return text __init__ ( self , tokens , blank_token = '<blank>' , pad_token = None , unknown_token = None , start_token = None , end_token = None ) special Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Parameters: Name Type Description Default tokens List[str] Basic list of tokens that will be part of the vocabulary. Check docs required blank_token str Token that will represent the ctc blank. '<blank>' pad_token Optional[str] Token that will represent padding, might also act as the ctc blank. None unknown_token Optional[str] Token that will represent unknown elements. Notice that this is different than the blank used by ctc. None start_token Optional[str] Token that will represent the beginning of the sequence. None end_token Optional[str] Token that will represent the end of the sequence. None Source code in thunder/text_processing/vocab.py def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : Optional [ str ] = None , unknown_token : Optional [ str ] = None , start_token : Optional [ str ] = None , end_token : Optional [ str ] = None , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: tokens: Basic list of tokens that will be part of the vocabulary. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset) blank_token: Token that will represent the ctc blank. pad_token: Token that will represent padding, might also act as the ctc blank. unknown_token: Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token: Token that will represent the beginning of the sequence. end_token: Token that will represent the end of the sequence. \"\"\" super () . __init__ () self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token self . blank_token = blank_token self . pad_token = pad_token or blank_token self . itos = tokens self . _maybe_add_token ( blank_token ) self . _maybe_add_token ( pad_token ) self . _maybe_add_token ( unknown_token ) self . _maybe_add_token ( start_token ) self . _maybe_add_token ( end_token ) self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . itos . index ( self . pad_token ) self . _unk_idx = - 1 if self . unknown_token is not None : self . _unk_idx = self . itos . index ( self . unknown_token ) add_special_tokens ( self , tokens ) Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens: Tokenized text Returns: Text with the special tokens added. \"\"\" if self . start_token is not None : tokens = [ self . start_token ] + tokens if self . end_token is not None : tokens = tokens + [ self . end_token ] return tokens decode_into_text ( self , indices ) Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices: Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] numericalize ( self , tokens ) Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens: A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . unknown_token is None : # When in there's no unknown token # we filter out all of the tokens not in the vocab tokens = [ t for t in tokens if t in self . itos ] return torch . tensor ( [ self . stoi . get ( it , self . _unk_idx ) for it in tokens ], dtype = torch . long ) remove_special_tokens ( self , text ) Function to remove the special tokens from the prediction. Parameters: Name Type Description Default text str Decoded text required Returns: Type Description str Text with the special tokens removed. Source code in thunder/text_processing/vocab.py @torch . jit . export def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text: Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) if self . start_token is not None : text = text . replace ( self . start_token , \"\" ) if self . end_token is not None : text = text . replace ( self . end_token , \"\" ) return text","title":"Vocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary","text":"Source code in thunder/text_processing/vocab.py class Vocabulary ( nn . Module ): def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : Optional [ str ] = None , unknown_token : Optional [ str ] = None , start_token : Optional [ str ] = None , end_token : Optional [ str ] = None , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: tokens: Basic list of tokens that will be part of the vocabulary. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset) blank_token: Token that will represent the ctc blank. pad_token: Token that will represent padding, might also act as the ctc blank. unknown_token: Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token: Token that will represent the beginning of the sequence. end_token: Token that will represent the end of the sequence. \"\"\" super () . __init__ () self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token self . blank_token = blank_token self . pad_token = pad_token or blank_token self . itos = tokens self . _maybe_add_token ( blank_token ) self . _maybe_add_token ( pad_token ) self . _maybe_add_token ( unknown_token ) self . _maybe_add_token ( start_token ) self . _maybe_add_token ( end_token ) self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . itos . index ( self . pad_token ) self . _unk_idx = - 1 if self . unknown_token is not None : self . _unk_idx = self . itos . index ( self . unknown_token ) def _maybe_add_token ( self , token : Optional [ str ]): # Only adds tokens if they are not optional # and are not included in the vocabulary already if token and ( token not in self . itos ): self . itos = self . itos + [ token ] def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens: A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . unknown_token is None : # When in there's no unknown token # we filter out all of the tokens not in the vocab tokens = [ t for t in tokens if t in self . itos ] return torch . tensor ( [ self . stoi . get ( it , self . _unk_idx ) for it in tokens ], dtype = torch . long ) @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices: Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens: Tokenized text Returns: Text with the special tokens added. \"\"\" if self . start_token is not None : tokens = [ self . start_token ] + tokens if self . end_token is not None : tokens = tokens + [ self . end_token ] return tokens @torch . jit . export def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text: Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) if self . start_token is not None : text = text . replace ( self . start_token , \"\" ) if self . end_token is not None : text = text . replace ( self . end_token , \"\" ) return text","title":"Vocabulary"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.__init__","text":"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Parameters: Name Type Description Default tokens List[str] Basic list of tokens that will be part of the vocabulary. Check docs required blank_token str Token that will represent the ctc blank. '<blank>' pad_token Optional[str] Token that will represent padding, might also act as the ctc blank. None unknown_token Optional[str] Token that will represent unknown elements. Notice that this is different than the blank used by ctc. None start_token Optional[str] Token that will represent the beginning of the sequence. None end_token Optional[str] Token that will represent the end of the sequence. None Source code in thunder/text_processing/vocab.py def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : Optional [ str ] = None , unknown_token : Optional [ str ] = None , start_token : Optional [ str ] = None , end_token : Optional [ str ] = None , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: tokens: Basic list of tokens that will be part of the vocabulary. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset) blank_token: Token that will represent the ctc blank. pad_token: Token that will represent padding, might also act as the ctc blank. unknown_token: Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token: Token that will represent the beginning of the sequence. end_token: Token that will represent the end of the sequence. \"\"\" super () . __init__ () self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token self . blank_token = blank_token self . pad_token = pad_token or blank_token self . itos = tokens self . _maybe_add_token ( blank_token ) self . _maybe_add_token ( pad_token ) self . _maybe_add_token ( unknown_token ) self . _maybe_add_token ( start_token ) self . _maybe_add_token ( end_token ) self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . itos . index ( self . pad_token ) self . _unk_idx = - 1 if self . unknown_token is not None : self . _unk_idx = self . itos . index ( self . unknown_token )","title":"__init__()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.add_special_tokens","text":"Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens: Tokenized text Returns: Text with the special tokens added. \"\"\" if self . start_token is not None : tokens = [ self . start_token ] + tokens if self . end_token is not None : tokens = tokens + [ self . end_token ] return tokens","title":"add_special_tokens()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.decode_into_text","text":"Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices: Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ]","title":"decode_into_text()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.numericalize","text":"Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens: A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . unknown_token is None : # When in there's no unknown token # we filter out all of the tokens not in the vocab tokens = [ t for t in tokens if t in self . itos ] return torch . tensor ( [ self . stoi . get ( it , self . _unk_idx ) for it in tokens ], dtype = torch . long )","title":"numericalize()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.remove_special_tokens","text":"Function to remove the special tokens from the prediction. Parameters: Name Type Description Default text str Decoded text required Returns: Type Description str Text with the special tokens removed. Source code in thunder/text_processing/vocab.py @torch . jit . export def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text: Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) if self . start_token is not None : text = text . replace ( self . start_token , \"\" ) if self . end_token is not None : text = text . replace ( self . end_token , \"\" ) return text","title":"remove_special_tokens()"}]}